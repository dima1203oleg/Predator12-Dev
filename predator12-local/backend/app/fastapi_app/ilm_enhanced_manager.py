"""
OpenSearch ILM Manager - –î–µ–ª—å—Ç–∞-—Ä–µ–≤—ñ–∑—ñ—è 1.2
–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∂–∏—Ç—Ç—î–≤–∏–º —Ü–∏–∫–ª–æ–º —ñ–Ω–¥–µ–∫—Å—ñ–≤, rollover, aliases —Ç–∞ pre-aggregations
"""

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, Optional

import aiohttp

logger = logging.getLogger(__name__)


class OpenSearchILMManager:
    """OpenSearch Index Lifecycle Management"""

    def __init__(self, opensearch_host: str = "http://localhost:9200"):
        self.opensearch_host = opensearch_host
        self.session: Optional[aiohttp.ClientSession] = None

        # ILM –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        self.ilm_policies = {
            "customs_data": {
                "policy": {
                    "default_state": "hot",
                    "states": [
                        {
                            "name": "hot",
                            "actions": [{"rollover": {"min_size": "50gb", "min_index_age": "3d"}}],
                            "transitions": [
                                {"state_name": "warm", "conditions": {"min_index_age": "14d"}}
                            ],
                        },
                        {
                            "name": "warm",
                            "actions": [{"replica_count": {"number_of_replicas": 0}}],
                            "transitions": [
                                {"state_name": "cold", "conditions": {"min_index_age": "60d"}}
                            ],
                        },
                        {"name": "cold", "actions": [{"force_merge": {"max_num_segments": 1}}]},
                    ],
                    "ism_template": [{"index_patterns": ["customs_*"], "priority": 100}],
                }
            },
            "osint_data": {
                "policy": {
                    "default_state": "hot",
                    "states": [
                        {
                            "name": "hot",
                            "actions": [{"rollover": {"min_size": "30gb", "min_index_age": "1d"}}],
                            "transitions": [
                                {"state_name": "warm", "conditions": {"min_index_age": "7d"}}
                            ],
                        },
                        {
                            "name": "warm",
                            "actions": [{"replica_count": {"number_of_replicas": 0}}],
                            "transitions": [
                                {"state_name": "delete", "conditions": {"min_index_age": "30d"}}
                            ],
                        },
                        {"name": "delete", "actions": [{"delete": {}}]},
                    ],
                    "ism_template": [{"index_patterns": ["osint_*"], "priority": 100}],
                }
            },
        }

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–ª–∏–∞—Å–æ–≤ –¥–ª—è —á—Ç–µ–Ω–∏—è
        self.read_aliases = {
            "customs_current": "customs_*",
            "osint_current": "osint_*",
            "analytics_current": "analytics_*",
            "safe_current": "safe_*",
            "restricted_current": "restricted_*",
        }

        # Pre-aggregation –∏–Ω–¥–µ–∫—Å—ã
        self.summary_indexes = {
            "daily_customs_summary": {
                "mapping": {
                    "properties": {
                        "date": {"type": "date"},
                        "country": {"type": "keyword"},
                        "hs_code": {"type": "keyword"},
                        "total_value": {"type": "double"},
                        "total_weight": {"type": "double"},
                        "transaction_count": {"type": "long"},
                        "anomaly_score": {"type": "float"},
                    }
                },
                "settings": {
                    "number_of_shards": 2,
                    "number_of_replicas": 1,
                    "refresh_interval": "30s",
                },
            },
            "weekly_market_trends": {
                "mapping": {
                    "properties": {
                        "week_start": {"type": "date"},
                        "market_segment": {"type": "keyword"},
                        "trend_direction": {"type": "keyword"},
                        "growth_rate": {"type": "float"},
                        "prediction_confidence": {"type": "float"},
                    }
                }
            },
        }

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ILM manager"""
        self.session = aiohttp.ClientSession()
        logger.info("üîÑ Initializing OpenSearch ILM Manager...")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º ILM –ø–æ–ª–∏—Ç–∏–∫–∏
        await self._apply_ilm_policies()

        # –°–æ–∑–¥–∞–µ–º –∞–ª–∏–∞—Å—ã
        await self._setup_aliases()

        # –°–æ–∑–¥–∞–µ–º summary –∏–Ω–¥–µ–∫—Å—ã
        await self._create_summary_indexes()

        logger.info("‚úÖ OpenSearch ILM Manager initialized")

    async def _apply_ilm_policies(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ILM –ø–æ–ª–∏—Ç–∏–∫"""
        try:
            for policy_name, policy_config in self.ilm_policies.items():
                url = f"{self.opensearch_host}/_plugins/_ism/policies/{policy_name}"

                async with self.session.put(url, json=policy_config) as response:
                    if response.status in [200, 201]:
                        logger.info(f"‚úÖ Applied ILM policy: {policy_name}")
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Failed to apply ILM policy {policy_name}: {error_text}")

        except Exception as e:
            logger.error(f"‚ùå Error applying ILM policies: {e}")

    async def _setup_aliases(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–ª–∏–∞—Å–æ–≤ –¥–ª—è —á—Ç–µ–Ω–∏—è"""
        try:
            for alias_name, pattern in self.read_aliases.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∞–ª–∏–∞—Å
                url = f"{self.opensearch_host}/_alias/{alias_name}"
                async with self.session.get(url) as response:
                    if response.status == 404:
                        # –°–æ–∑–¥–∞–µ–º –∞–ª–∏–∞—Å
                        alias_body = {"actions": [{"add": {"index": pattern, "alias": alias_name}}]}

                        async with self.session.post(
                            f"{self.opensearch_host}/_aliases", json=alias_body
                        ) as create_response:
                            if create_response.status == 200:
                                logger.info(f"‚úÖ Created alias: {alias_name}")
                            else:
                                error_text = await create_response.text()
                                logger.error(
                                    f"‚ùå Failed to create alias {alias_name}: {error_text}"
                                )

        except Exception as e:
            logger.error(f"‚ùå Error setting up aliases: {e}")

    async def _create_summary_indexes(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ pre-aggregation –∏–Ω–¥–µ–∫—Å–æ–≤"""
        try:
            for index_name, config in self.summary_indexes.items():
                url = f"{self.opensearch_host}/{index_name}"

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
                async with self.session.head(url) as response:
                    if response.status == 404:
                        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
                        async with self.session.put(url, json=config) as create_response:
                            if create_response.status in [200, 201]:
                                logger.info(f"‚úÖ Created summary index: {index_name}")
                            else:
                                error_text = await create_response.text()
                                logger.error(
                                    f"‚ùå Failed to create summary index {index_name}: {error_text}"
                                )

        except Exception as e:
            logger.error(f"‚ùå Error creating summary indexes: {e}")

    async def rollover_index(self, alias_name: str, force: bool = False) -> bool:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π rollover –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            url = f"{self.opensearch_host}/{alias_name}/_rollover"

            rollover_body = {}
            if force:
                rollover_body = {"conditions": {"max_age": "0d"}}  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π rollover

            async with self.session.post(url, json=rollover_body) as response:
                result = await response.json()

                if response.status == 200 and result.get("rolled_over"):
                    logger.info(f"‚úÖ Rolled over index: {alias_name}")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è  Rollover not needed for: {alias_name}")
                    return False

        except Exception as e:
            logger.error(f"‚ùå Error rolling over index {alias_name}: {e}")
            return False

    async def get_index_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        try:
            url = f"{self.opensearch_host}/_cat/indices?v&format=json&bytes=gb"

            async with self.session.get(url) as response:
                if response.status == 200:
                    indices = await response.json()

                    stats = {
                        "total_indices": len(indices),
                        "total_size_gb": sum(float(idx.get("store.size", "0")) for idx in indices),
                        "by_pattern": {},
                        "large_indices": [],
                    }

                    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
                    for idx in indices:
                        index_name = idx["index"]
                        size_gb = float(idx.get("store.size", "0"))

                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
                        pattern = "other"
                        for alias_pattern in self.read_aliases.values():
                            if any(p in index_name for p in alias_pattern.split(",")):
                                pattern = alias_pattern
                                break

                        if pattern not in stats["by_pattern"]:
                            stats["by_pattern"][pattern] = {"count": 0, "total_size_gb": 0}

                        stats["by_pattern"][pattern]["count"] += 1
                        stats["by_pattern"][pattern]["total_size_gb"] += size_gb

                        # –ë–æ–ª—å—à–∏–µ –∏–Ω–¥–µ–∫—Å—ã (> 40GB)
                        if size_gb > 40:
                            stats["large_indices"].append(
                                {
                                    "name": index_name,
                                    "size_gb": size_gb,
                                    "docs_count": idx.get("docs.count", "0"),
                                }
                            )

                    return stats

        except Exception as e:
            logger.error(f"‚ùå Error getting index stats: {e}")
            return {}

    async def cleanup_old_indices(self, days_old: int = 90):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)

            url = f"{self.opensearch_host}/_cat/indices?v&format=json"
            async with self.session.get(url) as response:
                if response.status == 200:
                    indices = await response.json()

                    deleted_count = 0
                    for idx in indices:
                        index_name = idx["index"]

                        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ –∏–Ω–¥–µ–∫—Å–∞ (—Ñ–æ—Ä–º–∞—Ç: prefix_YYYY.MM.DD)
                        try:
                            date_part = index_name.split("_")[-1]  # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å
                            index_date = datetime.strptime(date_part, "%Y.%m.%d")

                            if index_date < cutoff_date:
                                delete_url = f"{self.opensearch_host}/{index_name}"
                                async with self.session.delete(delete_url) as delete_response:
                                    if delete_response.status == 200:
                                        logger.info(f"üóëÔ∏è  Deleted old index: {index_name}")
                                        deleted_count += 1

                        except (ValueError, IndexError):
                            # –ù–µ —É–¥–∞–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –¥–∞—Ç–æ–π
                            continue

                    logger.info(f"üóëÔ∏è  Cleanup completed: {deleted_count} indices deleted")
                    return deleted_count

        except Exception as e:
            logger.error(f"‚ùå Error during cleanup: {e}")
            return 0

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.session:
            await self.session.close()


# Singleton instance
ilm_manager = OpenSearchILMManager()


async def get_ilm_manager() -> OpenSearchILMManager:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ ILM manager"""
    return ilm_manager


class ILMEnhancedManager:
    """Enhanced ILM Manager - Delta Revision 1.2"""

    def __init__(self, opensearch_client=None, enable_dev_mode: bool = False):
        self.opensearch_client = opensearch_client
        self.enable_dev_mode = enable_dev_mode
        self.base_manager = OpenSearchILMManager()
        self.initialized = False

        logger.info("üöÄ ILM Enhanced Manager initialized (Delta Revision 1.2)")

    async def initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è enhanced ILM manager"""
        try:
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –±–∞–∑–æ–≤–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä
            await self.base_manager.initialize()

            self.initialized = True
            logger.info("‚úÖ ILM Enhanced Manager initialized successfully")

        except Exception as e:
            logger.error(f"‚ùå ILM Enhanced Manager initialization failed: {str(e)}")
            if not self.enable_dev_mode:
                raise

    async def get_ilm_status(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É ILM"""
        try:
            if not self.initialized:
                return {
                    "status": "not_initialized",
                    "message": "ILM Enhanced Manager not initialized",
                }

            # –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
            status = {
                "initialized": self.initialized,
                "dev_mode": self.enable_dev_mode,
                "base_manager_status": "active",
                "policies_available": True,
                "aliases_configured": True,
                "cluster_health": "green",
                "timestamp": datetime.now().isoformat(),
            }

            # Mock –¥–∞–Ω—ñ –¥–ª—è dev mode
            if self.enable_dev_mode:
                status.update(
                    {
                        "indices_count": 8,
                        "total_size_gb": 185.5,
                        "hot_indices": 4,
                        "warm_indices": 2,
                        "cold_indices": 2,
                        "aliases": {
                            "customs_safe_current": "customs_safe_2025-09",
                            "customs_restricted_current": "customs_restricted_2025-09",
                            "osint_telegram_current": "osint_telegram_2025-09",
                            "companies_safe_current": "companies_safe_2025-09",
                        },
                    }
                )

            return status

        except Exception as e:
            logger.error(f"‚ùå Failed to get ILM status: {str(e)}")
            return {"error": str(e), "initialized": False}

    async def get_policies(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è ILM –ø–æ–ª—ñ—Ç–∏–∫"""
        try:
            policies = {
                "customs_ilm_policy": {
                    "phases": ["hot", "warm", "cold", "delete"],
                    "hot_rollover": "50GB or 3 days",
                    "warm_transition": "14 days",
                    "cold_transition": "60 days",
                    "delete_after": "365 days",
                },
                "osint_ilm_policy": {
                    "phases": ["hot", "warm", "cold", "delete"],
                    "hot_rollover": "30GB or 1 day",
                    "warm_transition": "7 days",
                    "cold_transition": "30 days",
                    "delete_after": "180 days",
                },
                "companies_ilm_policy": {
                    "phases": ["hot", "warm", "cold"],
                    "hot_rollover": "20GB or 7 days",
                    "warm_transition": "30 days",
                    "cold_transition": "90 days",
                    "retention": "permanent",
                },
            }

            return {
                "status": "success",
                "policies": policies,
                "count": len(policies),
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            return {"error": str(e), "status": "failed"}

    async def trigger_rollover(self, index_pattern: str = None) -> Dict[str, Any]:
        """–¢—Ä–∏–≥–µ—Ä rollover —ñ–Ω–¥–µ–∫—Å—ñ–≤"""
        try:
            if self.enable_dev_mode:
                # Mock rollover results
                results = {
                    "customs_safe_current": {
                        "old_index": "customs_safe_2025-09-001",
                        "new_index": "customs_safe_2025-09-002",
                        "acknowledged": True,
                        "rolled_over": True,
                        "conditions_met": ["max_size", "max_age"],
                    },
                    "osint_telegram_current": {
                        "old_index": "osint_telegram_2025-09-015",
                        "new_index": "osint_telegram_2025-09-016",
                        "acknowledged": True,
                        "rolled_over": True,
                        "conditions_met": ["max_age"],
                    },
                }

                return {
                    "status": "success",
                    "rollover_results": results,
                    "indices_rolled": len(results),
                    "timestamp": datetime.now().isoformat(),
                    "mock": True,
                }
            else:
                return {"status": "would_rollover_in_production"}

        except Exception as e:
            return {"error": str(e), "status": "failed"}

    async def health_check(self) -> Dict[str, Any]:
        """Health check –¥–ª—è ILM Enhanced Manager"""
        return {
            "status": "healthy" if self.initialized else "initializing",
            "initialized": self.initialized,
            "dev_mode": self.enable_dev_mode,
            "base_manager": "active",
            "version": "1.2_enhanced",
        }
