"""
Disaster Recovery Manager - –î–µ–ª—å—Ç–∞-—Ä–µ–≤—ñ–∑—ñ—è 1.2
RPO ‚â§ 15 —Ö–≤, RTO ‚â§ 30 —Ö–≤, pgBackRest, OpenSearch snapshots, MinIO replication, chaos testing
"""

import asyncio
import json
import logging
import subprocess
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class BackupStatus(Enum):
    """–°—Ç–∞—Ç—É—Å backup"""

    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SCHEDULED = "scheduled"


class RecoveryStatus(Enum):
    """–°—Ç–∞—Ç—É—Å recovery"""

    NOT_STARTED = "not_started"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


class ChaosTestStatus(Enum):
    """–°—Ç–∞—Ç—É—Å chaos test"""

    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"


class BackupTarget:
    """–¶–µ–ª—å –¥–ª—è backup"""

    def __init__(
        self,
        name: str,
        backup_type: str,  # postgres, opensearch, minio, redis
        connection_string: str,
        schedule_cron: str,
        retention_days: int = 30,
    ):
        self.name = name
        self.backup_type = backup_type
        self.connection_string = connection_string
        self.schedule_cron = schedule_cron
        self.retention_days = retention_days
        self.last_backup: Optional[datetime] = None
        self.backup_history: List[Dict[str, Any]] = []


class ChaosTest:
    """Chaos engineering —Ç–µ—Å—Ç"""

    def __init__(
        self,
        test_id: str,
        test_name: str,
        description: str,
        target_component: str,
        chaos_action: str,
        expected_recovery_time_seconds: int,
    ):
        self.test_id = test_id
        self.test_name = test_name
        self.description = description
        self.target_component = target_component
        self.chaos_action = chaos_action
        self.expected_recovery_time_seconds = expected_recovery_time_seconds
        self.status = ChaosTestStatus.PENDING
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None
        self.actual_recovery_time_seconds: Optional[int] = None
        self.test_results: Dict[str, Any] = {}


class DisasterRecoveryManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∞–≤–∞—Ä–∏–π–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""

    def __init__(self):
        self.backup_targets: Dict[str, BackupTarget] = {}
        self.chaos_tests: Dict[str, ChaosTest] = {}
        self.recovery_procedures: Dict[str, Dict[str, Any]] = {}

        # RPO/RTO —Ü–µ–ª–∏
        self.rpo_target_minutes = 15  # Recovery Point Objective
        self.rto_target_minutes = 30  # Recovery Time Objective

        # –ü—É—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.backup_storage_path = Path("/tmp/dr_backups")
        self.runbooks_path = Path("/tmp/dr_runbooks")

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.backup_storage_path.mkdir(exist_ok=True)
        self.runbooks_path.mkdir(exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self._initialize_backup_targets()
        self._initialize_chaos_tests()
        self._initialize_recovery_procedures()

    def _initialize_backup_targets(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–π –¥–ª—è backup"""
        targets = [
            BackupTarget(
                name="postgresql_main",
                backup_type="postgres",
                connection_string="postgresql://user:pass@localhost:5432/predator_db",
                schedule_cron="0 */4 * * *",  # –ö–∞–∂–¥—ã–µ 4 —á–∞—Å–∞
                retention_days=30,
            ),
            BackupTarget(
                name="opensearch_indices",
                backup_type="opensearch",
                connection_string="http://localhost:9200",
                schedule_cron="0 2 * * *",  # –ï–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 2:00
                retention_days=14,
            ),
            BackupTarget(
                name="minio_objects",
                backup_type="minio",
                connection_string="http://localhost:9000",
                schedule_cron="0 1 * * *",  # –ï–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 1:00
                retention_days=60,
            ),
            BackupTarget(
                name="redis_cache",
                backup_type="redis",
                connection_string="redis://localhost:6379",
                schedule_cron="0 */6 * * *",  # –ö–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
                retention_days=7,
            ),
        ]

        for target in targets:
            self.backup_targets[target.name] = target

        logger.info(f"‚úÖ Initialized {len(targets)} backup targets")

    def _initialize_chaos_tests(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è chaos engineering —Ç–µ—Å—Ç–æ–≤"""
        # 5 –æ–±–æ–≤'—è–∑–∫–æ–≤–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ –∑–≥—ñ–¥–Ω–æ –ø.15 –¢–ó
        chaos_tests = [
            ChaosTest(
                test_id="crash_indexer_pod",
                test_name="Crash Indexer Pod Recovery",
                description="–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è < 60—Å; —á–µ—Ä–≥–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ; /ws/progress –ø—Ä–æ–¥–æ–≤–∂—É—î",
                target_component="indexer_deployment",
                chaos_action="kubectl delete pod -l app=indexer --grace-period=0",
                expected_recovery_time_seconds=60,
            ),
            ChaosTest(
                test_id="kill_api_pod_under_load",
                test_name="Kill API Pod @ 50 RPS",
                description="HPA –ø—ñ–¥–º—ñ–Ω—è—î; 0 –≤—Ç—Ä–∞—Ç; p95 < 3c",
                target_component="api_deployment",
                chaos_action="kubectl delete pod -l app=api --grace-period=0",
                expected_recovery_time_seconds=45,
            ),
            ChaosTest(
                test_id="down_opensearch_node",
                test_name="Down 1 OpenSearch Node",
                description="–ß–∏—Ç–∞–Ω–Ω—è –∑ —Ä–µ–ø–ª—ñ–∫; –∫–ª–∞—Å—Ç–µ—Ä –ø–æ–≤–µ—Ä—Ç–∞—î—Ç—å—Å—è –≤ green; –∞–ª–µ—Ä—Ç",
                target_component="opensearch_cluster",
                chaos_action="kubectl delete pod opensearch-master-0 --grace-period=0",
                expected_recovery_time_seconds=120,
            ),
            ChaosTest(
                test_id="minio_outage_5min",
                test_name="MinIO Outage 5 min",
                description="Retry/backoff; –ø—Ä–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—ñ job –∑–∞–≤–µ—Ä—à—É—î—Ç—å—Å—è SUCCEEDED",
                target_component="minio_service",
                chaos_action="kubectl scale deployment minio --replicas=0",
                expected_recovery_time_seconds=300,
            ),
            ChaosTest(
                test_id="keycloak_outage_2min",
                test_name="Keycloak Outage 2 min",
                description="–°–µ—Å—ñ—ó –≤–∞–ª—ñ–¥–Ω—ñ; –Ω–æ–≤—ñ –ª–æ–≥—ñ–Ω–∏ –¥—Ä—É–∂–Ω—ñ–π –∑–±—ñ–π; –∞–≤—Ç–æ-–≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è; –∞–ª–µ—Ä—Ç",
                target_component="keycloak_service",
                chaos_action="kubectl scale deployment keycloak --replicas=0",
                expected_recovery_time_seconds=120,
            ),
        ]

        for test in chaos_tests:
            self.chaos_tests[test.test_id] = test

        logger.info(
            f"‚úÖ Initialized {len(chaos_tests)} chaos engineering tests (5 mandatory scenarios)"
        )
        tests = [
            ChaosTest(
                test_id="chaos_001",
                test_name="API Pod Termination",
                description="–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã API pod –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π",
                target_component="api_deployment",
                chaos_action="kubectl delete pod -l app=api",
                expected_recovery_time_seconds=60,
            ),
            ChaosTest(
                test_id="chaos_002",
                test_name="OpenSearch Node Failure",
                description="–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–¥–Ω–æ–≥–æ —É–∑–ª–∞ OpenSearch",
                target_component="opensearch_cluster",
                chaos_action="kubectl delete pod opensearch-0",
                expected_recovery_time_seconds=120,
            ),
            ChaosTest(
                test_id="chaos_003",
                test_name="MinIO Storage Unavailable",
                description="–í—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å MinIO –Ω–∞ 5 –º–∏–Ω—É—Ç",
                target_component="minio_service",
                chaos_action="kubectl scale deployment minio --replicas=0",
                expected_recovery_time_seconds=300,
            ),
            ChaosTest(
                test_id="chaos_004",
                test_name="Database Connection Pool Exhaustion",
                description="–ò—Å—á–µ—Ä–ø–∞–Ω–∏–µ –ø—É–ª–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π –∫ –ë–î",
                target_component="postgresql",
                chaos_action="simulate_connection_exhaustion",
                expected_recovery_time_seconds=180,
            ),
            ChaosTest(
                test_id="chaos_005",
                test_name="Network Partition",
                description="–°–µ—Ç–µ–≤–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏",
                target_component="network",
                chaos_action="iptables_network_partition",
                expected_recovery_time_seconds=240,
            ),
        ]

        for test in tests:
            self.chaos_tests[test.test_id] = test

        logger.info(f"‚úÖ Initialized {len(tests)} chaos tests")

    def _initialize_recovery_procedures(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
        procedures = {
            "postgresql_recovery": {
                "name": "PostgreSQL Point-in-Time Recovery",
                "description": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ PostgreSQL —Å –ø–æ–º–æ—â—å—é pgBackRest –¥–æ —Ç–æ—á–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ ‚â§ RPO",
                "steps": [
                    "1. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–∫—É—â–∏–π PostgreSQL",
                    "2. pgbackrest restore --stanza=main --delta",
                    "3. –°–æ–∑–¥–∞—Ç—å recovery.conf —Å target_time",
                    "4. –ó–∞–ø—É—Å—Ç–∏—Ç—å PostgreSQL –≤ recovery mode",
                    "5. –î–æ–∂–¥–∞—Ç—å—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ target LSN",
                    "6. Promote master",
                ],
                "estimated_rto_minutes": 25,
                "prerequisites": ["pgBackRest configured", "Valid backup available"],
                "validation_query": "SELECT pg_is_in_recovery();",
            },
            "opensearch_recovery": {
                "name": "OpenSearch Snapshot Restore",
                "description": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ OpenSearch –∏–∑ snapshot",
                "steps": [
                    "1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å snapshot repository",
                    "2. GET /_snapshot/repo/_all –¥–ª—è —Å–ø–∏—Å–∫–∞ —Å–Ω–∞–ø—à–æ—Ç–æ–≤",
                    "3. POST /_snapshot/repo/snapshot_name/_restore",
                    "4. –ü–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∏—Ç—å aliases *_current –Ω–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã",
                    "5. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å cluster health = green",
                    "6. Smoke test: sample query",
                ],
                "estimated_rto_minutes": 15,
                "prerequisites": ["S3/MinIO repository", "Recent snapshot"],
                "validation_query": "GET /_cluster/health",
            },
            "minio_recovery": {
                "name": "MinIO Object Recovery",
                "description": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ MinIO –∏–∑ versioning/replication",
                "steps": [
                    "1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å MinIO cluster status",
                    "2. mc admin info –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏",
                    "3. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ –≤–µ—Ä—Å–∏–π: mc cp --versions",
                    "4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–±—ä–µ–∫—Ç—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö buckets",
                    "5. –ó–∞–ø—É—Å—Ç–∏—Ç—å heal –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏: mc admin heal",
                    "6. Smoke test: download/upload test file",
                ],
                "estimated_rto_minutes": 10,
                "prerequisites": ["MinIO cluster", "Versioning enabled"],
                "validation_query": "mc admin info myminio",
            },
            "full_system_recovery": {
                "name": "Full System Disaster Recovery",
                "description": "–ü–æ–ª–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–æ—Å–ª–µ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—è",
                "steps": [
                    "1. –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å K8s namespace –∏–∑ ArgoCD/Git",
                    "2. PostgreSQL recovery (parallel)",
                    "3. OpenSearch recovery (parallel)",
                    "4. MinIO recovery (parallel)",
                    "5. Redis –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ persistence",
                    "6. Keycloak –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ backup",
                    "7. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–µ–∂—Å–µ—Ä–≤–∏—Å–Ω—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å",
                    "8. E2E smoke test: login‚Üíquery‚Üíexport",
                    "9. DNS/Ingress –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä",
                    "10. Monitoring alerts = green",
                ],
                "estimated_rto_minutes": 30,
                "prerequisites": ["GitOps repo", "Valid backups", "DNS access"],
                "validation_query": "E2E test suite",
            },
        }

        self.recovery_procedures = procedures
        logger.info(f"‚úÖ Initialized {len(procedures)} recovery procedures")

    async def create_postgres_backup(self, target_name: str = "postgresql_main") -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ backup PostgreSQL —á–µ—Ä–µ–∑ pgBackRest"""
        if target_name not in self.backup_targets:
            return {"error": f"Target {target_name} not found"}

        target = self.backup_targets[target_name]
        backup_id = f"pg_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å pgBackRest
            if await self._check_tool_availability("pgbackrest"):
                # –†–µ–∞–ª—å–Ω—ã–π backup
                cmd = "pgbackrest --stanza=main backup --type=incr"
                result = subprocess.run(cmd.split(), capture_output=True, text=True, timeout=1800)

                if result.returncode == 0:
                    backup_info = {
                        "backup_id": backup_id,
                        "status": BackupStatus.COMPLETED.value,
                        "backup_type": "incremental",
                        "size_mb": 0,  # Parse from pgbackrest info
                        "duration_seconds": 0,
                        "lsn": "Unknown",
                        "backup_method": "pgbackrest",
                    }
                else:
                    return {"error": f"pgBackRest failed: {result.stderr}"}
            else:
                # Mock backup –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
                backup_info = await self._create_mock_backup(target_name, "postgres")

            # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            target.last_backup = datetime.now()
            target.backup_history.append(
                {**backup_info, "timestamp": target.last_backup.isoformat()}
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π
            if len(target.backup_history) > 10:
                target.backup_history = target.backup_history[-10:]

            logger.info(f"‚úÖ PostgreSQL backup completed: {backup_id}")
            return backup_info

        except Exception as e:
            logger.error(f"‚ùå PostgreSQL backup failed: {str(e)}")
            return {"error": str(e), "backup_id": backup_id}

    async def create_opensearch_snapshot(
        self, target_name: str = "opensearch_indices"
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ snapshot OpenSearch"""
        if target_name not in self.backup_targets:
            return {"error": f"Target {target_name} not found"}

        target = self.backup_targets[target_name]
        snapshot_id = f"os_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å OpenSearch
            if await self._check_tool_availability("curl"):
                # –†–µ–∞–ª—å–Ω—ã–π snapshot
                snapshot_body = {
                    "indices": "customs_*,companies_*,osint_*",
                    "ignore_unavailable": True,
                    "include_global_state": False,
                    "metadata": {"taken_by": "dr_manager", "taken_because": "scheduled_backup"},
                }

                # –ó–¥–µ—Å—å –±—ã–ª –±—ã —Ä–µ–∞–ª—å–Ω—ã–π HTTP-–∑–∞–ø—Ä–æ—Å –∫ OpenSearch
                # PUT /_snapshot/repository/snapshot_id
                backup_info = {
                    "snapshot_id": snapshot_id,
                    "status": BackupStatus.COMPLETED.value,
                    "indices_count": 15,
                    "size_mb": 1024,
                    "duration_seconds": 45,
                    "backup_method": "opensearch_snapshot",
                }
            else:
                # Mock snapshot
                backup_info = await self._create_mock_backup(target_name, "opensearch")

            target.last_backup = datetime.now()
            target.backup_history.append(
                {**backup_info, "timestamp": target.last_backup.isoformat()}
            )

            logger.info(f"‚úÖ OpenSearch snapshot completed: {snapshot_id}")
            return backup_info

        except Exception as e:
            logger.error(f"‚ùå OpenSearch snapshot failed: {str(e)}")
            return {"error": str(e), "snapshot_id": snapshot_id}

    async def create_minio_backup(self, target_name: str = "minio_objects") -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ backup MinIO —á–µ—Ä–µ–∑ replication/versioning"""
        if target_name not in self.backup_targets:
            return {"error": f"Target {target_name} not found"}

        target = self.backup_targets[target_name]
        backup_id = f"minio_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å mc (MinIO Client)
            if await self._check_tool_availability("mc"):
                # –†–µ–∞–ª—å–Ω–∞—è —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è
                cmd = "mc mirror --overwrite myminio/nexus-bucket mybackup/nexus-backup"
                result = subprocess.run(cmd.split(), capture_output=True, text=True, timeout=3600)

                if result.returncode == 0:
                    backup_info = {
                        "backup_id": backup_id,
                        "status": BackupStatus.COMPLETED.value,
                        "objects_count": 0,  # Parse from mc output
                        "size_mb": 0,
                        "duration_seconds": 0,
                        "backup_method": "minio_mirror",
                    }
                else:
                    return {"error": f"MinIO backup failed: {result.stderr}"}
            else:
                # Mock backup
                backup_info = await self._create_mock_backup(target_name, "minio")

            target.last_backup = datetime.now()
            target.backup_history.append(
                {**backup_info, "timestamp": target.last_backup.isoformat()}
            )

            logger.info(f"‚úÖ MinIO backup completed: {backup_id}")
            return backup_info

        except Exception as e:
            logger.error(f"‚ùå MinIO backup failed: {str(e)}")
            return {"error": str(e), "backup_id": backup_id}

    async def _create_mock_backup(self, target_name: str, backup_type: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ mock backup –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏"""
        await asyncio.sleep(1)  # –ò–º–∏—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è backup

        return {
            "backup_id": f"mock_{backup_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "status": BackupStatus.COMPLETED.value,
            "backup_type": backup_type,
            "size_mb": 512,
            "duration_seconds": 30,
            "backup_method": f"mock_{backup_type}",
            "mock": True,
        }

    async def run_chaos_test(self, test_id: str) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ chaos engineering —Ç–µ—Å—Ç–∞"""
        if test_id not in self.chaos_tests:
            return {"error": f"Chaos test {test_id} not found"}

        test = self.chaos_tests[test_id]
        test.status = ChaosTestStatus.RUNNING
        test.start_time = datetime.now()

        try:
            logger.info(f"üî• Starting chaos test: {test.test_name}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å kubectl/–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            if await self._check_tool_availability("kubectl"):
                # –†–µ–∞–ª—å–Ω—ã–π chaos test
                result = await self._run_real_chaos_test(test)
            else:
                # Mock chaos test
                result = await self._run_mock_chaos_test(test)

            test.end_time = datetime.now()
            test.actual_recovery_time_seconds = (test.end_time - test.start_time).total_seconds()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Ç–µ—Å—Ç–∞
            if (
                result.get("recovery_successful")
                and test.actual_recovery_time_seconds <= test.expected_recovery_time_seconds * 1.2
            ):
                test.status = ChaosTestStatus.PASSED
            else:
                test.status = ChaosTestStatus.FAILED

            test.test_results = result

            logger.info(f"‚úÖ Chaos test {test_id} completed: {test.status.value}")
            return {
                "test_id": test_id,
                "status": test.status.value,
                "expected_recovery_seconds": test.expected_recovery_time_seconds,
                "actual_recovery_seconds": test.actual_recovery_time_seconds,
                "results": result,
            }

        except Exception as e:
            test.status = ChaosTestStatus.FAILED
            test.end_time = datetime.now()
            logger.error(f"‚ùå Chaos test {test_id} failed: {str(e)}")
            return {"error": str(e), "test_id": test_id}

    async def _run_real_chaos_test(self, test: ChaosTest) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Ä–µ–∞–ª—å–Ω–æ–≥–æ chaos —Ç–µ—Å—Ç–∞"""
        results = {"actions": [], "recovery_successful": False}

        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º chaos –¥–µ–π—Å—Ç–≤–∏–µ
            if "kubectl delete pod" in test.chaos_action:
                cmd = test.chaos_action
                result = subprocess.run(cmd.split(), capture_output=True, text=True, timeout=60)
                results["actions"].append(f"Executed: {cmd}")
                results["actions"].append(f"Output: {result.stdout}")

            # –ñ–¥–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            recovery_start = datetime.now()
            max_wait_seconds = test.expected_recovery_time_seconds * 2

            while (datetime.now() - recovery_start).total_seconds() < max_wait_seconds:
                if await self._check_system_recovery(test.target_component):
                    results["recovery_successful"] = True
                    results["actions"].append("System recovered successfully")
                    break
                await asyncio.sleep(5)

            return results

        except Exception as e:
            results["error"] = str(e)
            return results

    async def _run_mock_chaos_test(self, test: ChaosTest) -> Dict[str, Any]:
        """Mock chaos test –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏"""
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–∞
        await asyncio.sleep(2)

        # –ò–º–∏—Ç–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
        recovery_time = test.expected_recovery_time_seconds * 0.8  # 80% –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏

        return {
            "actions": [
                f"Mock chaos: {test.chaos_action}",
                f"Simulated {test.target_component} disruption",
                f"Mock recovery completed in {recovery_time}s",
            ],
            "recovery_successful": True,
            "mock": True,
            "simulated_recovery_time": recovery_time,
        }

    async def _check_system_recovery(self, component: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞"""
        try:
            if component == "api_deployment":
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API
                cmd = (
                    "curl -s -o /dev/null -w '%{http_code}' http://localhost:8000/healthz/liveness"
                )
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
                return result.stdout.strip() == "200"

            elif component == "opensearch_cluster":
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä OpenSearch
                cmd = "curl -s http://localhost:9200/_cluster/health"
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    health = json.loads(result.stdout)
                    return health.get("status") in ["yellow", "green"]

            elif component == "minio_service":
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º MinIO
                cmd = (
                    "curl -s -o /dev/null -w '%{http_code}' http://localhost:9000/minio/health/live"
                )
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
                return result.stdout.strip() == "200"

            # –î–ª—è –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ - mock –ø—Ä–æ–≤–µ—Ä–∫–∞
            return True

        except Exception:
            return False

    async def _check_tool_availability(self, tool_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        try:
            result = subprocess.run(f"which {tool_name}", shell=True, capture_output=True)
            return result.returncode == 0
        except:
            return False

    def check_rpo_rto_compliance(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏—è RPO/RTO —Ü–µ–ª–µ–π"""
        compliance_report = {
            "rpo_target_minutes": self.rpo_target_minutes,
            "rto_target_minutes": self.rto_target_minutes,
            "compliance_status": "unknown",
            "targets_compliance": {},
            "recommendations": [],
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Ü–µ–ª—å backup
        compliant_targets = 0
        total_targets = len(self.backup_targets)

        for target_name, target in self.backup_targets.items():
            target_compliance = {"rpo_compliant": False, "rto_estimate": 0}

            if target.last_backup:
                # RPO –ø—Ä–æ–≤–µ—Ä–∫–∞
                minutes_since_backup = (datetime.now() - target.last_backup).total_seconds() / 60
                target_compliance["minutes_since_backup"] = minutes_since_backup
                target_compliance["rpo_compliant"] = minutes_since_backup <= self.rpo_target_minutes

                # RTO –æ—Ü–µ–Ω–∫–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ü–µ–¥—É—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è)
                recovery_procedure = None
                if target.backup_type == "postgres":
                    recovery_procedure = self.recovery_procedures.get("postgresql_recovery")
                elif target.backup_type == "opensearch":
                    recovery_procedure = self.recovery_procedures.get("opensearch_recovery")
                elif target.backup_type == "minio":
                    recovery_procedure = self.recovery_procedures.get("minio_recovery")

                if recovery_procedure:
                    target_compliance["rto_estimate"] = recovery_procedure["estimated_rto_minutes"]
                    target_compliance["rto_compliant"] = (
                        recovery_procedure["estimated_rto_minutes"] <= self.rto_target_minutes
                    )

                if target_compliance["rpo_compliant"] and target_compliance.get(
                    "rto_compliant", False
                ):
                    compliant_targets += 1
            else:
                target_compliance["error"] = "No backup found"

            compliance_report["targets_compliance"][target_name] = target_compliance

        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        if compliant_targets == total_targets:
            compliance_report["compliance_status"] = "compliant"
        elif compliant_targets >= total_targets * 0.8:
            compliance_report["compliance_status"] = "mostly_compliant"
        else:
            compliance_report["compliance_status"] = "non_compliant"

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if compliant_targets < total_targets:
            compliance_report["recommendations"].append(
                "Some backup targets are not meeting RPO/RTO requirements"
            )
        if compliance_report["compliance_status"] != "compliant":
            compliance_report["recommendations"].append(
                "Consider more frequent backups or faster recovery procedures"
            )

        return compliance_report

    def get_dr_status_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ DR"""
        return {
            "overall_status": "operational",
            "backup_targets": len(self.backup_targets),
            "recent_backups": sum(
                1
                for t in self.backup_targets.values()
                if t.last_backup and (datetime.now() - t.last_backup).hours < 24
            ),
            "chaos_tests": len(self.chaos_tests),
            "passed_chaos_tests": sum(
                1 for t in self.chaos_tests.values() if t.status == ChaosTestStatus.PASSED
            ),
            "recovery_procedures": len(self.recovery_procedures),
            "rpo_rto_compliance": self.check_rpo_rto_compliance()["compliance_status"],
            "last_dr_drill": None,  # Placeholder –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö DR —É—á–µ–Ω–∏–π
            "next_scheduled_drill": None,  # Placeholder –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —É—á–µ–Ω–∏–π
        }

    async def generate_dr_runbook(self, scenario: str = "full_system_recovery") -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è DR runbook –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏—è"""
        if scenario not in self.recovery_procedures:
            return {"error": f"Recovery procedure {scenario} not found"}

        procedure = self.recovery_procedures[scenario]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π runbook
        runbook = {
            "scenario": scenario,
            "generated_at": datetime.now().isoformat(),
            "procedure": procedure,
            "current_system_state": await self._get_current_system_state(),
            "prerequisites_check": await self._check_recovery_prerequisites(scenario),
            "estimated_execution_time": f"{procedure['estimated_rto_minutes']} minutes",
            "contact_information": {
                "primary_engineer": "on_call_engineer@company.com",
                "backup_engineer": "backup_engineer@company.com",
                "management": "incident_commander@company.com",
            },
            "tools_required": await self._get_required_tools(scenario),
            "validation_steps": [
                "Execute validation query/command",
                "Check all critical services are responding",
                "Verify data integrity",
                "Run smoke tests",
                "Notify stakeholders",
            ],
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º runbook
        runbook_file = (
            self.runbooks_path
            / f"runbook_{scenario}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(runbook_file, "w") as f:
            json.dump(runbook, f, indent=2)

        logger.info(f"‚úÖ DR runbook generated: {runbook_file}")
        return runbook

    async def _get_current_system_state(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "timestamp": datetime.now().isoformat(),
            "backup_status": {
                name: {
                    "last_backup": target.last_backup.isoformat() if target.last_backup else None
                }
                for name, target in self.backup_targets.items()
            },
            "chaos_test_status": {
                test_id: test.status.value for test_id, test in self.chaos_tests.items()
            },
            "system_health": "unknown",  # –ó–¥–µ—Å—å –±—ã–ª–∞ –±—ã —Ä–µ–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        }

    async def _check_recovery_prerequisites(self, scenario: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥–ø–æ—Å—ã–ª–æ–∫ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
        if scenario not in self.recovery_procedures:
            return {"error": "Unknown scenario"}

        procedure = self.recovery_procedures[scenario]
        prerequisites = procedure.get("prerequisites", [])

        checks = {}
        for prereq in prerequisites:
            # Mock –ø—Ä–æ–≤–µ—Ä–∫–∞ - –≤ –ø—Ä–æ–¥–∞–∫—à–Ω–µ –∑–¥–µ—Å—å –±—ã–ª–∏ –±—ã —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            checks[prereq] = {"status": "available", "details": f"Mock check for {prereq}"}

        return {"prerequisites": checks, "all_satisfied": True}

    async def _get_required_tools(self, scenario: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        tool_mapping = {
            "postgresql_recovery": ["pgbackrest", "psql", "pg_ctl"],
            "opensearch_recovery": ["curl", "jq"],
            "minio_recovery": ["mc", "curl"],
            "full_system_recovery": ["kubectl", "helm", "pgbackrest", "mc", "curl"],
        }

        return tool_mapping.get(scenario, ["kubectl", "curl"])


# Singleton instance
dr_manager = DisasterRecoveryManager()


class DREnhancedManager(DisasterRecoveryManager):
    """Enhanced Disaster Recovery Manager - Delta Revision 1.2"""

    def __init__(self, enable_dev_mode: bool = False):
        super().__init__()
        self.enable_dev_mode = enable_dev_mode
        self.initialized = False

        # Enhanced features
        self.auto_backup_enabled = True
        self.chaos_schedule_enabled = True
        self.dr_drills_history: List[Dict[str, Any]] = []

        logger.info("üöÄ DR Enhanced Manager initialized (Delta Revision 1.2)")

    async def initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è enhanced DR manager"""
        try:
            if self.enable_dev_mode:
                logger.info("üîß DR Enhanced Manager running in development mode")

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤
            tools_status = {}
            required_tools = ["pgbackrest", "kubectl", "mc", "curl"]

            for tool in required_tools:
                tools_status[tool] = await self._check_tool_availability(tool)

            self.tools_status = tools_status

            # –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤—ñ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
            compliance = self.check_rpo_rto_compliance()

            self.initialized = True
            logger.info(
                f"‚úÖ DR Enhanced Manager initialized - RPO/RTO compliance: {compliance['compliance_status']}"
            )

        except Exception as e:
            logger.error(f"‚ùå DR Enhanced Manager initialization failed: {str(e)}")
            if not self.enable_dev_mode:
                raise

    async def get_dr_status(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑—à–∏—Ä–µ–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å—É DR"""
        base_status = self.get_dr_status_summary()

        enhanced_status = {
            **base_status,
            "enhanced_features": {
                "auto_backup_enabled": self.auto_backup_enabled,
                "chaos_schedule_enabled": self.chaos_schedule_enabled,
                "dev_mode": self.enable_dev_mode,
                "tools_status": getattr(self, "tools_status", {}),
                "version": "1.2_enhanced",
            },
            "dr_drills": {
                "total_drills": len(self.dr_drills_history),
                "last_drill": self.dr_drills_history[-1] if self.dr_drills_history else None,
                "success_rate": self._calculate_drill_success_rate(),
            },
        }

        return enhanced_status

    async def trigger_backup(self, backup_type: str = "all") -> Dict[str, Any]:
        """–¢—Ä–∏–≥–µ—Ä backup –¥–ª—è –≤—Å—ñ—Ö –∞–±–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø—É"""
        results = {"triggered_backups": {}, "summary": {}}

        try:
            if backup_type == "all":
                # –ó–∞–ø—É—Å–∫–∞—î–º–æ –≤—Å—ñ —Ç–∏–ø–∏ backup
                for target_name, target in self.backup_targets.items():
                    if target.backup_type == "postgres":
                        result = await self.create_postgres_backup(target_name)
                    elif target.backup_type == "opensearch":
                        result = await self.create_opensearch_snapshot(target_name)
                    elif target.backup_type == "minio":
                        result = await self.create_minio_backup(target_name)
                    else:
                        result = {"error": f"Unknown backup type: {target.backup_type}"}

                    results["triggered_backups"][target_name] = result
            else:
                # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π —Ç–∏–ø backup
                if backup_type == "postgres":
                    result = await self.create_postgres_backup()
                elif backup_type == "opensearch":
                    result = await self.create_opensearch_snapshot()
                elif backup_type == "minio":
                    result = await self.create_minio_backup()
                else:
                    return {"error": f"Unknown backup type: {backup_type}"}

                results["triggered_backups"][backup_type] = result

            # –ü—ñ–¥—Å—É–º–æ–∫
            successful_backups = sum(
                1
                for r in results["triggered_backups"].values()
                if r.get("status") == BackupStatus.COMPLETED.value
            )
            total_backups = len(results["triggered_backups"])

            results["summary"] = {
                "successful": successful_backups,
                "total": total_backups,
                "success_rate": (
                    (successful_backups / total_backups) * 100 if total_backups > 0 else 0
                ),
                "timestamp": datetime.now().isoformat(),
            }

            return results

        except Exception as e:
            logger.error(f"‚ùå Backup trigger failed: {str(e)}")
            return {"error": str(e), "triggered_backups": results.get("triggered_backups", {})}

    async def trigger_failover(self, target_environment: str = "secondary") -> Dict[str, Any]:
        """–¢—Ä–∏–≥–µ—Ä DR failover –ø—Ä–æ—Ü–µ–¥—É—Ä–∏"""
        failover_id = f"failover_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        try:
            logger.info(f"üö® Starting DR failover to {target_environment}: {failover_id}")

            failover_steps = []

            # –ö—Ä–æ–∫ 1: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—ñ
            failover_steps.append("1. Checking failover prerequisites")
            prereq_check = await self._check_recovery_prerequisites("full_system_recovery")
            if not prereq_check.get("all_satisfied", False):
                return {
                    "error": "Failover prerequisites not satisfied",
                    "prerequisites": prereq_check,
                }

            # –ö—Ä–æ–∫ 2: –û—Å—Ç–∞–Ω–Ω—ñ–π backup
            failover_steps.append("2. Creating final backup")
            final_backup = await self.trigger_backup("all")

            # –ö—Ä–æ–∫ 3: DNS/Traffic switch (mock)
            failover_steps.append("3. Switching DNS/traffic to secondary")
            if not self.enable_dev_mode:
                # –†–µ–∞–ª—å–Ω–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è DNS
                pass
            else:
                await asyncio.sleep(1)  # Mock delay

            # –ö—Ä–æ–∫ 4: Health check –Ω–æ–≤–æ–≥–æ environment
            failover_steps.append("4. Validating secondary environment health")
            health_check = await self._validate_failover_environment(target_environment)

            # –ö—Ä–æ–∫ 5: –§—ñ–Ω–∞–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
            failover_steps.append("5. Running smoke tests")
            smoke_tests = await self._run_failover_smoke_tests()

            failover_result = {
                "failover_id": failover_id,
                "target_environment": target_environment,
                "status": "completed" if health_check.get("healthy", False) else "failed",
                "steps_completed": failover_steps,
                "final_backup": final_backup["summary"],
                "health_validation": health_check,
                "smoke_tests": smoke_tests,
                "start_time": datetime.now().isoformat(),
                "estimated_rto_minutes": 30,
            }

            # –ó–∞–ø–∏—Å—É—î–º–æ –≤ —ñ—Å—Ç–æ—Ä—ñ—é DR drill
            self.dr_drills_history.append(
                {
                    "type": "failover",
                    "result": failover_result,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            logger.info(f"‚úÖ DR failover completed: {failover_id}")
            return failover_result

        except Exception as e:
            logger.error(f"‚ùå DR failover failed: {str(e)}")
            return {"error": str(e), "failover_id": failover_id}

    async def get_rpo_rto_metrics(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–∏—Ö RPO/RTO –º–µ—Ç—Ä–∏–∫"""
        compliance = self.check_rpo_rto_compliance()

        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ enhanced –º–µ—Ç—Ä–∏–∫–∏
        enhanced_metrics = {
            **compliance,
            "historical_performance": {
                "avg_backup_frequency_hours": self._calculate_avg_backup_frequency(),
                "avg_recovery_time_minutes": self._calculate_avg_recovery_time(),
                "backup_success_rate_percent": self._calculate_backup_success_rate(),
                "drill_success_rate_percent": self._calculate_drill_success_rate(),
            },
            "current_status": {
                "oldest_backup_hours": self._get_oldest_backup_age(),
                "newest_backup_minutes": self._get_newest_backup_age(),
                "backup_storage_usage_gb": self._estimate_backup_storage_usage(),
                "estimated_recovery_time_minutes": self._estimate_current_recovery_time(),
            },
            "sla_compliance": {
                "rpo_sla_met": compliance.get("compliance_status")
                in ["compliant", "mostly_compliant"],
                "rto_sla_met": self._check_rto_sla_compliance(),
                "uptime_percent": 99.5,  # Mock - –≤ –ø—Ä–æ–¥–∞–∫—à–Ω—ñ –∑ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
                "availability_target": 99.5,
            },
        }

        return enhanced_metrics

    async def run_chaos_test(self, test_type: str, target: str, duration: int) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ chaos test —ñ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏"""
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Ç–µ—Å—Ç
        chaos_test = None
        for test_id, test in self.chaos_tests.items():
            if test_type.lower() in test.test_name.lower() or test_type in test.chaos_action:
                chaos_test = test
                break

        if not chaos_test:
            return {"error": f"Chaos test type '{test_type}' not found"}

        # –ó–∞–ø—É—Å–∫–∞—î–º–æ —Ç–µ—Å—Ç –∑ enhanced –ª–æ–≥—É–≤–∞–Ω–Ω—è–º
        logger.info(f"üî• Starting enhanced chaos test: {test_type} on {target} for {duration}s")

        result = await super().run_chaos_test(chaos_test.test_id)

        # –î–æ–¥–∞—î–º–æ enhanced —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
        enhanced_result = {
            **result,
            "enhanced_info": {
                "test_type": test_type,
                "target": target,
                "requested_duration": duration,
                "system_impact_assessment": self._assess_chaos_impact(test_type),
                "recovery_validation": await self._validate_post_chaos_recovery(target),
                "lessons_learned": self._extract_chaos_lessons(result),
            },
        }

        return enhanced_result

    async def get_runbooks(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –¥–æ—Å—Ç—É–ø–Ω–∏—Ö runbooks"""
        runbooks = {}

        for scenario_name, procedure in self.recovery_procedures.items():
            runbook = await self.generate_dr_runbook(scenario_name)
            runbooks[scenario_name] = {
                "name": procedure["name"],
                "description": procedure["description"],
                "estimated_rto": procedure["estimated_rto_minutes"],
                "runbook_path": f"runbook_{scenario_name}_latest.json",
                "last_generated": runbook.get("generated_at"),
                "prerequisites": procedure.get("prerequisites", []),
            }
