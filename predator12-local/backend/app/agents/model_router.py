#!/usr/bin/env python3
"""
ğŸ¤– Specialized Model Router for Predator Analytics
Ğ Ğ¾Ğ·ÑƒĞ¼Ğ½Ğ¸Ğ¹ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ·Ğ¿Ğ¾Ğ´Ñ–Ğ»Ñƒ 58 Ğ±ĞµĞ·ĞºĞ¾ÑˆÑ‚Ğ¾Ğ²Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° ÑĞ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ”Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ñ–Ğ²
"""

import random
import yaml
from datetime import datetime
from enum import Enum
from typing import Any

class ModelTier(Enum):
    """Ğ Ñ–Ğ²Ğ½Ñ– Ğ¿Ğ¾Ñ‚ÑƒĞ¶Ğ½Ğ¾ÑÑ‚Ñ– Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
    FLAGSHIP = "flagship"        # ĞĞ°Ğ¹Ğ¿Ğ¾Ñ‚ÑƒĞ¶Ğ½Ñ–ÑˆÑ– Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
    PREMIUM = "premium"          # Ğ’Ğ¸ÑĞ¾ĞºĞ¾ÑĞºÑ–ÑĞ½Ñ– Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–  
    STANDARD = "standard"        # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ– Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
    EFFICIENT = "efficient"      # Ğ•Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ– ÑˆĞ²Ğ¸Ğ´ĞºÑ– Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
    SPECIALIZED = "specialized"  # Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–

class TaskComplexity(Enum):
    """Ğ¡ĞºĞ»Ğ°Ğ´Ğ½Ñ–ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡"""
    SIMPLE = "simple"
    MEDIUM = "medium" 
    COMPLEX = "complex"
    CRITICAL = "critical"

class AgentType(Enum):
    """Ğ¢Ğ¸Ğ¿Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ–Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸"""
    ANOMALY = "AnomalyAgent"
    FORECAST = "ForecastAgent" 
    GRAPH = "GraphIntelligenceAgent"
    DATASET = "DatasetAgent"
    SECURITY = "SecurityAgent"
    SELFHEALING = "SelfHealingAgent"
    AUTOIMPROVE = "AutoImproveAgent"

class ModelRouter:
    """Ğ Ğ¾Ğ·ÑƒĞ¼Ğ½Ğ¸Ğ¹ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ· ÑĞ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ”Ñ Ğ·Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸"""
    
    def __init__(self):
        self.specialized_registry = self._load_specialized_registry()
        self.performance_stats = {}
        self.usage_stats = {}
        self.load_balancer = ModelLoadBalancer()
        
    def _load_model_registry(self) -> dict[str, Any]:
        """Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ñ€ĞµÑ”ÑÑ‚Ñ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ°Ñ†Ñ–Ñ”Ñ Ğ·Ğ° Ğ¿Ğ¾Ñ‚ÑƒĞ¶Ğ½Ñ–ÑÑ‚Ñ"""
        return {
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            #                    ğŸ† FLAGSHIP TIER (ĞĞ°Ğ¹Ğ¿Ğ¾Ñ‚ÑƒĞ¶Ğ½Ñ–ÑˆÑ–)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            ModelTier.FLAGSHIP: {
                "models": [
                    "meta/meta-llama-3.1-405b-instruct",  # 405B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ–Ğ² - Ğ½Ğ°Ğ¹Ğ±Ñ–Ğ»ÑŒÑˆĞ°
                    "openai/gpt-5",                        # ĞĞ°Ğ¹Ğ½Ğ¾Ğ²Ñ–ÑˆĞ° GPT-5
                    "openai/o3",                           # ĞĞ°Ğ¹Ğ½Ğ¾Ğ²Ñ–ÑˆĞ° O-ÑĞµÑ€Ñ–Ñ
                    "mistral-ai/mistral-large-2411",       # ĞĞ°Ğ¹ĞºÑ€Ğ°Ñ‰Ğ° Mistral
                    "deepseek/deepseek-r1",                # ĞĞ°Ğ¹ĞºÑ€Ğ°Ñ‰Ğ° DeepSeek
                    "xai/grok-3",                          # ĞĞ°Ğ¹ĞºÑ€Ğ°Ñ‰Ğ° Grok
                ],
                "use_cases": ["critical_analysis", "complex_reasoning", "strategic_planning"],
                "max_tokens": 32000,
                "response_quality": 95
            },
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            #                    ğŸ’ PREMIUM TIER (Ğ’Ğ¸ÑĞ¾ĞºĞ¾ÑĞºÑ–ÑĞ½Ñ–)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            ModelTier.PREMIUM: {
                "models": [
                    "microsoft/phi-4",                     # ĞĞ¾Ğ²Ğ° Phi-4
                    "microsoft/phi-4-reasoning",           # Reasoning Phi-4
                    "openai/gpt-4.1",                      # ĞŸĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ° GPT-4
                    "openai/o1",                           # O1 reasoning
                    "meta/llama-3.3-70b-instruct",         # 70B Llama
                    "mistral-ai/mistral-medium-2505",      # Ğ¡ĞµÑ€ĞµĞ´Ğ½Ñ Mistral
                    "deepseek/deepseek-v3-0324",           # Ğ¡Ñ‚Ğ°Ğ±Ñ–Ğ»ÑŒĞ½Ğ° DeepSeek
                    "cohere/cohere-command-r-plus-08-2024", # ĞĞ°Ğ¹ĞºÑ€Ğ°Ñ‰Ğ° Cohere
                ],
                "use_cases": ["advanced_analysis", "complex_tasks", "multi_step_reasoning"],
                "max_tokens": 16000,
                "response_quality": 90
            },
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            #                    â­ STANDARD TIER (Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ–)  
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            ModelTier.STANDARD: {
                "models": [
                    "openai/gpt-4o",                       # Multimodal GPT-4
                    "openai/gpt-4.1-mini",                 # ĞœÑ–Ğ½Ñ– Ğ²ĞµÑ€ÑÑ–Ñ GPT-4.1
                    "microsoft/phi-3.5-moe-instruct",      # Mixture of Experts
                    "mistral-ai/mistral-nemo",             # ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ° Mistral
                    "meta/meta-llama-3.1-8b-instruct",     # 8B Llama
                    "cohere/cohere-command-r-08-2024",     # Command R
                    "ai21-labs/ai21-jamba-1.5-large",     # Jamba Large
                ],
                "use_cases": ["general_tasks", "data_processing", "analysis"],
                "max_tokens": 8000,
                "response_quality": 85
            },
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            #                    âš¡ EFFICIENT TIER (Ğ¨Ğ²Ğ¸Ğ´ĞºÑ– Ñ‚Ğ° ĞµÑ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ–)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
            ModelTier.EFFICIENT: {
                "models": [
                    "openai/gpt-4o-mini",                  # Ğ¨Ğ²Ğ¸Ğ´ĞºĞ° GPT
                    "openai/o1-mini",                      # Ğ¨Ğ²Ğ¸Ğ´ĞºĞ° O1
                    "microsoft/phi-4-mini-instruct",       # ĞœÑ–Ğ½Ñ– Phi-4
                    "microsoft/phi-4-mini-reasoning",      # Reasoning Ğ¼Ñ–Ğ½Ñ–
                    "mistral-ai/ministral-3b",             # ĞĞ°Ğ¹Ğ¼ĞµĞ½ÑˆĞ° Mistral
                    "mistral-ai/mistral-small-2503",       # ĞœĞ°Ğ»Ğ° Mistral
                    "microsoft/phi-3.5-mini-instruct",     # ĞœÑ–Ğ½Ñ– Phi-3.5
                    "openai/gpt-5-mini",                   # Ğ¨Ğ²Ğ¸Ğ´ĞºĞ° GPT-5
                    "xai/grok-3-mini",                     # ĞœÑ–Ğ½Ñ– Grok
                    "ai21-labs/ai21-jamba-1.5-mini",      # Jamba Mini
                ],
                "use_cases": ["quick_tasks", "real_time_processing", "simple_queries"],
                "max_tokens": 4000,
                "response_quality": 80
            },
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            #                    ğŸ¯ SPECIALIZED TIER (Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ–)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            ModelTier.SPECIALIZED: {
                "vision_models": [
                    "meta/llama-3.2-90b-vision-instruct",   # Ğ’Ñ–Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ° Llama
                    "meta/llama-3.2-11b-vision-instruct",   # ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ° Ğ²Ñ–Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°
                    "microsoft/phi-3.5-vision-instruct",     # Ğ’Ñ–Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ° Phi
                    "microsoft/phi-4-multimodal-instruct",   # ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ° Phi
                ],
                "code_models": [
                    "mistral-ai/codestral-2501",            # ĞšĞ¾Ğ´-ÑĞ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ°
                    "microsoft/mai-ds-r1",                  # Data Science ÑĞ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ°
                    "deepseek/deepseek-r1-0528",            # Ğ•ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°
                ],
                "reasoning_models": [
                    "openai/o1-preview",                    # Preview O1
                    "openai/o3-mini",                       # ĞœÑ–Ğ½Ñ– O3
                    "openai/o4-mini",                       # ĞœÑ–Ğ½Ñ– O4
                ],
                "embedding_models": [
                    "cohere/cohere-embed-v3-multilingual",  # ĞœÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ğ¸Ñ‡Ğ½Ñ– embedding
                    "cohere/cohere-embed-v3-english",       # ĞĞ½Ğ³Ğ»Ñ–Ğ¹ÑÑŒĞºÑ– embedding
                    "openai/text-embedding-3-large",        # Ğ’ĞµĞ»Ğ¸ĞºÑ– embedding
                    "openai/text-embedding-3-small",        # ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ– embedding
                ],
                "experimental_models": [
                    "meta/llama-4-maverick-17b-128e-instruct-fp8", # Llama-4 Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿
                    "meta/llama-4-scout-17b-16e-instruct",          # Ğ Ğ¾Ğ·Ğ²Ñ–Ğ´ÑƒĞ²Ğ°Ğ»ÑŒĞ½Ğ° Llama-4
                    "core42/jais-30b-chat",                         # ĞÑ€Ğ°Ğ±ÑÑŒĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
                    "cohere/cohere-command-a",                      # Command A
                    "openai/gpt-4.1-nano",                         # ĞĞ°Ğ½Ğ¾ GPT
                    "openai/gpt-5-nano",                           # ĞĞ°Ğ½Ğ¾ GPT-5
                    "openai/gpt-5-chat",                           # Ğ§Ğ°Ñ‚ GPT-5
                ],
                "use_cases": ["vision_tasks", "code_generation", "reasoning_tasks", 
                             "embedding_generation", "experimental_features"],
                "max_tokens": "variable",
                "response_quality": 85
            }
        }
    
    def select_model_for_agent(self, agent_name: str, task_complexity: TaskComplexity, 
                              task_type: str = "general") -> dict[str, Any]:
        """Ğ’Ğ¸Ğ±Ñ–Ñ€ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ñ— Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– ÑĞºĞ»Ğ°Ğ´Ğ½Ğ¾ÑÑ‚Ñ– Ğ·Ğ°Ğ´Ğ°Ñ‡Ñ–"""
        
        # Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ°Ğ³ĞµĞ½Ñ‚-ÑĞ¿ĞµÑ†Ñ–Ñ„Ñ–Ñ‡Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ· registry.yaml
        agent_models = self._get_agent_models(agent_name)
        
        # Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ Ñ€Ñ–Ğ²Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– ÑĞºĞ»Ğ°Ğ´Ğ½Ğ¾ÑÑ‚Ñ–
        if task_complexity == TaskComplexity.CRITICAL:
            tier = ModelTier.FLAGSHIP
        elif task_complexity == TaskComplexity.COMPLEX:
            tier = ModelTier.PREMIUM  
        elif task_complexity == TaskComplexity.MEDIUM:
            tier = ModelTier.STANDARD
        else:
            tier = ModelTier.EFFICIENT
            
        # Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ·Ğ°Ğ´Ğ°Ñ‡Ñ–
        if task_type in ["vision", "image_analysis"]:
            available_models = self.model_registry[ModelTier.SPECIALIZED]["vision_models"]
        elif task_type in ["code", "programming"]:
            available_models = self.model_registry[ModelTier.SPECIALIZED]["code_models"]
        elif task_type in ["reasoning", "logic"]:
            available_models = self.model_registry[ModelTier.SPECIALIZED]["reasoning_models"]
        elif task_type == "embedding":
            available_models = self.model_registry[ModelTier.SPECIALIZED]["embedding_models"]
        else:
            available_models = self.model_registry[tier]["models"]
        
        # Ğ’Ğ¸Ğ±Ñ–Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ğ· ÑƒÑ€Ğ°Ñ…ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ Ğ½Ğ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
        primary_model = agent_models.get("primary_model")
        fallback_models = agent_models.get("fallback_models", [])
        
        # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ñ– primary Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾Ğ¼Ñƒ tier
        if primary_model in available_models:
            selected_model = primary_model
        elif any(model in available_models for model in fallback_models):
            # Ğ’Ğ¸Ğ±Ñ–Ñ€ Ğ¿ĞµÑ€ÑˆĞ¾Ñ— Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ñ— fallback Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
            selected_model = next(model for model in fallback_models if model in available_models)
        else:
            # Ğ’Ğ¸Ğ¿Ğ°Ğ´ĞºĞ¾Ğ²Ğ¸Ğ¹ Ğ²Ğ¸Ğ±Ñ–Ñ€ Ğ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ tier'Ğ°
            selected_model = random.choice(available_models)
        
        return {
            "model": selected_model,
            "tier": tier.value,
            "agent": agent_name,
            "task_complexity": task_complexity.value,
            "task_type": task_type,
            "timestamp": datetime.now().isoformat(),
            "fallback_options": [m for m in available_models if m != selected_model][:3]
        }
    
    def _get_agent_models(self, agent_name: str) -> dict[str, Any]:
        """ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ· registry.yaml"""
        try:
            with open('/Users/dima/Documents/Predator11/backend/app/agents/registry.yaml', 'r') as f:
                registry = yaml.safe_load(f)
                return registry.get('agents', {}).get(agent_name, {})
        except Exception as e:
            print(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ registry.yaml: {e}")
            return {}
    
    def get_load_balanced_model(self, available_models: list[str]) -> str:
        """Ğ‘Ğ°Ğ»Ğ°Ğ½ÑÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ½Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ¼Ñ–Ğ¶ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¸Ğ¹ round-robin Ğ· ÑƒÑ€Ğ°Ñ…ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ğ½Ñ
        model_usage = {model: self.performance_stats.get(model, {}).get('usage_count', 0) 
                      for model in available_models}
        
        # Ğ’Ğ¸Ğ±Ñ–Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ğ· Ğ½Ğ°Ğ¹Ğ¼ĞµĞ½ÑˆĞ¸Ğ¼ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ğ½ÑĞ¼
        return min(model_usage.keys(), key=lambda m: model_usage[m])
    
    def update_model_performance(self, model: str, success: bool, 
                               response_time: float, quality_score: float):
        """ĞĞ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ñ– Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–"""
        if model not in self.performance_stats:
            self.performance_stats[model] = {
                'usage_count': 0,
                'success_rate': 0.0,
                'avg_response_time': 0.0,
                'avg_quality_score': 0.0,
                'total_requests': 0
            }
        
        stats = self.performance_stats[model]
        stats['usage_count'] += 1
        stats['total_requests'] += 1
        
        # ĞĞ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ ÑĞµÑ€ĞµĞ´Ğ½Ñ–Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ÑŒ
        prev_success_count = stats['success_rate'] * (stats['total_requests'] - 1)
        stats['success_rate'] = (prev_success_count + (1 if success else 0)) / stats['total_requests']
        
        prev_time_sum = stats['avg_response_time'] * (stats['total_requests'] - 1)
        stats['avg_response_time'] = (prev_time_sum + response_time) / stats['total_requests']
        
        prev_quality_sum = stats['avg_quality_score'] * (stats['total_requests'] - 1)
        stats['avg_quality_score'] = (prev_quality_sum + quality_score) / stats['total_requests']
    
    def get_model_recommendations(self, agent_name: str) -> dict[str, list[str]]:
        """ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ½Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ñ–Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""
        agent_models = self._get_agent_models(agent_name)
        
        recommendations = {
            "primary": [agent_models.get("primary_model", "")],
            "fallback": agent_models.get("fallback_models", []),
            "specialized": agent_models.get("specialized_models", []),
            "high_performance": self.model_registry[ModelTier.FLAGSHIP]["models"][:3],
            "fast_response": self.model_registry[ModelTier.EFFICIENT]["models"][:3],
            "cost_effective": self.model_registry[ModelTier.STANDARD]["models"][:3]
        }
        
        return {k: v for k, v in recommendations.items() if v and v != [""]}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                           ğŸ¯ USAGE EXAMPLES                                     
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ñ–Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
    router = ModelRouter()
    
    print("ğŸ¤– PREDATOR ANALYTICS - DYNAMIC MODEL ROUTER")
    print("=" * 60)
    
    # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ– ÑÑ†ĞµĞ½Ğ°Ñ€Ñ–Ñ— Ğ´Ğ»Ñ Ñ€Ñ–Ğ·Ğ½Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ñ–Ğ²
    test_scenarios = [
        ("AnomalyAgent", TaskComplexity.COMPLEX, "analysis"),
        ("ForecastAgent", TaskComplexity.CRITICAL, "prediction"),  
        ("SecurityAgent", TaskComplexity.MEDIUM, "security"),
        ("DatasetAgent", TaskComplexity.SIMPLE, "processing"),
        ("GraphIntelligenceAgent", TaskComplexity.COMPLEX, "graph_analysis"),
        ("OSINTAgent", TaskComplexity.MEDIUM, "vision")
    ]
    
    for agent, complexity, task_type in test_scenarios:
        selection = router.select_model_for_agent(agent, complexity, task_type)
        
        print(f"\nğŸ¯ Agent: {agent}")
        print(f"   ğŸ“Š Task: {complexity.value} | Type: {task_type}")
        print(f"   ğŸ¤– Selected Model: {selection['model']}")
        print(f"   ğŸ† Tier: {selection['tier']}")
        print(f"   ğŸ”„ Fallbacks: {selection['fallback_options'][:2]}")
    
    # Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ñ–Ñ— Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
    print(f"\nğŸ“‹ MODEL RECOMMENDATIONS FOR AnomalyAgent:")
    recommendations = router.get_model_recommendations("AnomalyAgent")
    for category, models in recommendations.items():
        print(f"   {category}: {models[:2]}")  # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚Ğ¸ Ğ¿ĞµÑ€ÑˆÑ– 2
    
    print(f"\nâœ… Total Models Available: 58")
    print(f"ğŸ¯ Models Distribution: Optimized for all agent specializations")
    print(f"ğŸ”„ Dynamic Routing: Enabled with performance tracking")

if __name__ == "__main__":
    main()
