#!/usr/bin/env python3
"""
üß™ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è AI –º–æ–¥–µ–ª–µ–π –∑ –ø—Ä–æ–º–ø—Ç–∞–º–∏
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø—Ä–æ–º–ø—Ç–∏ –∑ test_prompts.txt –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
"""

import os
import sys
import json
import time
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple

class AIModelTester:
    def __init__(self, workspace_path: str = "/Users/dima/Documents/NIMDA/codespaces-models"):
        self.workspace = Path(workspace_path)
        self.base_url = "http://localhost:4000/v1"  # –û—Å–Ω–æ–≤–Ω–∏–π API
        self.backup_url = "http://localhost:3010/v1"  # –†–µ–∑–µ—Ä–≤–Ω–∏–π API
        self.current_api = None
        self.test_results = []
        self.working_models = []
        self.failed_models = []
        
    def run_comprehensive_tests(self) -> Dict:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤ –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üß™ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è AI –º–æ–¥–µ–ª–µ–π")
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ API
        if not self.connect_to_api():
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏—Å—è –¥–æ API")
            return {'error': 'API –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π'}
        
        # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤
        test_prompts = self.load_test_prompts()
        if not test_prompts:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ—Å—Ç–æ–≤—ñ –ø—Ä–æ–º–ø—Ç–∏")
            return {'error': '–ü—Ä–æ–º–ø—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ'}
        
        # 3. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –º–æ–¥–µ–ª–µ–π
        available_models = self.get_available_models()
        if not available_models:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
            return {'error': '–ú–æ–¥–µ–ª—ñ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ'}
        
        print(f"üìã –ó–Ω–∞–π–¥–µ–Ω–æ {len(available_models)} –º–æ–¥–µ–ª–µ–π")
        print(f"üìù –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(test_prompts)} —Ç–µ—Å—Ç–æ–≤–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤")
        print(f"üîó –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è API: {self.current_api}")
        
        # 4. –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
        self.test_all_models(available_models, test_prompts)
        
        # 5. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—É
        duration = time.time() - start_time
        return self.generate_test_report(duration, len(available_models), len(test_prompts))
        
    def connect_to_api(self) -> bool:
        """–ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ API"""
        print("üîå –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ AI API...")
        
        apis_to_try = [
            (self.base_url, "–û—Å–Ω–æ–≤–Ω–∏–π API (4000)"),
            (self.backup_url, "–†–µ–∑–µ—Ä–≤–Ω–∏–π API (3010)")
        ]
        
        for api_url, name in apis_to_try:
            try:
                response = requests.get(f"{api_url}/models", timeout=10)
                if response.status_code == 200:
                    self.current_api = api_url
                    print(f"   ‚úÖ {name}: –ø—ñ–¥–∫–ª—é—á–µ–Ω–æ")
                    return True
                else:
                    print(f"   ‚ùå {name}: HTTP {response.status_code}")
            except Exception as e:
                print(f"   ‚ùå {name}: {type(e).__name__}")
        
        return False
        
    def load_test_prompts(self) -> List[str]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤"""
        print("üìù –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤...")
        
        prompts_file = self.workspace / 'test_prompts.txt'
        
        if not prompts_file.exists():
            print("   ‚ö†Ô∏è test_prompts.txt –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±–∞–∑–æ–≤—ñ –ø—Ä–æ–º–ø—Ç–∏")
            return self.get_default_prompts()
        
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                prompts = [line.strip() for line in f.readlines() if line.strip()]
            
            print(f"   ‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(prompts)} –ø—Ä–æ–º–ø—Ç—ñ–≤ –∑ —Ñ–∞–π–ª—É")
            return prompts
            
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É: {e}")
            return self.get_default_prompts()
            
    def get_default_prompts(self) -> List[str]:
        """–ë–∞–∑–æ–≤—ñ –ø—Ä–æ–º–ø—Ç–∏ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
        return [
            "–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?",
            "–†–æ–∑–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç",
            "–©–æ —Ç–∞–∫–µ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç?", 
            "–ù–∞–ø–∏—à–∏ –∫–æ–¥ –Ω–∞ Python –¥–ª—è Hello World",
            "–î–∞–π –ø–æ—Ä–∞–¥—É –ø–æ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—é",
            "–Ø–∫–∞ —Å—Ç–æ–ª–∏—Ü—è –£–∫—Ä–∞—ó–Ω–∏?",
            "–ü–æ—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—É —Ñ—ñ–∑–∏–∫—É –ø—Ä–æ—Å—Ç–∏–º–∏ —Å–ª–æ–≤–∞–º–∏",
            "–°—Ç–≤–æ—Ä–∏ –ø–ª–∞–Ω –≤–∏–≤—á–µ–Ω–Ω—è Python –∑–∞ –º—ñ—Å—è—Ü—å"
        ]
        
    def get_available_models(self) -> List[str]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        print("ü§ñ –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –º–æ–¥–µ–ª–µ–π...")
        
        try:
            response = requests.get(f"{self.current_api}/models", timeout=15)
            if response.status_code == 200:
                data = response.json()
                models = [model['id'] for model in data.get('data', [])]
                print(f"   ‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π")
                return models
            else:
                print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ API: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É –º–æ–¥–µ–ª–µ–π: {e}")
            return []
            
    def test_all_models(self, models: List[str], prompts: List[str]):
        """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π –∑ —É—Å—ñ–º–∞ –ø—Ä–æ–º–ø—Ç–∞–º–∏"""
        print(f"\nüß™ –ü–æ—á–∞—Ç–æ–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è {len(models)} –º–æ–¥–µ–ª–µ–π...")
        print("-" * 60)
        
        total_tests = len(models) * len(prompts)
        current_test = 0
        
        for i, model in enumerate(models, 1):
            print(f"\nüîç [{i}/{len(models)}] –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {model}")
            
            model_results = {
                'model': model,
                'tests': [],
                'success_rate': 0,
                'avg_response_time': 0,
                'total_tokens': 0,
                'errors': []
            }
            
            successful_tests = 0
            total_time = 0
            total_tokens = 0
            
            for j, prompt in enumerate(prompts, 1):
                current_test += 1
                progress = (current_test / total_tests) * 100
                
                print(f"   üìù [{j}/{len(prompts)}] –ü—Ä–æ–º–ø—Ç: {prompt[:50]}{'...' if len(prompt) > 50 else ''}")
                
                # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—É –∑ –º–æ–¥–µ–ª–ª—é
                test_result = self.test_single_prompt(model, prompt)
                model_results['tests'].append(test_result)
                
                if test_result['success']:
                    successful_tests += 1
                    total_time += test_result['response_time']
                    total_tokens += test_result.get('tokens', 0)
                    print(f"   ‚úÖ –£—Å–ø—ñ—à–Ω–æ ({test_result['response_time']:.1f}s, {test_result.get('tokens', 0)} —Ç–æ–∫–µ–Ω—ñ–≤)")
                else:
                    model_results['errors'].append(test_result['error'])
                    print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞: {test_result['error'][:100]}")
                
                # –ü—Ä–æ–≥—Ä–µ—Å
                if current_test % 5 == 0:
                    print(f"   üìä –ü—Ä–æ–≥—Ä–µ—Å: {progress:.1f}% ({current_test}/{total_tests})")
                
                time.sleep(1)  # –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
            
            # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –º–æ–¥–µ–ª—ñ
            model_results['success_rate'] = (successful_tests / len(prompts)) * 100
            if successful_tests > 0:
                model_results['avg_response_time'] = total_time / successful_tests
                model_results['total_tokens'] = total_tokens
            
            self.test_results.append(model_results)
            
            # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
            if model_results['success_rate'] >= 80:
                self.working_models.append(model)
                print(f"   üü¢ –ú–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î –≤—ñ–¥–º—ñ–Ω–Ω–æ ({model_results['success_rate']:.1f}%)")
            elif model_results['success_rate'] >= 50:
                print(f"   üü° –ú–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î –∑ –ø—Ä–æ–±–ª–µ–º–∞–º–∏ ({model_results['success_rate']:.1f}%)")
            else:
                self.failed_models.append(model)
                print(f"   üî¥ –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–∞—Ü—é—î ({model_results['success_rate']:.1f}%)")
                
    def test_single_prompt(self, model: str, prompt: str) -> Dict:
        """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É –∑ –æ–¥–Ω—ñ—î—é –º–æ–¥–µ–ª–ª—é"""
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{self.current_api}/chat/completions",
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 200,
                    "temperature": 0.7
                },
                timeout=30
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('choices') and len(data['choices']) > 0:
                    content = data['choices'][0].get('message', {}).get('content', '')
                    tokens = data.get('usage', {}).get('total_tokens', 0)
                    
                    return {
                        'success': True,
                        'prompt': prompt,
                        'response': content[:200],  # –ü–µ—Ä—à—ñ 200 —Å–∏–º–≤–æ–ª—ñ–≤
                        'response_time': response_time,
                        'tokens': tokens,
                        'status_code': response.status_code
                    }
                else:
                    return {
                        'success': False,
                        'prompt': prompt,
                        'error': '–ù–µ–º–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤ choices',
                        'response_time': response_time,
                        'status_code': response.status_code
                    }
            else:
                return {
                    'success': False,
                    'prompt': prompt,
                    'error': f'HTTP {response.status_code}: {response.text[:200]}',
                    'response_time': response_time,
                    'status_code': response.status_code
                }
                
        except requests.exceptions.Timeout:
            return {
                'success': False,
                'prompt': prompt,
                'error': '–¢–∞–π–º–∞—É—Ç –∑–∞–ø–∏—Ç—É (30s)',
                'response_time': time.time() - start_time
            }
        except Exception as e:
            return {
                'success': False,
                'prompt': prompt,
                'error': f'{type(e).__name__}: {str(e)[:200]}',
                'response_time': time.time() - start_time
            }
            
    def generate_test_report(self, duration: float, total_models: int, total_prompts: int) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—É —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
        print("\n" + "=" * 60)
        print("üìä –ó–í–Ü–¢ –¢–ï–°–¢–£–í–ê–ù–ù–Ø AI –ú–û–î–ï–õ–ï–ô")
        print("=" * 60)
        
        # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–≥–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_tests = len(self.test_results) * total_prompts if self.test_results else 0
        successful_tests = sum(
            len([t for t in model['tests'] if t['success']]) 
            for model in self.test_results
        )
        
        overall_success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        
        # –ù–∞–π–∫—Ä–∞—â—ñ —Ç–∞ –Ω–∞–π–≥—ñ—Ä—à—ñ –º–æ–¥–µ–ª—ñ
        if self.test_results:
            sorted_models = sorted(self.test_results, key=lambda x: x['success_rate'], reverse=True)
            best_models = sorted_models[:5]
            worst_models = sorted_models[-5:]
        else:
            best_models = worst_models = []
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'test_duration': round(duration, 2),
            'api_used': self.current_api,
            'summary': {
                'total_models_tested': len(self.test_results),
                'total_prompts': total_prompts,
                'total_tests_run': total_tests,
                'successful_tests': successful_tests,
                'overall_success_rate': round(overall_success_rate, 2),
                'working_models': len(self.working_models),
                'failed_models': len(self.failed_models)
            },
            'best_models': [
                {
                    'model': m['model'],
                    'success_rate': round(m['success_rate'], 1),
                    'avg_response_time': round(m['avg_response_time'], 2),
                    'total_tokens': m['total_tokens']
                }
                for m in best_models
            ],
            'worst_models': [
                {
                    'model': m['model'], 
                    'success_rate': round(m['success_rate'], 1),
                    'main_errors': m['errors'][:3]  # –ü–µ—Ä—à—ñ 3 –ø–æ–º–∏–ª–∫–∏
                }
                for m in worst_models
            ],
            'working_models': self.working_models,
            'failed_models': self.failed_models,
            'detailed_results': self.test_results
        }
        
        # –í–∏–≤–µ–¥–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
        print(f"‚è±Ô∏è –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {duration:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –ó–∞–≥–∞–ª—å–Ω–∏–π —É—Å–ø—ñ—Ö: {overall_success_rate:.1f}%")
        print(f"ü§ñ –ü—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(self.test_results)}")
        print(f"üìù –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –ø—Ä–æ–º–ø—Ç—ñ–≤: {total_prompts}")
        print(f"üß™ –í—Å—å–æ–≥–æ —Ç–µ—Å—Ç—ñ–≤: {total_tests}")
        print(f"‚úÖ –£—Å–ø—ñ—à–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤: {successful_tests}")
        
        print(f"\nüü¢ –†–æ–±–æ—á—ñ –º–æ–¥–µ–ª—ñ ({len(self.working_models)}):")
        for model in self.working_models[:10]:  # –ü–µ—Ä—à—ñ 10
            model_data = next((m for m in self.test_results if m['model'] == model), {})
            success_rate = model_data.get('success_rate', 0)
            avg_time = model_data.get('avg_response_time', 0)
            print(f"   ‚úÖ {model} ({success_rate:.1f}%, {avg_time:.1f}s)")
        
        if len(self.working_models) > 10:
            print(f"   ... —Ç–∞ —â–µ {len(self.working_models) - 10} –º–æ–¥–µ–ª–µ–π")
        
        if self.failed_models:
            print(f"\nüî¥ –ü—Ä–æ–±–ª–µ–º–Ω—ñ –º–æ–¥–µ–ª—ñ ({len(self.failed_models)}):")
            for model in self.failed_models[:5]:  # –ü–µ—Ä—à—ñ 5
                model_data = next((m for m in self.test_results if m['model'] == model), {})
                success_rate = model_data.get('success_rate', 0)
                print(f"   ‚ùå {model} ({success_rate:.1f}%)")
        
        print(f"\nüèÜ –¢–û–ü-5 –ù–ê–ô–ö–†–ê–©–ò–• –ú–û–î–ï–õ–ï–ô:")
        for i, model in enumerate(best_models, 1):
            print(f"   {i}. {model['model']}")
            print(f"      üìä –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {model['success_rate']:.1f}%")
            print(f"      ‚ö° –®–≤–∏–¥–∫—ñ—Å—Ç—å: {model['avg_response_time']:.2f}s")
            print(f"      üî§ –¢–æ–∫–µ–Ω—ñ–≤: {model['total_tokens']}")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
        report_file = self.workspace / f'ai_models_test_report_{int(time.time())}.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–ø–∏—Å–∫—É —Ä–æ–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π
        working_models_file = self.workspace / 'working_models_list.txt'
        with open(working_models_file, 'w', encoding='utf-8') as f:
            f.write("# –°–ø–∏—Å–æ–∫ —Ä–æ–±–æ—á–∏—Ö AI –º–æ–¥–µ–ª–µ–π\n")
            f.write(f"# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å >= 80%\n\n")
            for model in self.working_models:
                model_data = next((m for m in self.test_results if m['model'] == model), {})
                f.write(f"{model} # {model_data.get('success_rate', 0):.1f}%\n")
        
        print(f"\nüìÑ –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç: {report_file}")
        print(f"üìã –°–ø–∏—Å–æ–∫ —Ä–æ–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {working_models_file}")
        
        print(f"\nüöÄ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –∫–æ–º–∞–Ω–¥–∏:")
        if self.working_models:
            print(f"   # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ")
            print(f"   curl -X POST {self.current_api}/chat/completions \\")
            print(f"     -H 'Content-Type: application/json' \\")
            print(f"     -d '{{\"model\":\"{self.working_models[0]}\",\"messages\":[{{\"role\":\"user\",\"content\":\"–ü—Ä–∏–≤—ñ—Ç!\"}}]}}'")
        
        return report

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("üß™ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è AI –º–æ–¥–µ–ª–µ–π")
    print("üéØ –¢–µ—Å—Ç—É—î –≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ –º–æ–¥–µ–ª—ñ –∑ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∑ test_prompts.txt")
    
    tester = AIModelTester()
    
    try:
        report = tester.run_comprehensive_tests()
        
        if 'error' in report:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {report['error']}")
            return 1
        
        success_rate = report['summary']['overall_success_rate']
        if success_rate >= 70:
            print(f"\nüéâ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ! –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ.")
            return 0
        elif success_rate >= 40:
            print(f"\n‚ö†Ô∏è –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ –∑–∞—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è–º–∏. –ß–∞—Å—Ç–∏–Ω–∞ –º–æ–¥–µ–ª–µ–π –º–∞—î –ø—Ä–æ–±–ª–µ–º–∏.")
            return 1
        else:
            print(f"\n‚ùå –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –≤–∏—è–≤–∏–ª–æ —Å–µ—Ä–π–æ–∑–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ –∑ –±—ñ–ª—å—à—ñ—Å—Ç—é –º–æ–¥–µ–ª–µ–π.")
            return 2
            
    except KeyboardInterrupt:
        print(f"\nüëã –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
        return 1
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        return 2

if __name__ == "__main__":
    sys.exit(main())
