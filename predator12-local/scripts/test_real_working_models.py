#!/usr/bin/env python3
"""
üß™ –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –¢–ï–°–¢ –í–°–Ü–• –†–ï–ê–õ–¨–ù–ò–• AI –ú–û–î–ï–õ–ï–ô
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ä–µ–∞–ª—å–Ω—ñ –Ω–∞–∑–≤–∏ –º–æ–¥–µ–ª–µ–π –∑ —Å–µ—Ä–≤–µ—Ä–∞
"""

import asyncio
import aiohttp
import json
from typing import Dict, Any, Tuple
from datetime import datetime
import logging

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/dima/Documents/Predator11/logs/real_model_testing.log'),
        logging.StreamHandler()
    ]
)

# –†–ï–ê–õ–¨–ù–Ü –º–æ–¥–µ–ª—ñ –∑ —Å–µ—Ä–≤–µ—Ä–∞ (–æ—Ç—Ä–∏–º–∞–Ω—ñ –∑ /v1/models)
REAL_MODELS = [
    "ai21-labs/ai21-jamba-1.5-large",
    "ai21-labs/ai21-jamba-1.5-mini", 
    "cohere/cohere-command-a",
    "cohere/cohere-command-r-08-2024",
    "cohere/cohere-command-r-plus-08-2024",
    "cohere/cohere-embed-v3-english",
    "cohere/cohere-embed-v3-multilingual",
    "core42/jais-30b-chat",
    "deepseek/deepseek-r1",
    "deepseek/deepseek-r1-0528",
    "deepseek/deepseek-v3-0324",
    "meta/llama-3.2-11b-vision-instruct",
    "meta/llama-3.2-90b-vision-instruct",
    "meta/llama-3.3-70b-instruct",
    "meta/llama-4-maverick-17b-128e-instruct-fp8",
    "meta/llama-4-scout-17b-16e-instruct",
    "meta/meta-llama-3.1-405b-instruct",
    "meta/meta-llama-3.1-8b-instruct",
    "microsoft/mai-ds-r1",
    "microsoft/phi-3-medium-128k-instruct",
    "microsoft/phi-3-medium-4k-instruct",
    "microsoft/phi-3-mini-128k-instruct",
    "microsoft/phi-3-mini-4k-instruct",
    "microsoft/phi-3-small-128k-instruct",
    "microsoft/phi-3-small-8k-instruct",
    "microsoft/phi-3.5-mini-instruct",
    "microsoft/phi-3.5-moe-instruct",
    "microsoft/phi-3.5-vision-instruct",
    "microsoft/phi-4",
    "microsoft/phi-4-mini-instruct",
    "microsoft/phi-4-mini-reasoning",
    "microsoft/phi-4-multimodal-instruct",
    "microsoft/phi-4-reasoning",
    "mistral-ai/codestral-2501",
    "mistral-ai/ministral-3b",
    "mistral-ai/mistral-large-2411",
    "mistral-ai/mistral-medium-2505",
    "mistral-ai/mistral-nemo",
    "mistral-ai/mistral-small-2503",
    "openai/gpt-4.1",
    "openai/gpt-4.1-mini",
    "openai/gpt-4.1-nano",
    "openai/gpt-4o",
    "openai/gpt-4o-mini",
    "openai/gpt-5",
    "openai/gpt-5-chat",
    "openai/gpt-5-mini",
    "openai/gpt-5-nano",
    "openai/o1",
    "openai/o1-mini",
    "openai/o1-preview",
    "openai/o3",
    "openai/o3-mini",
    "openai/o4-mini",
    "openai/text-embedding-3-large",
    "openai/text-embedding-3-small",
    "xai/grok-3",
    "xai/grok-3-mini"
]

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Å–µ—Ä–≤–µ—Ä—ñ–≤
SERVERS = [
    {"url": "http://localhost:3011", "name": "Main AI Server 3011"},
    {"url": "http://localhost:3010", "name": "Backup Server 3010"},
]

class RealModelTester:
    def __init__(self):
        self.results = {
            "tested_at": datetime.now().isoformat(),
            "total_models": len(REAL_MODELS),
            "servers": SERVERS,
            "results": {},
            "summary": {
                "working": 0,
                "failed": 0,
                "unavailable": 0,
                "errors": []
            }
        }
        
    async def test_model_on_server(self, session: aiohttp.ClientSession, 
                                 model: str, server: Dict[str, str]) -> Tuple[bool, str]:
        """–¢–µ—Å—Ç—É—î –æ–¥–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—ñ"""
        try:
            # –û—Å–Ω–æ–≤–Ω–∏–π OpenAI-compatible endpoint
            endpoint = f"{server['url']}/v1/chat/completions"
            
            test_payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": "Test message: respond with 'OK'"}
                ],
                "max_tokens": 5,
                "temperature": 0.0
            }
            
            async with session.post(
                endpoint,
                json=test_payload,
                timeout=aiohttp.ClientTimeout(total=30),
                headers={"Content-Type": "application/json"}
            ) as response:
                
                if response.status == 200:
                    data = await response.json()
                    if "choices" in data and data["choices"]:
                        content = data["choices"][0].get("message", {}).get("content", "")
                        logging.info(f"‚úÖ {model} –ø—Ä–∞—Ü—é—î –Ω–∞ {server['name']}: {content[:50]}")
                        return True, f"Response: {content[:100]}"
                        
                elif response.status == 404:
                    return False, "Model not found (404)"
                elif response.status == 429:
                    return False, "Rate limited (429)"
                elif response.status == 503:
                    return False, "Service unavailable (503)"
                else:
                    error_text = await response.text()
                    logging.warning(f"‚ö†Ô∏è {model} –Ω–∞ {server['name']}: {response.status} - {error_text[:100]}")
                    return False, f"HTTP {response.status}: {error_text[:100]}"
                    
        except asyncio.TimeoutError:
            logging.warning(f"‚è±Ô∏è Timeout –¥–ª—è {model} –Ω–∞ {server['name']}")
            return False, "Timeout after 30s"
        except Exception as e:
            error_msg = f"Error: {str(e)[:100]}"
            logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –¥–ª—è {model} –Ω–∞ {server['name']}: {error_msg}")
            return False, error_msg
    
    async def test_all_models(self):
        """–¢–µ—Å—Ç—É—î –≤—Å—ñ –º–æ–¥–µ–ª—ñ –Ω–∞ –≤—Å—ñ—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö"""
        logging.info(f"üöÄ –ü–æ—á–∏–Ω–∞—é —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è {len(REAL_MODELS)} —Ä–µ–∞–ª—å–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ {len(SERVERS)} —Å–µ—Ä–≤–µ—Ä–∞—Ö...")
        
        async with aiohttp.ClientSession() as session:
            for i, model in enumerate(REAL_MODELS, 1):
                logging.info(f"\nüìä [{i}/{len(REAL_MODELS)}] –¢–µ—Å—Ç—É—é –º–æ–¥–µ–ª—å: {model}")
                
                model_results = {}
                model_working = False
                
                # –¢–µ—Å—Ç—É—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–∂–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—ñ
                for server in SERVERS:
                    success, message = await self.test_model_on_server(session, model, server)
                    model_results[server["name"]] = {
                        "success": success,
                        "message": message,
                        "tested_at": datetime.now().isoformat()
                    }
                    
                    if success:
                        model_working = True
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                self.results["results"][model] = {
                    "working": model_working,
                    "servers": model_results,
                    "status": "‚úÖ Working" if model_working else "‚ùå Failed"
                }
                
                # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                if model_working:
                    self.results["summary"]["working"] += 1
                    logging.info(f"‚úÖ {model} - –ü–†–ê–¶–Æ–Ñ")
                else:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ –≤–∑–∞–≥–∞–ª—ñ
                    if "unavailable" in str(model_results).lower() or "404" in str(model_results):
                        self.results["summary"]["unavailable"] += 1
                    else:
                        self.results["summary"]["failed"] += 1
                    
                    self.results["summary"]["errors"].append({
                        "model": model,
                        "reason": message
                    })
                    logging.error(f"‚ùå {model} - –ù–ï –ü–†–ê–¶–Æ–Ñ: {message}")
                
                # –ü–∞—É–∑–∞ –º—ñ–∂ —Ç–µ—Å—Ç–∞–º–∏ –¥–ª—è rate limiting
                await asyncio.sleep(1)
        
        logging.info("üèÅ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logging.info(f"‚úÖ –ü—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {self.results['summary']['working']}")
        logging.info(f"‚ùå –ù–µ –ø—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {self.results['summary']['failed']}")
        logging.info(f"üö´ –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π: {self.results['summary']['unavailable']}")
        
    def save_results(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç
        report_path = f"/Users/dima/Documents/Predator11/REAL_MODEL_TESTING_REPORT_{timestamp}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # –ö–æ—Ä–æ—Ç–∫–∏–π –∑–≤—ñ—Ç –∑ –ø—Ä–∞—Ü—é—é—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
        working_models = [model for model, result in self.results["results"].items() if result["working"]]
        
        summary_path = f"/Users/dima/Documents/Predator11/WORKING_MODELS_LIST_{timestamp}.md"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# üéØ –ü–†–ê–¶–Æ–Æ–ß–Ü AI –ú–û–î–ï–õ–Ü\n\n")
            f.write(f"**–¢–µ—Å—Ç–æ–≤–∞–Ω–æ:** {self.results['tested_at']}\n\n")
            f.write("## üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n\n")
            f.write(f"- üî¢ –í—Å—å–æ–≥–æ –º–æ–¥–µ–ª–µ–π: {self.results['total_models']}\n")
            f.write(f"- ‚úÖ –ü—Ä–∞—Ü—é—é—á–∏—Ö: {self.results['summary']['working']}\n")
            f.write(f"- ‚ùå –ù–µ –ø—Ä–∞—Ü—é—é—á–∏—Ö: {self.results['summary']['failed']}\n")
            f.write(f"- üö´ –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∏—Ö: {self.results['summary']['unavailable']}\n")
            f.write(f"- üéØ –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {(self.results['summary']['working']/self.results['total_models']*100):.1f}%\n\n")
            
            f.write("## ‚úÖ –°–ü–ò–°–û–ö –ü–†–ê–¶–Æ–Æ–ß–ò–• –ú–û–î–ï–õ–ï–ô\n\n")
            f.write("```python\n")
            f.write("WORKING_MODELS = [\n")
            for model in working_models:
                f.write(f'    "{model}",\n')
            f.write("]\n")
            f.write("```\n\n")
            
            f.write("## ‚ùå –ù–ï –ü–†–ê–¶–Æ–Æ–ß–Ü –ú–û–î–ï–õ–Ü\n\n")
            for model, result in self.results["results"].items():
                if not result["working"]:
                    f.write(f"- `{model}` - {list(result['servers'].values())[0]['message']}\n")
                    
            f.write("\n## üîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á\n\n")
            f.write("1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø—Ä–∞—Ü—é—é—á—ñ –º–æ–¥–µ–ª—ñ –≤ production\n")
            f.write("2. –û–Ω–æ–≤–∏—Ç–∏ agent registry –∑ —Ä–µ–∞–ª—å–Ω–∏–º–∏ –Ω–∞–∑–≤–∞–º–∏ –º–æ–¥–µ–ª–µ–π\n")
            f.write("3. –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ fallback –º—ñ–∂ –ø—Ä–∞—Ü—é—é—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏\n")
        
        # –û–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª —Ç—ñ–ª—å–∫–∏ –∑ –ø—Ä–∞—Ü—é—é—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
        working_models_path = f"/Users/dima/Documents/Predator11/scripts/working_models.py"
        with open(working_models_path, 'w', encoding='utf-8') as f:
            f.write("#!/usr/bin/env python3\n")
            f.write('"""\n')
            f.write("üéØ –°–ü–ò–°–û–ö –ü–†–ê–¶–Æ–Æ–ß–ò–• AI –ú–û–î–ï–õ–ï–ô\n")
            f.write(f"–û–Ω–æ–≤–ª–µ–Ω–æ: {self.results['tested_at']}\n")
            f.write('"""\n\n')
            f.write("# –í—Å—ñ –ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω—ñ —Ç–∞ –ø—Ä–∞—Ü—é—é—á—ñ –º–æ–¥–µ–ª—ñ\n")
            f.write("WORKING_MODELS = [\n")
            for model in working_models:
                f.write(f'    "{model}",\n')
            f.write("]\n\n")
            f.write(f"TOTAL_WORKING = {len(working_models)}\n")
            f.write(f"SUCCESS_RATE = {(len(working_models)/len(REAL_MODELS)*100):.1f}  # %\n")
        
        logging.info(f"üìÑ –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç: {report_path}")
        logging.info(f"üìã –ö–æ—Ä–æ—Ç–∫–∏–π –∑–≤—ñ—Ç: {summary_path}")
        logging.info(f"üêç Python —Ñ–∞–π–ª: {working_models_path}")
        
        return report_path, summary_path, working_models_path

async def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("üß™ –¢–ï–°–¢–£–í–ê–ù–ù–Ø –†–ï–ê–õ–¨–ù–ò–• AI –ú–û–î–ï–õ–ï–ô –ó –°–ï–†–í–ï–†–ê")
    print("=" * 50)
    
    tester = RealModelTester()
    
    try:
        await tester.test_all_models()
        report_path, summary_path, working_path = tester.save_results()
        
        print("\nüéØ –§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"‚úÖ –ü—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {tester.results['summary']['working']}/{len(REAL_MODELS)}")
        print(f"‚ùå –ù–µ –ø—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {tester.results['summary']['failed']}/{len(REAL_MODELS)}")
        print(f"üö´ –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π: {tester.results['summary']['unavailable']}/{len(REAL_MODELS)}")
        print(f"üéØ –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {(tester.results['summary']['working']/len(REAL_MODELS)*100):.1f}%")
        print(f"\nüìä –§–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ:")
        print(f"   üìÑ –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç: {report_path}")
        print(f"   üìã –ö–æ—Ä–æ—Ç–∫–∏–π –∑–≤—ñ—Ç: {summary_path}")
        print(f"   üêç –ü—Ä–∞—Ü—é—é—á—ñ –º–æ–¥–µ–ª—ñ: {working_path}")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª —Ç—ñ–ª—å–∫–∏ –∑ –ø—Ä–∞—Ü—é—é—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
        working_models = [model for model, result in tester.results["results"].items() if result["working"]]
        if len(working_models) > 20:  # –î–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–ª—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É
            print(f"\nüéØ –°—Ç–≤–æ—Ä—é—é –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –∑ {len(working_models)} –ø—Ä–∞—Ü—é—é—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏...")
            await create_optimized_distribution(working_models)
        else:
            print(f"\n‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π ({len(working_models)}) –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É")
            
    except Exception as e:
        logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {e}")

async def create_optimized_distribution(working_models):
    """–°—Ç–≤–æ—Ä—é—î –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª —Ç—ñ–ª—å–∫–∏ –∑ –ø—Ä–∞—Ü—é—é—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    
    # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
    import sys
    sys.path.append('/Users/dima/Documents/Predator11/scripts')
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
    optimized_config = {
        "model_distribution": {
            "total_working_models": len(working_models),
            "total_agents": 26,
            "distribution_strategy": "working_models_only",
            "last_tested": datetime.now().isoformat(),
            "failover_enabled": True,
            "thermal_protection": True
        },
        "working_models": working_models,
        "agents": {}
    }
    
    # –†–æ–∑–ø–æ–¥—ñ–ª—è—î–º–æ –ø—Ä–∞—Ü—é—é—á—ñ –º–æ–¥–µ–ª—ñ –º—ñ–∂ –∞–≥–µ–Ω—Ç–∞–º–∏
    models_per_agent = len(working_models) // 26
    extra_models = len(working_models) % 26
    
    agent_names = [
        "ChiefOrchestrator", "ModelRouter", "QueryPlanner", "DataQuality", "Anomaly",
        "Forecast", "AutoHeal", "SelfDiagnosis", "DatasetIngest", "ETLOrchestrator",
        "Indexer", "Embedding", "OSINTCrawler", "GraphBuilder", "Simulator",
        "SyntheticData", "ReportExport", "BillingGate", "PIIGuardian", "SelfImprovement",
        "RedTeam", "ComplianceMonitor", "PerformanceOptimizer", "Arbiter", "NexusGuide",
        "SchemaMapper"
    ]
    
    model_index = 0
    for i, agent_name in enumerate(agent_names):
        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü—å–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        models_for_agent = models_per_agent + (1 if i < extra_models else 0)
        
        # –í–∏–±–∏—Ä–∞—î–º–æ –º–æ–¥–µ–ª—ñ
        agent_models = working_models[model_index:model_index + models_for_agent]
        model_index += models_for_agent
        
        optimized_config["agents"][agent_name] = {
            "primary_models": agent_models[:len(agent_models)//2] if len(agent_models) > 1 else agent_models,
            "fallback_models": agent_models[len(agent_models)//2:] if len(agent_models) > 1 else [],
            "max_concurrent": 3,
            "load_balancing": "round_robin"
        }
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    optimized_path = f"/Users/dima/Documents/Predator11/OPTIMIZED_WORKING_MODELS_DISTRIBUTION_{timestamp}.json"
    
    with open(optimized_path, 'w', encoding='utf-8') as f:
        json.dump(optimized_config, f, indent=2, ensure_ascii=False)
    
    print(f"üéØ –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {optimized_path}")

if __name__ == "__main__":
    asyncio.run(main())
