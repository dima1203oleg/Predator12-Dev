#!/usr/bin/env python3
"""
üß™ –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ï –¢–ï–°–¢–£–í–ê–ù–ù–Ø –í–°–Ü–• 58 –ë–ï–ó–ö–û–®–¢–û–í–ù–ò–• AI –ú–û–î–ï–õ–ï–ô
–ü–µ—Ä–µ–≤—ñ—Ä—è—î –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ –Ω–∞ —Å–µ—Ä–≤–µ—Ä—ñ —Ç–∞ –≤–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏
"""

import asyncio
import aiohttp
import json
from typing import Dict, Any, Tuple
from datetime import datetime
import logging

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/dima/Documents/Predator11/logs/model_testing.log'),
        logging.StreamHandler()
    ]
)

# –í—Å—ñ 58 –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
ALL_MODELS = [
    "deepseek/deepseek-r1", "meta/meta-llama-3.3-70b-instruct", "openai/gpt-4o-2024-11-20",
    "microsoft/phi-4", "qwen/qwen2.5-72b-instruct", "mistral/mixtral-8x22b-instruct",
    "openai/o1-preview-2024-09-12", "xai/grok-2-1212", "meta/meta-llama-3.2-90b-vision-instruct",
    "deepseek/deepseek-v3", "openai/gpt-4o-mini-2024-07-18", "microsoft/phi-3-medium-128k-instruct",
    "qwen/qwen2.5-32b-instruct", "mistral/mistral-large-2411", "cohere/command-r-plus-08-2024",
    "meta/meta-llama-3.2-11b-vision-instruct", "openai/o1-mini-2024-09-12", 
    "microsoft/phi-3-vision-128k-instruct", "deepseek/deepseek-coder-v2-lite", 
    "ai21/ai21-jamba-1-5-large", "meta/meta-llama-3.2-3b-instruct", "meta/meta-llama-3.2-1b-instruct",
    "microsoft/phi-3-small-128k-instruct", "microsoft/phi-3-small-8k-instruct", 
    "microsoft/phi-3-mini-128k-instruct", "microsoft/phi-3-mini-4k-instruct",
    "qwen/qwen2.5-14b-instruct", "qwen/qwen2.5-7b-instruct", "qwen/qwen2.5-3b-instruct",
    "qwen/qwen2.5-1.5b-instruct", "qwen/qwen2.5-0.5b-instruct", "mistral/ministral-8b-2410",
    "mistral/ministral-3b-2410", "cohere/command-r-08-2024", "cohere/command-r7b-12-2024",
    "deepseek/deepseek-coder-v2", "ai21/ai21-jamba-1-5-mini", "core42/jais-30b-chat",
    "xai/grok-2-vision-1212", "meta/meta-llama-3.1-8b-instruct", "meta/meta-llama-3.1-70b-instruct",
    "meta/meta-llama-3.1-405b-instruct", "openai/gpt-4-turbo-2024-04-09", "openai/gpt-4-0613",
    "openai/gpt-4-0125-preview", "openai/gpt-3.5-turbo-0125", "openai/gpt-3.5-turbo-1106",
    "openai/chatgpt-4o-latest", "openai/gpt-4o-2024-08-06", "openai/gpt-4o-2024-05-13",
    "openai/o2-2024-12-17", "openai/o3-mini-2024-12-17", "openai/o4-2024-12-17",
    "mistral/mistral-nemo-2407", "mistral/codestral-2405", "cohere/command-light",
    "cohere/embed-english-v3.0", "cohere/embed-multilingual-v3.0"
]

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Å–µ—Ä–≤–µ—Ä—ñ–≤
SERVERS = [
    {"url": "http://localhost:3010", "name": "Local Server 3010"},
    {"url": "http://localhost:3011", "name": "Local Server 3011"},
    {"url": "http://localhost:8000", "name": "Backend API 8000"},
]

class ModelTester:
    def __init__(self):
        self.results = {
            "tested_at": datetime.now().isoformat(),
            "total_models": len(ALL_MODELS),
            "servers": SERVERS,
            "results": {},
            "summary": {
                "working": 0,
                "failed": 0,
                "errors": []
            }
        }
        
    async def test_model_on_server(self, session: aiohttp.ClientSession, 
                                 model: str, server: Dict[str, str]) -> Tuple[bool, str]:
        """–¢–µ—Å—Ç—É—î –æ–¥–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—ñ"""
        try:
            # –†—ñ–∑–Ω—ñ endpoints –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–µ—Ä–≤–µ—Ä—ñ–≤
            endpoints = [
                f"{server['url']}/v1/chat/completions",
                f"{server['url']}/api/v1/chat/completions", 
                f"{server['url']}/chat/completions",
                f"{server['url']}/api/models/{model}/chat"
            ]
            
            test_payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": "Hi! Please respond with just 'OK' if you're working."}
                ],
                "max_tokens": 10,
                "temperature": 0.1
            }
            
            for endpoint in endpoints:
                try:
                    async with session.post(
                        endpoint,
                        json=test_payload,
                        timeout=aiohttp.ClientTimeout(total=15),
                        headers={"Content-Type": "application/json"}
                    ) as response:
                        
                        if response.status == 200:
                            data = await response.json()
                            if "choices" in data and data["choices"]:
                                content = data["choices"][0].get("message", {}).get("content", "")
                                logging.info(f"‚úÖ {model} –ø—Ä–∞—Ü—é—î –Ω–∞ {server['name']} ({endpoint}): {content[:50]}")
                                return True, f"Working on {endpoint}"
                                
                        elif response.status in [404, 400]:
                            continue  # –°–ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π endpoint
                        else:
                            error_text = await response.text()
                            logging.warning(f"‚ö†Ô∏è {model} –Ω–∞ {server['name']} ({endpoint}): {response.status} - {error_text[:100]}")
                            
                except asyncio.TimeoutError:
                    logging.warning(f"‚è±Ô∏è Timeout –¥–ª—è {model} –Ω–∞ {server['name']} ({endpoint})")
                    continue
                except Exception as e:
                    logging.warning(f"üîß –ü–æ–º–∏–ª–∫–∞ {model} –Ω–∞ {server['name']} ({endpoint}): {str(e)[:100]}")
                    continue
                    
            return False, f"Failed on all endpoints for {server['name']}"
            
        except Exception as e:
            error_msg = f"Critical error: {str(e)}"
            logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {model} –Ω–∞ {server['name']}: {error_msg}")
            return False, error_msg
    
    async def test_all_models(self):
        """–¢–µ—Å—Ç—É—î –≤—Å—ñ –º–æ–¥–µ–ª—ñ –Ω–∞ –≤—Å—ñ—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö"""
        logging.info(f"üöÄ –ü–æ—á–∏–Ω–∞—é —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è {len(ALL_MODELS)} –º–æ–¥–µ–ª–µ–π –Ω–∞ {len(SERVERS)} —Å–µ—Ä–≤–µ—Ä–∞—Ö...")
        
        async with aiohttp.ClientSession() as session:
            for i, model in enumerate(ALL_MODELS, 1):
                logging.info(f"\nüìä [{i}/{len(ALL_MODELS)}] –¢–µ—Å—Ç—É—é –º–æ–¥–µ–ª—å: {model}")
                
                model_results = {}
                model_working = False
                
                # –¢–µ—Å—Ç—É—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–∂–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—ñ
                for server in SERVERS:
                    success, message = await self.test_model_on_server(session, model, server)
                    model_results[server["name"]] = {
                        "success": success,
                        "message": message,
                        "tested_at": datetime.now().isoformat()
                    }
                    
                    if success:
                        model_working = True
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                self.results["results"][model] = {
                    "working": model_working,
                    "servers": model_results,
                    "status": "‚úÖ Working" if model_working else "‚ùå Failed"
                }
                
                # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                if model_working:
                    self.results["summary"]["working"] += 1
                    logging.info(f"‚úÖ {model} - –ü–†–ê–¶–Æ–Ñ")
                else:
                    self.results["summary"]["failed"] += 1 
                    self.results["summary"]["errors"].append({
                        "model": model,
                        "reason": "Not working on any server"
                    })
                    logging.error(f"‚ùå {model} - –ù–ï –ü–†–ê–¶–Æ–Ñ")
                
                # –ù–µ–≤–µ–ª–∏–∫–∞ –ø–∞—É–∑–∞ –º—ñ–∂ —Ç–µ—Å—Ç–∞–º–∏
                await asyncio.sleep(0.5)
        
        logging.info("\nüèÅ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logging.info(f"‚úÖ –ü—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {self.results['summary']['working']}")
        logging.info(f"‚ùå –ù–µ –ø—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {self.results['summary']['failed']}")
        
    def save_results(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç
        report_path = f"/Users/dima/Documents/Predator11/MODEL_TESTING_REPORT_{timestamp}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ—Ä–æ—Ç–∫–∏–π –∑–≤—ñ—Ç  
        summary_path = f"/Users/dima/Documents/Predator11/MODEL_TEST_SUMMARY_{timestamp}.md"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# üß™ –ó–í–Ü–¢ –¢–ï–°–¢–£–í–ê–ù–ù–Ø 58 AI –ú–û–î–ï–õ–ï–ô\n\n")
            f.write(f"**–î–∞—Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è:** {self.results['tested_at']}\n\n")
            f.write("## üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n\n")
            f.write(f"- üî¢ –í—Å—å–æ–≥–æ –º–æ–¥–µ–ª–µ–π: {self.results['total_models']}\n")
            f.write(f"- ‚úÖ –ü—Ä–∞—Ü—é—é—á–∏—Ö: {self.results['summary']['working']}\n")
            f.write(f"- ‚ùå –ù–µ –ø—Ä–∞—Ü—é—é—á–∏—Ö: {self.results['summary']['failed']}\n")
            f.write(f"- üéØ –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {(self.results['summary']['working']/self.results['total_models']*100):.1f}%\n\n")
            
            f.write(f"## ‚úÖ –ü–†–ê–¶–Æ–Æ–ß–Ü –ú–û–î–ï–õ–Ü\n\n")
            for model, result in self.results["results"].items():
                if result["working"]:
                    working_servers = [name for name, srv in result["servers"].items() if srv["success"]]
                    f.write(f"- `{model}` - {', '.join(working_servers)}\n")
            
            f.write(f"\n## ‚ùå –ù–ï –ü–†–ê–¶–Æ–Æ–ß–Ü –ú–û–î–ï–õ–Ü\n\n")
            for model, result in self.results["results"].items():
                if not result["working"]:
                    f.write(f"- `{model}` - –ø–æ—Ç—Ä–µ–±—É—î –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è\n")
                    
            f.write(f"\n## üîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á\n\n")
            if self.results['summary']['failed'] > 0:
                f.write(f"1. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —Å–µ—Ä–≤–µ—Ä—ñ–≤ –¥–ª—è –Ω–µ—Ä–æ–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π\n")
                f.write(f"2. –û–Ω–æ–≤–∏—Ç–∏ model registry –∑ —Ä–æ–±–æ—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏\n") 
                f.write(f"3. –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ fallback –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–∏—Ö –∞–≥–µ–Ω—Ç—ñ–≤\n")
            else:
                f.write(f"üéâ –í—Å—ñ –º–æ–¥–µ–ª—ñ –ø—Ä–∞—Ü—é—é—Ç—å –≤—ñ–¥–º—ñ–Ω–Ω–æ!\n")
        
        logging.info(f"üìÑ –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç: {report_path}")
        logging.info(f"üìã –ö–æ—Ä–æ—Ç–∫–∏–π –∑–≤—ñ—Ç: {summary_path}")
        
        return report_path, summary_path

async def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("üß™ –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ï –¢–ï–°–¢–£–í–ê–ù–ù–Ø 58 AI –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)
    
    tester = ModelTester()
    
    try:
        await tester.test_all_models()
        report_path, summary_path = tester.save_results()
        
        print(f"\nüéØ –§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"‚úÖ –ü—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {tester.results['summary']['working']}/58")
        print(f"‚ùå –ù–µ –ø—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {tester.results['summary']['failed']}/58") 
        print(f"üéØ –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {(tester.results['summary']['working']/58*100):.1f}%")
        print(f"\nüìä –ó–≤—ñ—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ:")
        print(f"   üìÑ –î–µ—Ç–∞–ª—å–Ω–∏–π: {report_path}")
        print(f"   üìã –ö–æ—Ä–æ—Ç–∫–∏–π: {summary_path}")
        
        # –Ø–∫—â–æ —î –Ω–µ—Ä–æ–±–æ—á—ñ –º–æ–¥–µ–ª—ñ - —Å—Ç–≤–æ—Ä—é—î–º–æ –ø–ª–∞–Ω –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è
        if tester.results['summary']['failed'] > 0:
            print(f"\nüîß –°—Ç–≤–æ—Ä—é—é –ø–ª–∞–Ω –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è...")
            await create_fix_plan(tester.results)
        else:
            print(f"\nüéâ –í—Å—ñ –º–æ–¥–µ–ª—ñ –ø—Ä–∞—Ü—é—é—Ç—å –≤—ñ–¥–º—ñ–Ω–Ω–æ!")
            
    except Exception as e:
        logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {e}")

async def create_fix_plan(results: Dict[str, Any]):
    """–°—Ç–≤–æ—Ä—é—î –ø–ª–∞–Ω –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –¥–ª—è –Ω–µ—Ä–æ–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    failed_models = []
    for model, result in results["results"].items():
        if not result["working"]:
            failed_models.append(model)
    
    fix_plan = {
        "failed_models": failed_models,
        "fix_strategies": [
            {
                "step": 1,
                "action": "–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä—ñ–≤",
                "commands": [
                    "curl -s http://localhost:3010/health || echo 'Server 3010 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π'",
                    "curl -s http://localhost:3011/health || echo 'Server 3011 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π'"
                ]
            },
            {
                "step": 2, 
                "action": "–û–Ω–æ–≤–∏—Ç–∏ model registry –∑ —Ä–æ–±–æ—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏",
                "file": "/Users/dima/Documents/Predator11/agents/registry.yaml"
            },
            {
                "step": 3,
                "action": "–ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ fallback –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –∞–≥–µ–Ω—Ç—ñ–≤", 
                "priority_agents": ["ChiefOrchestrator", "ModelRouter", "QueryPlanner"]
            }
        ]
    }
    
    fix_path = f"/Users/dima/Documents/Predator11/MODEL_FIX_PLAN_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(fix_path, 'w', encoding='utf-8') as f:
        json.dump(fix_plan, f, indent=2, ensure_ascii=False)
    
    print(f"üîß –ü–ª–∞–Ω –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {fix_path}")

if __name__ == "__main__":
    asyncio.run(main())
