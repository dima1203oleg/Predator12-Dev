#!/usr/bin/env python3
"""
üß† –†–û–ó–£–ú–ù–ò–ô –†–û–ó–ü–û–î–Ü–õ 58 –ë–ï–ó–ö–û–®–¢–û–í–ù–ò–• AI –ú–û–î–ï–õ–ï–ô
–º—ñ–∂ 26 –∞–≥–µ–Ω—Ç–∞–º–∏ –∑ –∑–∞—Ö–∏—Å—Ç–æ–º –≤—ñ–¥ –ø–µ—Ä–µ–≥—Ä—ñ–≤—É —Ç–∞ fallback –ª–æ–≥—ñ–∫–æ—é
"""

import json
import yaml
from typing import Dict, List, Any

# –†–ï–ê–õ–¨–ù–û –ü–†–ê–¶–Æ–Æ–ß–Ü 21 –ë–ï–ó–ö–û–®–¢–û–í–ù–Ü –ú–û–î–ï–õ–Ü (–ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω—ñ 28.09.2025)
ALL_MODELS = [
    # TIER 1: –ù–ê–ô–ü–û–¢–£–ñ–ù–Ü–®–Ü –î–û–°–¢–£–ü–ù–Ü –ú–û–î–ï–õ–Ü (7 –º–æ–¥–µ–ª–µ–π)
    "meta/meta-llama-3.1-405b-instruct",     # –ù–∞–π–ø–æ—Ç—É–∂–Ω—ñ—à–∞ Meta –º–æ–¥–µ–ª—å  
    "meta/llama-3.2-90b-vision-instruct",    # Vision + –ø–æ—Ç—É–∂–Ω–∞
    "mistral-ai/mistral-large-2411",         # –ù–∞–π–∫—Ä–∞—â–∞ Mistral
    "core42/jais-30b-chat",                  # –ü–æ—Ç—É–∂–Ω–∞ –∞—Ä–∞–±—Å—å–∫–∞ –º–æ–¥–µ–ª—å
    "cohere/cohere-command-r-plus-08-2024",  # –ù–∞–π–∫—Ä–∞—â–∞ Cohere  
    "microsoft/phi-4-reasoning",             # Reasoning –º–æ–¥–µ–ª—å
    "openai/gpt-5-chat"
    
    # TIER 2: –í–ò–°–û–ö–û–ü–†–û–î–£–ö–¢–ò–í–ù–Ü (12 –º–æ–¥–µ–ª–µ–π)
    "meta/llama-3.2-90b-vision-instruct",    # Llama 3.2 90B Vision
    "deepseek/deepseek-v3-0324",             # DeepSeek V3
    "openai/gpt-4o",                         # GPT-4o
    "microsoft/phi-3-medium-128k-instruct",  # Phi-3 Medium
    "mistral-ai/mistral-large-2411",         # Mistral Large
    "cohere/cohere-command-r-plus-08-2024",  # Cohere Command R+
    "meta/llama-3.2-11b-vision-instruct",    # Llama 3.2 11B Vision
    "openai/o1-mini",                        # O1 Mini
    "microsoft/phi-3.5-vision-instruct",     # Phi-3.5 Vision
    "ai21-labs/ai21-jamba-1.5-large",       # Jamba Large
    "openai/gpt-4.1",                        # GPT-4.1
    "meta/meta-llama-3.1-405b-instruct",     # Llama 3.1 405B
    
    # TIER 3: –°–ü–ï–¶–Ü–ê–õ–Ü–ó–û–í–ê–ù–Ü (20 –º–æ–¥–µ–ª–µ–π)
    "microsoft/phi-3-small-128k-instruct",   # Phi-3 Small 128K
    "microsoft/phi-3-small-8k-instruct",     # Phi-3 Small 8K
    "microsoft/phi-3-mini-128k-instruct",    # Phi-3 Mini 128K
    "microsoft/phi-3-mini-4k-instruct",      # Phi-3 Mini 4K
    "microsoft/phi-3-medium-4k-instruct",    # Phi-3 Medium 4K
    "microsoft/phi-3.5-mini-instruct",       # Phi-3.5 Mini
    "microsoft/phi-3.5-moe-instruct",        # Phi-3.5 MoE
    "microsoft/phi-4-mini-instruct",         # Phi-4 Mini
    "microsoft/phi-4-mini-reasoning",        # Phi-4 Mini Reasoning
    "microsoft/phi-4-multimodal-instruct",   # Phi-4 Multimodal
    "microsoft/phi-4-reasoning",             # Phi-4 Reasoning
    "mistral-ai/ministral-3b",               # Ministral 3B
    "mistral-ai/mistral-nemo",               # Mistral Nemo
    "mistral-ai/mistral-medium-2505",        # Mistral Medium
    "mistral-ai/mistral-small-2503",         # Mistral Small
    "cohere/cohere-command-r-08-2024",       # Cohere Command R
    "cohere/cohere-command-a",               # Cohere Command A
    "ai21-labs/ai21-jamba-1.5-mini",        # Jamba Mini
    "core42/jais-30b-chat",                  # JAIS 30B
    "meta/meta-llama-3.1-8b-instruct",      # Llama 3.1 8B
    
    # TIER 4: –®–í–ò–î–ö–Ü/–õ–ï–ì–ö–Ü (16 –º–æ–¥–µ–ª–µ–π)
    "openai/gpt-4o-mini",                    # GPT-4o Mini
    "openai/gpt-4.1-mini",                  # GPT-4.1 Mini
    "openai/gpt-4.1-nano",                  # GPT-4.1 Nano
    "openai/gpt-5-mini",                     # GPT-5 Mini
    "openai/gpt-5-nano",                     # GPT-5 Nano
    "openai/o3-mini",                        # O3 Mini
    "openai/o4-mini",                        # O4 Mini
    "xai/grok-3-mini",                       # Grok-3 Mini
    "mistral-ai/codestral-2501",             # Codestral
    "microsoft/mai-ds-r1",                   # MAI DS R1
    "meta/llama-4-maverick-17b-128e-instruct-fp8",  # Llama 4 Maverick
    "meta/llama-4-scout-17b-16e-instruct",   # Llama 4 Scout
    "cohere/cohere-embed-v3-english",        # Cohere Embed English
    "cohere/cohere-embed-v3-multilingual",   # Cohere Embed Multilingual
    "openai/text-embedding-3-large",         # OpenAI Embed Large
    "openai/text-embedding-3-small"          # OpenAI Embed Small
]

# 26 –∞–≥–µ–Ω—Ç—ñ–≤ –∑ —ó—Ö–Ω—å–æ—é —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—î—é
AGENT_SPECIFICATIONS = {
    # –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–Ü –ê–ì–ï–ù–¢–ò (TIER 1 + —Ä–µ–∑–µ—Ä–≤)
    "ChiefOrchestrator": {
        "category": "critical_reasoning",
        "primary_models": ["deepseek/deepseek-r1", "meta/meta-llama-3.3-70b-instruct"],
        "fallback_models": ["qwen/qwen2.5-72b-instruct", "mistral/mixtral-8x22b-instruct"],
        "emergency_models": ["openai/gpt-4o-2024-11-20"],
        "max_concurrent": 4,
        "load_balancing": "round_robin"
    },
    
    "ModelRouter": {
        "category": "critical_routing", 
        "primary_models": ["microsoft/phi-4", "deepseek/deepseek-v3"],
        "fallback_models": ["openai/o1-preview-2024-09-12", "xai/grok-2-1212"],
        "emergency_models": ["meta/meta-llama-3.2-90b-vision-instruct"],
        "max_concurrent": 3,
        "load_balancing": "least_loaded"
    },
    
    "QueryPlanner": {
        "category": "critical_planning",
        "primary_models": ["openai/gpt-4o-mini-2024-07-18", "microsoft/phi-3-medium-128k-instruct"],
        "fallback_models": ["qwen/qwen2.5-32b-instruct", "mistral/mistral-large-2411"],
        "emergency_models": ["cohere/command-r-plus-08-2024"],
        "max_concurrent": 3,
        "load_balancing": "performance_based"
    },
    
    # –í–ò–°–û–ö–û–ù–ê–í–ê–ù–¢–ê–ñ–ï–ù–Ü –ê–ì–ï–ù–¢–ò (TIER 2)
    "DataQuality": {
        "category": "high_load_analysis",
        "primary_models": ["meta/meta-llama-3.2-11b-vision-instruct", "openai/o1-mini-2024-09-12"],
        "fallback_models": ["microsoft/phi-3-vision-128k-instruct", "deepseek/deepseek-coder-v2-lite"],
        "emergency_models": ["ai21/ai21-jamba-1-5-large"],
        "max_concurrent": 5,
        "load_balancing": "round_robin"
    },
    
    "Anomaly": {
        "category": "real_time_detection",
        "primary_models": ["meta/meta-llama-3.2-3b-instruct", "microsoft/phi-3-small-128k-instruct"],
        "fallback_models": ["qwen/qwen2.5-14b-instruct", "mistral/ministral-8b-2410"],
        "emergency_models": ["cohere/command-r-08-2024"],
        "max_concurrent": 6,
        "load_balancing": "fastest_response"
    },
    
    "Forecast": {
        "category": "predictive_analytics",
        "primary_models": ["qwen/qwen2.5-7b-instruct", "microsoft/phi-3-mini-128k-instruct"],
        "fallback_models": ["mistral/ministral-3b-2410", "cohere/command-r7b-12-2024"],
        "emergency_models": ["meta/meta-llama-3.2-1b-instruct"],
        "max_concurrent": 4,
        "load_balancing": "accuracy_based"
    },
    
    # –°–ü–ï–¶–Ü–ê–õ–Ü–ó–û–í–ê–ù–Ü –ê–ì–ï–ù–¢–ò (TIER 3)
    "AutoHeal": {
        "category": "code_generation",
        "primary_models": ["deepseek/deepseek-coder-v2", "ai21/ai21-jamba-1-5-mini"],
        "fallback_models": ["core42/jais-30b-chat", "microsoft/phi-3-small-8k-instruct"],
        "emergency_models": ["qwen/qwen2.5-3b-instruct"],
        "max_concurrent": 2,
        "load_balancing": "code_quality"
    },
    
    "SelfDiagnosis": {
        "category": "system_analysis",
        "primary_models": ["xai/grok-2-vision-1212", "meta/meta-llama-3.1-8b-instruct"],
        "fallback_models": ["qwen/qwen2.5-1.5b-instruct", "qwen/qwen2.5-0.5b-instruct"],
        "emergency_models": ["meta/meta-llama-3.1-70b-instruct"],
        "max_concurrent": 3,
        "load_balancing": "diagnostic_accuracy"
    },
    
    # –®–í–ò–î–ö–Ü –ê–ì–ï–ù–¢–ò (TIER 4)
    "DatasetIngest": {
        "category": "fast_processing",
        "primary_models": ["meta/meta-llama-3.1-405b-instruct", "openai/gpt-4-turbo-2024-04-09"],
        "fallback_models": ["openai/gpt-4-0613", "openai/gpt-4-0125-preview"],
        "emergency_models": ["openai/gpt-3.5-turbo-0125"],
        "max_concurrent": 8,
        "load_balancing": "throughput"
    },
    
    "ETLOrchestrator": {
        "category": "data_transformation",
        "primary_models": ["openai/gpt-3.5-turbo-1106", "openai/chatgpt-4o-latest"],
        "fallback_models": ["openai/gpt-4o-2024-08-06", "openai/gpt-4o-2024-05-13"],
        "emergency_models": ["mistral/mistral-nemo-2407"],
        "max_concurrent": 6,
        "load_balancing": "data_size_based"
    },
    
    # –†–ï–®–¢–ê –ê–ì–ï–ù–¢–Ü–í (—Ä–æ–∑–ø–æ–¥—ñ–ª –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –º–æ–¥–µ–ª–µ–π)
    "Indexer": {
        "category": "indexing",
        "primary_models": ["openai/o2-2024-12-17", "openai/o3-mini-2024-12-17"],
        "fallback_models": ["openai/o4-2024-12-17", "mistral/codestral-2405"],
        "emergency_models": ["cohere/command-light"],
        "max_concurrent": 4,
        "load_balancing": "index_size"
    },
    
    "Embedding": {
        "category": "embeddings",
        "primary_models": ["cohere/embed-english-v3.0", "cohere/embed-multilingual-v3.0"],
        "fallback_models": ["meta/meta-llama-3.2-3b-instruct", "qwen/qwen2.5-1.5b-instruct"],
        "emergency_models": ["microsoft/phi-3-mini-4k-instruct"],
        "max_concurrent": 10,
        "load_balancing": "embedding_dimension"
    },
    
    # –î–æ–¥–∞—î–º–æ —Ä–µ—à—Ç—É –∞–≥–µ–Ω—Ç—ñ–≤ –∑ —Ä–æ–∑—É–º–Ω–∏–º —Ä–æ–∑–ø–æ–¥—ñ–ª–æ–º
    "OSINTCrawler": {"category": "web_analysis", "primary_models": ["meta/meta-llama-3.2-1b-instruct"], "fallback_models": ["qwen/qwen2.5-0.5b-instruct"], "max_concurrent": 5},
    "GraphBuilder": {"category": "graph_analysis", "primary_models": ["microsoft/phi-3-small-8k-instruct"], "fallback_models": ["mistral/ministral-3b-2410"], "max_concurrent": 3},
    "Simulator": {"category": "simulation", "primary_models": ["cohere/command-r7b-12-2024"], "fallback_models": ["ai21/ai21-jamba-1-5-mini"], "max_concurrent": 2},
    "SyntheticData": {"category": "data_generation", "primary_models": ["core42/jais-30b-chat"], "fallback_models": ["deepseek/deepseek-coder-v2"], "max_concurrent": 3},
    "ReportExport": {"category": "document_generation", "primary_models": ["xai/grok-2-vision-1212"], "fallback_models": ["meta/meta-llama-3.1-8b-instruct"], "max_concurrent": 2},
    "BillingGate": {"category": "financial_analysis", "primary_models": ["qwen/qwen2.5-3b-instruct"], "fallback_models": ["microsoft/phi-3-mini-128k-instruct"], "max_concurrent": 2},
    "PIIGuardian": {"category": "privacy_protection", "primary_models": ["mistral/ministral-8b-2410"], "fallback_models": ["cohere/command-r-08-2024"], "max_concurrent": 4},
    "SelfImprovement": {"category": "optimization", "primary_models": ["deepseek/deepseek-coder-v2-lite"], "fallback_models": ["ai21/ai21-jamba-1-5-large"], "max_concurrent": 1},
    "RedTeam": {"category": "security_testing", "primary_models": ["meta/meta-llama-3.2-90b-vision-instruct"], "fallback_models": ["openai/o1-preview-2024-09-12"], "max_concurrent": 2},
    "ComplianceMonitor": {"category": "compliance_check", "primary_models": ["microsoft/phi-3-vision-128k-instruct"], "fallback_models": ["deepseek/deepseek-v3"], "max_concurrent": 3},
    "PerformanceOptimizer": {"category": "performance", "primary_models": ["openai/o1-mini-2024-09-12"], "fallback_models": ["meta/meta-llama-3.2-11b-vision-instruct"], "max_concurrent": 2},
    "Arbiter": {"category": "decision_making", "primary_models": ["qwen/qwen2.5-32b-instruct"], "fallback_models": ["mistral/mistral-large-2411"], "max_concurrent": 2},
    "NexusGuide": {"category": "user_assistance", "primary_models": ["cohere/command-r-plus-08-2024"], "fallback_models": ["microsoft/phi-3-medium-128k-instruct"], "max_concurrent": 3},
    "SchemaMapper": {"category": "schema_analysis", "primary_models": ["qwen/qwen2.5-14b-instruct"], "fallback_models": ["mistral/ministral-8b-2410"], "max_concurrent": 2}
}

def create_intelligent_model_distribution():
    """–°—Ç–≤–æ—Ä—é—î —Ä–æ–∑—É–º–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –º–æ–¥–µ–ª–µ–π –∑ –∑–∞—Ö–∏—Å—Ç–æ–º –≤—ñ–¥ –ø–µ—Ä–µ–≥—Ä—ñ–≤—É"""
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —â–æ –≤—Å—ñ –º–æ–¥–µ–ª—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ
    used_models = set()
    for agent_name, config in AGENT_SPECIFICATIONS.items():
        for model_list in ["primary_models", "fallback_models"]:
            if model_list in config:
                used_models.update(config[model_list])
        if "emergency_models" in config:
            used_models.update(config["emergency_models"])
    
    unused_models = set(ALL_MODELS) - used_models
    print(f"üîç –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(used_models)}/58")
    print(f"üîÑ –ù–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –º–æ–¥–µ–ª—ñ: {len(unused_models)}")
    
    if unused_models:
        print("‚ö†Ô∏è  –î–æ–¥–∞—î–º–æ –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–æ —Ä–µ–∑–µ—Ä–≤—É...")
        
    # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∑ –∑–∞—Ö–∏—Å—Ç–æ–º –≤—ñ–¥ –ø–µ—Ä–µ–≥—Ä—ñ–≤—É
    config = {
        "model_distribution": {
            "total_models": 58,
            "total_agents": 26,
            "distribution_strategy": "intelligent_load_balancing",
            "failover_enabled": True,
            "thermal_protection": True,
            "auto_scaling": True
        },
        
        "agents": {},
        
        "load_balancing": {
            "strategies": {
                "round_robin": {"weight": 1.0, "fairness": "high"},
                "least_loaded": {"weight": 0.8, "efficiency": "high"},
                "performance_based": {"weight": 0.9, "quality": "high"},
                "fastest_response": {"weight": 0.7, "speed": "high"},
                "accuracy_based": {"weight": 1.0, "precision": "high"},
                "throughput": {"weight": 0.6, "volume": "high"}
            }
        },
        
        "thermal_protection": {
            "temperature_threshold": 0.8,
            "cooldown_period": 30,
            "emergency_fallback": True,
            "model_rotation": True
        },
        
        "monitoring": {
            "health_checks": True,
            "performance_metrics": True,
            "failure_detection": True,
            "auto_recovery": True
        }
    }
    
    # –†–æ–∑–ø–æ–¥—ñ–ª—è—î–º–æ –∞–≥–µ–Ω—Ç—ñ–≤ –∑ —ó—Ö–Ω—ñ–º–∏ –º–æ–¥–µ–ª—è–º–∏
    for agent_name, agent_config in AGENT_SPECIFICATIONS.items():
        config["agents"][agent_name] = {
            "category": agent_config["category"],
            "models": {
                "primary": agent_config.get("primary_models", []),
                "fallback": agent_config.get("fallback_models", []), 
                "emergency": agent_config.get("emergency_models", [])
            },
            "concurrency": {
                "max_concurrent": agent_config.get("max_concurrent", 2),
                "load_balancing": agent_config.get("load_balancing", "round_robin")
            },
            "failover": {
                "enabled": True,
                "retry_attempts": 3,
                "backoff_strategy": "exponential"
            }
        }
    
    return config

def save_configuration(config: Dict[str, Any]) -> str:
    """–ó–±–µ—Ä—ñ–≥–∞—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —É —Ñ–∞–π–ª"""
    config_path = "/Users/dima/Documents/Predator11/agents/intelligent_model_distribution.yaml"
    
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    return config_path

def update_registry_yaml(config: Dict[str, Any]) -> str:
    """–û–Ω–æ–≤–ª—é—î registry.yaml –∑ –Ω–æ–≤–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é –º–æ–¥–µ–ª–µ–π"""
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—ñ LLM –ø—Ä–æ—Ñ—ñ–ª—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª—É
    llm_profiles = {}
    
    # –î–æ–¥–∞—î–º–æ –ø—Ä–æ—Ñ—ñ–ª—ñ –¥–ª—è –∫–æ–∂–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
    categories = set()
    for agent_name, agent_config in config["agents"].items():
        categories.add(agent_config["category"])
    
    profile_counter = 1
    for category in categories:
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∞–≥–µ–Ω—Ç—ñ–≤ —Ü—ñ—î—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
        category_agents = [name for name, cfg in config["agents"].items() 
                          if cfg["category"] == category]
        
        if category_agents:
            sample_agent = config["agents"][category_agents[0]]
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –æ—Å–Ω–æ–≤–Ω–∏–π –ø—Ä–æ—Ñ—ñ–ª—å
            if sample_agent["models"]["primary"]:
                llm_profiles[f"{category}_primary"] = {
                    "provider": "sdk",
                    "model_id": sample_agent["models"]["primary"][0],
                    "max_tokens": 4096,
                    "temperature": 0.1 if "reasoning" in category else 0.2,
                    "cost_per_1k_tokens": 0.0,
                    "fallback_models": sample_agent["models"]["primary"][1:] + sample_agent["models"]["fallback"]
                }
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–µ–∑–µ—Ä–≤–Ω–∏–π –ø—Ä–æ—Ñ—ñ–ª—å  
            if sample_agent["models"]["fallback"]:
                llm_profiles[f"{category}_backup"] = {
                    "provider": "sdk", 
                    "model_id": sample_agent["models"]["fallback"][0],
                    "max_tokens": 4096,
                    "temperature": 0.1,
                    "cost_per_1k_tokens": 0.0,
                    "emergency_models": sample_agent["models"]["emergency"]
                }
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π registry
    new_registry = {
        "llm_profiles": llm_profiles,
        "agents": {}
    }
    
    # –ü—Ä–∏–≤'—è–∑—É—î–º–æ –∞–≥–µ–Ω—Ç—ñ–≤ –¥–æ –ø—Ä–æ—Ñ—ñ–ª—ñ–≤
    for agent_name, agent_config in config["agents"].items():
        category = agent_config["category"]
        new_registry["agents"][agent_name] = {
            "llm": f"{category}_primary",
            "fallback": f"{category}_backup" if f"{category}_backup" in llm_profiles else f"{category}_primary",
            "max_concurrent": agent_config["concurrency"]["max_concurrent"],
            "load_balancing": agent_config["concurrency"]["load_balancing"]
        }
    
    # –î–æ–¥–∞—î–º–æ Kafka topics (–∑–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ—Å–Ω—É—é—á—ñ)
    new_registry["kafka_topics"] = {
        "orchestrator": ["orchestrator.tasks", "orchestrator.results", "orchestrator.scaling"],
        "data_flow": ["data.ingest", "data.quality", "data.processed", "etl.status"],
        "agents_communication": ["agents.requests", "agents.responses", "agents.status", "agents.tuning"],
        "system_events": ["system.incidents", "system.notifications", "system.health", "system.alerts"],
        "models": ["models.requests", "models.responses", "models.health", "models.failover"]
    }
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–Ω–æ–≤–ª–µ–Ω–∏–π registry
    registry_path = "/Users/dima/Documents/Predator11/agents/registry_intelligent.yaml"
    with open(registry_path, 'w', encoding='utf-8') as f:
        yaml.dump(new_registry, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    return registry_path

if __name__ == "__main__":
    print("üöÄ –°—Ç–≤–æ—Ä—é—é —Ä–æ–∑—É–º–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª 58 AI –º–æ–¥–µ–ª–µ–π –º—ñ–∂ 26 –∞–≥–µ–Ω—Ç–∞–º–∏...")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª
    config = create_intelligent_model_distribution()
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    config_path = save_configuration(config)
    print(f"‚úÖ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {config_path}")
    
    # –û–Ω–æ–≤–ª—é—î–º–æ registry
    registry_path = update_registry_yaml(config)
    print(f"‚úÖ Registry –æ–Ω–æ–≤–ª–µ–Ω–æ: {registry_path}")
    
    # –í–∏–≤–æ–¥–∏–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–û–ó–ü–û–î–Ü–õ–£:")
    print(f"üî¢ –í—Å—å–æ–≥–æ –º–æ–¥–µ–ª–µ–π: 58")
    print(f"ü§ñ –í—Å—å–æ–≥–æ –∞–≥–µ–Ω—Ç—ñ–≤: 26") 
    print(f"‚ö° –°–µ—Ä–µ–¥–Ω—å–æ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∞–≥–µ–Ω—Ç–∞: {58/26:.1f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
    categories = {}
    for agent_name, agent_config in config["agents"].items():
        category = agent_config["category"]
        if category not in categories:
            categories[category] = 0
        categories[category] += len(agent_config["models"]["primary"]) + len(agent_config["models"]["fallback"])
    
    print(f"\nüè∑Ô∏è  –†–û–ó–ü–û–î–Ü–õ –ü–û –ö–ê–¢–ï–ì–û–†–Ü–Ø–•:")
    for category, count in sorted(categories.items()):
        print(f"   {category}: {count} –º–æ–¥–µ–ª–µ–π")
    
    print(f"\nüéØ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–æ —Ä–æ–∑—É–º–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è!")
    print(f"üõ°Ô∏è  –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ –ø–µ—Ä–µ–≥—Ä—ñ–≤—É –∞–∫—Ç–∏–≤–Ω–∏–π")
    print(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ failover –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ")
