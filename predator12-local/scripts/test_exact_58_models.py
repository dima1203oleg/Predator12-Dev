#!/usr/bin/env python3
"""
üß™ –¢–ï–°–¢–£–í–ê–ù–ù–Ø –í–°–Ü–• 58 –ú–û–î–ï–õ–ï–ô –ó INTELLIGENT_MODEL_DISTRIBUTION
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç—ñ–ª—å–∫–∏ —Ç—ñ –º–æ–¥–µ–ª—ñ —â–æ –Ω–∞–¥–∞–Ω—ñ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É —Ñ–∞–π–ª—ñ
"""

import asyncio
import aiohttp
import json
from typing import Dict, Any, Tuple
from datetime import datetime
import logging

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# –¢–û–ß–ù–û –¢–Ü –°–ê–ú–Ü 58 –ú–û–î–ï–õ–ï–ô –ó INTELLIGENT_MODEL_DISTRIBUTION.PY
EXACT_MODELS = [
    # TIER 1: –ù–ê–ô–ü–û–¢–£–ñ–ù–Ü–®–Ü –ú–û–î–ï–õ–Ü (8 –º–æ–¥–µ–ª–µ–π)
    "deepseek/deepseek-r1",
    "meta/meta-llama-3.3-70b-instruct",
    "openai/gpt-4o-2024-11-20",
    "microsoft/phi-4",
    "qwen/qwen2.5-72b-instruct",
    "mistral/mixtral-8x22b-instruct",
    "openai/o1-preview-2024-09-12",
    "xai/grok-2-1212",
    
    # TIER 2: –í–ò–°–û–ö–û–ü–†–û–î–£–ö–¢–ò–í–ù–Ü (12 –º–æ–¥–µ–ª–µ–π)
    "meta/meta-llama-3.2-90b-vision-instruct",
    "deepseek/deepseek-v3",
    "openai/gpt-4o-mini-2024-07-18",
    "microsoft/phi-3-medium-128k-instruct",
    "qwen/qwen2.5-32b-instruct",
    "mistral/mistral-large-2411",
    "cohere/command-r-plus-08-2024",
    "meta/meta-llama-3.2-11b-vision-instruct",
    "openai/o1-mini-2024-09-12",
    "microsoft/phi-3-vision-128k-instruct",
    "deepseek/deepseek-coder-v2-lite",
    "ai21/ai21-jamba-1-5-large",
    
    # TIER 3: –°–ü–ï–¶–Ü–ê–õ–Ü–ó–û–í–ê–ù–Ü (20 –º–æ–¥–µ–ª–µ–π)
    "meta/meta-llama-3.2-3b-instruct",
    "meta/meta-llama-3.2-1b-instruct",
    "microsoft/phi-3-small-128k-instruct",
    "microsoft/phi-3-small-8k-instruct",
    "microsoft/phi-3-mini-128k-instruct",
    "microsoft/phi-3-mini-4k-instruct",
    "qwen/qwen2.5-14b-instruct",
    "qwen/qwen2.5-7b-instruct",
    "qwen/qwen2.5-3b-instruct",
    "qwen/qwen2.5-1.5b-instruct",
    "qwen/qwen2.5-0.5b-instruct",
    "mistral/ministral-8b-2410",
    "mistral/ministral-3b-2410",
    "cohere/command-r-08-2024",
    "cohere/command-r7b-12-2024",
    "deepseek/deepseek-coder-v2",
    "ai21/ai21-jamba-1-5-mini",
    "core42/jais-30b-chat",
    "xai/grok-2-vision-1212",
    "meta/meta-llama-3.1-8b-instruct",
    
    # TIER 4: –®–í–ò–î–ö–Ü/–õ–ï–ì–ö–Ü (18 –º–æ–¥–µ–ª–µ–π)
    "meta/meta-llama-3.1-70b-instruct",
    "meta/meta-llama-3.1-405b-instruct",
    "openai/gpt-4-turbo-2024-04-09",
    "openai/gpt-4-0613",
    "openai/gpt-4-0125-preview",
    "openai/gpt-3.5-turbo-0125",
    "openai/gpt-3.5-turbo-1106",
    "openai/chatgpt-4o-latest",
    "openai/gpt-4o-2024-08-06",
    "openai/gpt-4o-2024-05-13",
    "openai/o2-2024-12-17",
    "openai/o3-mini-2024-12-17",
    "openai/o4-2024-12-17",
    "mistral/mistral-nemo-2407",
    "mistral/codestral-2405",
    "cohere/command-light",
    "cohere/embed-english-v3.0",
    "cohere/embed-multilingual-v3.0"
]

# API ENDPOINTS –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
API_ENDPOINTS = [
    "http://localhost:3010/v1/chat/completions",
    "http://localhost:3011/v1/chat/completions", 
    "http://localhost:8000/v1/chat/completions"
]

class ExactModelTester:
    def __init__(self):
        self.results = {
            "tested_at": datetime.now().isoformat(),
            "total_models": len(EXACT_MODELS),
            "working_models": [],
            "failed_models": [],
            "test_details": {}
        }
        
    async def test_single_model(self, session: aiohttp.ClientSession, model: str) -> Tuple[bool, str]:
        """–¢–µ—Å—Ç—É—î –æ–¥–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å—ñ—Ö –¥–æ—Å—Ç—É–ø–Ω–∏—Ö endpoints"""
        
        test_payload = {
            "model": model,
            "messages": [{"role": "user", "content": "Say 'OK' if working"}],
            "max_tokens": 5,
            "temperature": 0
        }
        
        for endpoint in API_ENDPOINTS:
            try:
                async with session.post(
                    endpoint,
                    json=test_payload,
                    timeout=aiohttp.ClientTimeout(total=10),
                    headers={"Content-Type": "application/json"}
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        if "choices" in data and data["choices"]:
                            content = data["choices"][0].get("message", {}).get("content", "")
                            logging.info(f"‚úÖ {model} –ü–†–ê–¶–Æ–Ñ –Ω–∞ {endpoint}")
                            return True, f"Working on {endpoint}: {content}"
                    else:
                        error_text = await response.text()
                        logging.warning(f"‚ö†Ô∏è {model} –ø–æ–º–∏–ª–∫–∞ {response.status} –Ω–∞ {endpoint}: {error_text[:100]}")
                        
            except Exception as e:
                logging.warning(f"üîß {model} exception –Ω–∞ {endpoint}: {str(e)[:100]}")
                continue
                
        logging.error(f"‚ùå {model} –ù–ï –ü–†–ê–¶–Æ–Ñ –Ω–∞ –∂–æ–¥–Ω–æ–º—É endpoint")
        return False, "Failed on all endpoints"
    
    async def test_all_models(self):
        """–¢–µ—Å—Ç—É—î –≤—Å—ñ 58 –º–æ–¥–µ–ª–µ–π"""
        logging.info("üöÄ –¢–µ—Å—Ç—É—é –≤—Å—ñ 58 –º–æ–¥–µ–ª–µ–π –∑ intelligent_model_distribution.py")
        
        async with aiohttp.ClientSession() as session:
            for i, model in enumerate(EXACT_MODELS, 1):
                print(f"[{i}/58] –¢–µ—Å—Ç—É—é: {model}")
                
                success, message = await self.test_single_model(session, model)
                
                self.results["test_details"][model] = {
                    "success": success,
                    "message": message,
                    "tested_at": datetime.now().isoformat()
                }
                
                if success:
                    self.results["working_models"].append(model)
                else:
                    self.results["failed_models"].append(model)
                
                # –ü–∞—É–∑–∞ –º—ñ–∂ —Ç–µ—Å—Ç–∞–º–∏
                await asyncio.sleep(0.2)
        
        print(f"\nüèÅ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"‚úÖ –ü—Ä–∞—Ü—é—î: {len(self.results['working_models'])}/58")
        print(f"‚ùå –ù–µ –ø—Ä–∞—Ü—é—î: {len(self.results['failed_models'])}/58")
    
    def save_results(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –î–µ—Ç–∞–ª—å–Ω–∏–π JSON –∑–≤—ñ—Ç
        json_path = f"/Users/dima/Documents/Predator11/EXACT_MODELS_TEST_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # –°–ø–∏—Å–æ–∫ –ø—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
        working_path = f"/Users/dima/Documents/Predator11/WORKING_MODELS_{timestamp}.py"
        with open(working_path, 'w', encoding='utf-8') as f:
            f.write("# –ü–†–ê–¶–Æ–Æ–ß–Ü –ú–û–î–ï–õ–Ü –ó –¢–ï–°–¢–£–í–ê–ù–ù–Ø\n\n")
            f.write("WORKING_MODELS = [\n")
            for model in self.results["working_models"]:
                f.write(f'    "{model}",\n')
            f.write("]\n\n")
            f.write(f"# –í—Å—å–æ–≥–æ –ø—Ä–∞—Ü—é—é—á–∏—Ö: {len(self.results['working_models'])}\n")
            f.write(f"# –î–∞—Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {self.results['tested_at']}\n")
        
        # Markdown –∑–≤—ñ—Ç
        md_path = f"/Users/dima/Documents/Predator11/MODEL_TEST_RESULTS_{timestamp}.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("# üß™ –†–ï–ó–£–õ–¨–¢–ê–¢–ò –¢–ï–°–¢–£–í–ê–ù–ù–Ø 58 –ú–û–î–ï–õ–ï–ô\n\n")
            f.write(f"**–î–∞—Ç–∞:** {self.results['tested_at']}\n")
            f.write(f"**–í—Å—å–æ–≥–æ –º–æ–¥–µ–ª–µ–π:** 58\n")
            f.write(f"**–ü—Ä–∞—Ü—é—î:** {len(self.results['working_models'])}\n")
            f.write(f"**–ù–µ –ø—Ä–∞—Ü—é—î:** {len(self.results['failed_models'])}\n\n")
            
            if self.results["working_models"]:
                f.write("## ‚úÖ –ü–†–ê–¶–Æ–Æ–ß–Ü –ú–û–î–ï–õ–Ü\n\n")
                for model in self.results["working_models"]:
                    f.write(f"- `{model}`\n")
                f.write("\n")
            
            if self.results["failed_models"]:
                f.write("## ‚ùå –ù–ï –ü–†–ê–¶–Æ–Æ–ß–Ü –ú–û–î–ï–õ–Ü\n\n")
                for model in self.results["failed_models"]:
                    f.write(f"- `{model}`\n")
        
        print(f"\nüìä –ó–≤—ñ—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ:")
        print(f"  üìÑ JSON: {json_path}")
        print(f"  üêç Python: {working_path}")
        print(f"  üìã Markdown: {md_path}")
        
        return json_path, working_path, md_path

async def main():
    print("üß™ –¢–ï–°–¢–£–í–ê–ù–ù–Ø 58 –ú–û–î–ï–õ–ï–ô –ó INTELLIGENT_MODEL_DISTRIBUTION")
    print("=" * 60)
    
    tester = ExactModelTester()
    
    try:
        await tester.test_all_models()
        json_path, working_path, md_path = tester.save_results()
        
        print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"‚úÖ –ü—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {len(tester.results['working_models'])}")
        print(f"‚ùå –ù–µ –ø—Ä–∞—Ü—é—é—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {len(tester.results['failed_models'])}")
        
        if len(tester.results['working_models']) < 58:
            print(f"\n‚ö†Ô∏è –£–í–ê–ì–ê: {58 - len(tester.results['working_models'])} –º–æ–¥–µ–ª–µ–π –Ω–µ –ø—Ä–∞—Ü—é—é—Ç—å!")
            print("üîß –ü–æ—Ç—Ä—ñ–±–Ω–æ:")
            print("1. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —Å–µ—Ä–≤–µ—Ä—ñ–≤")
            print("2. –î–æ–¥–∞—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤")
            print("3. –û–Ω–æ–≤–∏—Ç–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª —Ç—ñ–ª—å–∫–∏ –∑ –ø—Ä–∞—Ü—é—é—á–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏")
        else:
            print("üéâ –í—Å—ñ –º–æ–¥–µ–ª—ñ –ø—Ä–∞—Ü—é—é—Ç—å –≤—ñ–¥–º—ñ–Ω–Ω–æ!")
            
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        logging.error(f"Critical error: {e}")

if __name__ == "__main__":
    asyncio.run(main())
