#!/usr/bin/env python3
"""
üîç Predator11 Components Usage Analysis
–ê–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –ø—Ä–æ–µ–∫—Ç—É —Ç–∞ –≤–∏—è–≤–ª–µ–Ω–Ω—è –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Set, Any
from collections import defaultdict

def analyze_project_structure() -> Dict[str, Any]:
    """–ê–Ω–∞–ª—ñ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –ø—Ä–æ–µ–∫—Ç—É"""
    
    base_path = Path(__file__).parent.parent
    
    # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ —è–∫—ñ –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ –∑–∞–¥—ñ—è–Ω—ñ
    expected_components = {
        'agents': {
            'chief': ['chief_orchestrator.py', 'Dockerfile', 'requirements.txt'],
            'model-router': ['model_router.py', 'Dockerfile', 'requirements.txt'],
            'ingest': ['ingest_agent.py', 'Dockerfile', 'requirements.txt'],
            'anomaly': ['anomaly_agent.py', 'Dockerfile', 'requirements.txt'],
            'synthetic': ['synthetic_agent.py', 'Dockerfile', 'requirements.txt'],
            'data-quality': ['quality_agent.py', 'Dockerfile', 'requirements.txt'],
            'security-privacy': ['security_privacy_agent.py', 'Dockerfile', 'requirements.txt'],
            'self-healing': ['self_healing_agent.py', 'Dockerfile', 'requirements.txt']
        },
        'backend': {
            'core_files': ['Dockerfile', 'requirements.txt'],
            'app': ['main.py', 'config.py', 'celery_app.py'],
            'legacy_agents': ['agents']
        },
        'frontend': {
            'core_files': ['Dockerfile', 'package.json'],
            'src': ['App.tsx', 'main.tsx'],
            'public': ['index.html']
        },
        'infrastructure': {
            'docker-compose.yml': True,
            'prometheus': ['prometheus.yml'],
            'grafana': ['provisioning', 'dashboards'],
            'opensearch': ['mappings', 'dashboards'],
            'observability': ['alertmanager', 'prometheus', 'grafana'],
            'infra': ['terraform', 'k8s', 'helm']
        },
        'etl': {
            'core_files': ['Dockerfile', 'requirements.txt'],
            'dags': ['predator_etl_dag.py'],
            'parsing': ['pandas-pipelines'],
            'transforms': ['transforms']
        }
    }
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
    analysis_results = {
        'existing_components': {},
        'missing_components': {},
        'unused_components': {},
        'docker_services': {},
        'recommendations': []
    }
    
    # –ê–Ω–∞–ª—ñ–∑ docker-compose.yml
    docker_compose_path = base_path / 'docker-compose.yml'
    docker_services: Set[str] = set()

    if docker_compose_path.exists():
        try:
            import yaml  # type: ignore

            compose_data = yaml.safe_load(docker_compose_path.read_text())
            services_section = compose_data.get('services', {}) if isinstance(compose_data, dict) else {}
            if isinstance(services_section, dict):
                docker_services = set(services_section.keys())
                analysis_results['docker_services'] = sorted(docker_services)
            else:
                analysis_results['docker_services'] = []
        except Exception as e:
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–∏–π —Ç–µ–∫—Å—Ç–æ–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥, —è–∫—â–æ PyYAML –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π
            try:
                with open(docker_compose_path, 'r', encoding='utf-8') as f:
                    inside_services = False
                    for raw_line in f:
                        line = raw_line.rstrip()
                        if line.strip().startswith('#'):
                            continue
                        if line.startswith('services:'):
                            inside_services = True
                            continue
                        if inside_services:
                            if line and not line.startswith(' '):
                                # –í–∏–π—à–ª–∏ –∑ –±–ª–æ–∫—É services
                                break
                            striped = line.strip()
                            if striped.endswith(':') and not striped.startswith('-'):
                                docker_services.add(striped.rstrip(':'))
                analysis_results['docker_services'] = sorted(docker_services)
            except Exception as fallback_error:
                analysis_results['docker_services'] = f"Error reading docker-compose.yml: {fallback_error or e}"
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ agents
    agents_path = base_path / 'agents'
    if agents_path.exists():
        for agent_name, required_files in expected_components['agents'].items():
            agent_path = agents_path / agent_name
            component_key = f'agents/{agent_name}'
            analysis_results['existing_components'][component_key] = {
                'exists': agent_path.exists(),
                'files': []
            }

            if agent_path.exists():
                existing_files = sorted(f.name for f in agent_path.iterdir() if f.is_file())
                analysis_results['existing_components'][component_key]['files'] = existing_files

                missing_files = [req for req in required_files if req not in existing_files]
                if not any(name.endswith('_agent.py') for name in existing_files):
                    missing_files.append('*.py (agent entry point)')
                if missing_files:
                    analysis_results['missing_components'][component_key] = missing_files
            else:
                analysis_results['missing_components'][component_key] = 'Directory missing'
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ backend/agents (legacy)
    backend_agents_path = base_path / 'backend' / 'agents'
    if backend_agents_path.exists():
        legacy_agents = [d.name for d in backend_agents_path.iterdir() if d.is_dir()]
        analysis_results['unused_components']['backend/agents'] = {
            'legacy_agents': legacy_agents,
            'status': 'Should be replaced by new agents structure'
        }
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ—Å–Ω–æ–≤–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
    main_components = {
        'backend': base_path / 'backend',
        'frontend': base_path / 'frontend', 
        'etl': base_path / 'etl',
        'prometheus': base_path / 'prometheus',
        'grafana': base_path / 'grafana',
        'opensearch': base_path / 'opensearch',
        'observability': base_path / 'observability',
        'infra': base_path / 'infra'
    }
    
    for comp_name, comp_path in main_components.items():
        analysis_results['existing_components'][comp_name] = {
            'exists': comp_path.exists(),
            'size_mb': get_dir_size(comp_path) if comp_path.exists() else 0
        }
    
    # –í–∏—è–≤–ª–µ–Ω–Ω—è –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö –¥—É–±–ª—å–æ–≤–∞–Ω–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—ñ–≤
    duplicate_candidates = {
        'alertmanager.yml': [
            base_path / 'prometheus/alertmanager.yml',
            base_path / 'observability/alertmanager/alertmanager.yml'
        ],
        'prometheus.yml': [
            base_path / 'prometheus/prometheus.yml',
            base_path / 'observability/prometheus/prometheus.yml'
        ]
    }

    duplicates: Dict[str, List[str]] = {}
    for name, paths in duplicate_candidates.items():
        existing_paths = [str(path.relative_to(base_path)) for path in paths if path.exists()]
        if len(existing_paths) > 1:
            duplicates[name] = existing_paths

    if duplicates:
        analysis_results['duplicate_configs'] = duplicates

    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
    recommendations = []
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ legacy agents
    if 'backend/agents' in analysis_results['unused_components']:
        recommendations.append({
            'type': 'cleanup',
            'priority': 'high',
            'action': 'Remove legacy backend/agents directory',
            'reason': 'Replaced by new agents structure in ./agents/',
            'command': 'rm -rf backend/agents/'
        })
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
    for comp_name, missing in analysis_results['missing_components'].items():
        if isinstance(missing, list):
            recommendations.append({
                'type': 'missing',
                'priority': 'medium',
                'action': f'Create missing files in {comp_name}',
                'reason': f'Missing required files: {", ".join(missing)}',
                'files': missing
            })
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ Docker —Å–µ—Ä–≤—ñ—Å—ñ–≤
    new_agent_services = {
        'chief-orchestrator', 'model-router', 'ingest-agent', 
        'anomaly-agent', 'synthetic-agent', 'data-quality-agent',
        'security-privacy-agent', 'self-healing-agent'
    }
    
    missing_services = new_agent_services - docker_services
    if missing_services:
        recommendations.append({
            'type': 'docker',
            'priority': 'high', 
            'action': 'Add missing agent services to docker-compose.yml',
            'reason': 'New agent services not defined in docker-compose',
            'services': list(missing_services)
        })

    if duplicates:
        for name, locations in duplicates.items():
            recommendations.append({
                'type': 'cleanup',
                'priority': 'medium',
                'action': f'Consolidate duplicated {name} configurations',
                'reason': f'Duplicate configurations found in: {", ".join(locations)}',
                'files': locations
            })

    analysis_results['recommendations'] = recommendations
    
    return analysis_results

def get_dir_size(path: Path) -> float:
    """–û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –≤ MB"""
    try:
        total_size = 0
        for f in path.rglob('*'):
            if f.is_file():
                total_size += f.stat().st_size
        return round(total_size / 1024 / 1024, 2)
    except:
        return 0

def print_analysis_report(analysis: Dict[str, Any]):
    """–í–∏–≤–µ–¥–µ–Ω–Ω—è –∑–≤—ñ—Ç—É –∞–Ω–∞–ª—ñ–∑—É"""
    
    print("üîç PREDATOR11 COMPONENTS ANALYSIS REPORT")
    print("‚ïê" * 60)
    
    # –Ü—Å–Ω—É—é—á—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
    print("\nüìã EXISTING COMPONENTS:")
    print("-" * 30)
    for comp_name, info in analysis['existing_components'].items():
        status = "‚úÖ" if info['exists'] else "‚ùå"
        size_info = f" ({info.get('size_mb', 0)} MB)" if 'size_mb' in info else ""
        print(f"  {status} {comp_name}{size_info}")
        
        if 'files' in info and info['files']:
            print(f"      Files: {', '.join(info['files'][:5])}")
            if len(info['files']) > 5:
                print(f"      ... and {len(info['files']) - 5} more")
    
    # –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
    if analysis['missing_components']:
        print(f"\n‚ùå MISSING COMPONENTS ({len(analysis['missing_components'])}):")
        print("-" * 30)
        for comp_name, missing in analysis['missing_components'].items():
            print(f"  üî∏ {comp_name}: {missing}")
    
    # –ù–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
    if analysis['unused_components']:
        print(f"\nüóëÔ∏è UNUSED COMPONENTS ({len(analysis['unused_components'])}):")
        print("-" * 30)
        for comp_name, info in analysis['unused_components'].items():
            print(f"  üî∏ {comp_name}: {info}")
    
    # Docker —Å–µ—Ä–≤—ñ—Å–∏
    print(f"\nüê≥ DOCKER SERVICES ({len(analysis['docker_services'])}):")
    print("-" * 30)
    if isinstance(analysis['docker_services'], list):
        for service in sorted(analysis['docker_services']):
            print(f"  üîπ {service}")
    else:
        print(f"  ‚ùå {analysis['docker_services']}")

    duplicate_configs = analysis.get('duplicate_configs', {})
    if duplicate_configs:
        print(f"\n‚ö†Ô∏è DUPLICATE CONFIGURATIONS ({len(duplicate_configs)}):")
        print("-" * 30)
        for name, locations in duplicate_configs.items():
            print(f"  üî∏ {name}: {', '.join(locations)}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
    if analysis['recommendations']:
        print(f"\nüí° RECOMMENDATIONS ({len(analysis['recommendations'])}):")
        print("-" * 30)
        for i, rec in enumerate(analysis['recommendations'], 1):
            priority_icon = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}
            icon = priority_icon.get(rec['priority'], "üîµ")
            
            print(f"  {i}. {icon} [{rec['type'].upper()}] {rec['action']}")
            print(f"      Reason: {rec['reason']}")
            
            if 'command' in rec:
                print(f"      Command: {rec['command']}")
            if 'files' in rec:
                print(f"      Files: {', '.join(rec['files'])}")
            if 'services' in rec:
                print(f"      Services: {', '.join(rec['services'])}")
            print()
    
    # –ü—ñ–¥—Å—É–º–æ–∫
    total_existing = len([c for c in analysis['existing_components'].values() if c['exists']])
    total_missing = len(analysis['missing_components'])
    total_unused = len(analysis['unused_components'])
    
    print("üìä SUMMARY:")
    print("-" * 30)
    print(f"  ‚úÖ Existing components: {total_existing}")
    print(f"  ‚ùå Missing components:  {total_missing}")
    print(f"  üóëÔ∏è Unused components:   {total_unused}")
    print(f"  üí° Recommendations:     {len(analysis['recommendations'])}")
    
    # –û—Ü—ñ–Ω–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—ñ
    if total_missing == 0 and total_unused == 0:
        print(f"\nüéâ PROJECT STATUS: READY FOR PRODUCTION!")
    elif total_missing <= 2 and total_unused <= 1:
        print(f"\n‚ö° PROJECT STATUS: ALMOST READY (minor cleanup needed)")
    else:
        print(f"\nüîß PROJECT STATUS: NEEDS ATTENTION (see recommendations)")

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    try:
        print("Starting Predator11 components analysis...")
        analysis = analyze_project_structure()
        print_analysis_report(analysis)
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤ JSON
        output_file = Path(__file__).parent.parent / 'components_analysis.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nüìÑ Detailed results saved to: {output_file}")
        
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main())
