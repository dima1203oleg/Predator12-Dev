#!/usr/bin/env python3
"""
üß¨ Synthetic Data Agent - Automated Dataset Generation
–ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ –∑ —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏–º–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª–∞–º–∏ —Ç–∞ –∞–Ω–æ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—î—é
"""

import json
import time
import random
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
import pandas as pd
import numpy as np

import redis
from fastapi import FastAPI, HTTPException
from faker import Faker
import structlog

logger = structlog.get_logger(__name__)

@dataclass
class SyntheticConfig:
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    schema: Dict[str, Any]
    rows: int = 10000
    anomalies_pct: float = 0.05
    noise_level: float = 0.1
    mimic_dataset_id: Optional[str] = None
    pii_policy: str = "mask"  # mask, remove, pseudonymize
    output_format: str = "csv"  # csv, json, parquet

class SyntheticDataAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    
    def __init__(self):
        self.app = FastAPI(title="Synthetic Data Agent", version="1.0.0")
        self.redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)
        self.faker = Faker(['en_US', 'uk_UA'])  # –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –¥–∞–Ω–∏—Ö
        
        # –®–∞–±–ª–æ–Ω–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö
        self.data_generators = {
            'string': self._generate_string,
            'integer': self._generate_integer,
            'float': self._generate_float,
            'date': self._generate_date,
            'email': self._generate_email,
            'phone': self._generate_phone,
            'company': self._generate_company,
            'address': self._generate_address,
            'hs_code': self._generate_hs_code,
            'amount': self._generate_amount,
            'country': self._generate_country
        }
        
        self._setup_routes()
    
    def _setup_routes(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è HTTP –º–∞—Ä—à—Ä—É—Ç—ñ–≤"""
        
        @self.app.post("/synthetic/generate")
        async def generate_dataset(request: dict):
            """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
            try:
                config = SyntheticConfig(
                    schema=request["schema"],
                    rows=request.get("rows", 10000),
                    anomalies_pct=request.get("anomalies_pct", 0.05),
                    noise_level=request.get("noise_level", 0.1),
                    mimic_dataset_id=request.get("mimic_dataset_id"),
                    pii_policy=request.get("pii_policy", "mask"),
                    output_format=request.get("output_format", "csv")
                )
                
                result = await self.generate_synthetic_data(config)
                return result
                
            except Exception as e:
                logger.error("Error generating synthetic data", error=str(e))
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/synthetic/anonymize")
        async def anonymize_data(request: dict):
            """–ê–Ω–æ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è —ñ—Å–Ω—É—é—á–∏—Ö –¥–∞–Ω–∏—Ö"""
            try:
                dataset_id = request["dataset_id"]
                pii_fields = request.get("pii_fields", [])
                method = request.get("method", "k_anonymity")
                
                result = await self.anonymize_dataset(dataset_id, pii_fields, method)
                return result
                
            except Exception as e:
                logger.error("Error anonymizing data", error=str(e))
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/synthetic/health")
        async def health():
            """Health check"""
            return {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "generators_available": len(self.data_generators)
            }
    
    async def generate_synthetic_data(self, config: SyntheticConfig) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é"""
        
        start_time = time.time()
        dataset_id = str(uuid.uuid4())
        
        logger.info("Starting synthetic data generation", 
                   dataset_id=dataset_id, rows=config.rows)
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –±–∞–∑–æ–≤—ñ –¥–∞–Ω—ñ
        data = {}
        for field_name, field_config in config.schema.items():
            field_type = field_config.get("type", "string")
            
            if field_type in self.data_generators:
                data[field_name] = self.data_generators[field_type](
                    config.rows, field_config
                )
            else:
                # Fallback –¥–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ —Ç–∏–ø—É
                data[field_name] = self._generate_string(config.rows, field_config)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame
        df = pd.DataFrame(data)
        
        # –î–æ–¥–∞—î–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó
        if config.anomalies_pct > 0:
            df = self._inject_anomalies(df, config.anomalies_pct)
        
        # –î–æ–¥–∞—î–º–æ —à—É–º
        if config.noise_level > 0:
            df = self._add_noise(df, config.noise_level)
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ PII –ø–æ–ª—ñ—Ç–∏–∫–∏
        df = self._apply_pii_policy(df, config.pii_policy, config.schema)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        file_path = f"/tmp/synthetic_{dataset_id}.{config.output_format}"
        
        if config.output_format == "csv":
            df.to_csv(file_path, index=False)
        elif config.output_format == "json":
            df.to_json(file_path, orient="records", indent=2)
        elif config.output_format == "parquet":
            df.to_parquet(file_path, index=False)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            "rows_generated": len(df),
            "columns": len(df.columns),
            "file_size_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
            "generation_time": round(time.time() - start_time, 2),
            "anomalies_injected": int(len(df) * config.anomalies_pct)
        }
        
        # –ü—É–±–ª—ñ–∫—É—î–º–æ –ø–æ–¥—ñ—é
        await self._publish_event("synthetic.generated", {
            "dataset_id": dataset_id,
            "config": config.__dict__,
            "stats": stats
        })
        
        return {
            "dataset_id": dataset_id,
            "file_path": file_path,
            "stats": stats,
            "data_sample": df.head(5).to_dict('records')
        }
    
    def _generate_string(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä—è–¥–∫—ñ–≤"""
        min_len = config.get("min_length", 5)
        max_len = config.get("max_length", 50)
        pattern = config.get("pattern", "word")
        
        if pattern == "word":
            return [self.faker.word() for _ in range(count)]
        elif pattern == "sentence":
            return [self.faker.sentence() for _ in range(count)]
        elif pattern == "name":
            return [self.faker.name() for _ in range(count)]
        else:
            return [self.faker.text(max_nb_chars=random.randint(min_len, max_len))[:max_len] 
                   for _ in range(count)]
    
    def _generate_integer(self, count: int, config: Dict) -> List[int]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ü—ñ–ª–∏—Ö —á–∏—Å–µ–ª"""
        min_val = config.get("min_value", 1)
        max_val = config.get("max_value", 100000)
        distribution = config.get("distribution", "uniform")
        
        if distribution == "normal":
            mean = (min_val + max_val) / 2
            std = (max_val - min_val) / 6
            values = np.random.normal(mean, std, count)
            return np.clip(values, min_val, max_val).astype(int).tolist()
        else:
            return [random.randint(min_val, max_val) for _ in range(count)]
    
    def _generate_float(self, count: int, config: Dict) -> List[float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥—Ä–æ–±–æ–≤–∏—Ö —á–∏—Å–µ–ª"""
        min_val = config.get("min_value", 0.0)
        max_val = config.get("max_value", 1000.0)
        decimals = config.get("decimals", 2)
        distribution = config.get("distribution", "uniform")
        
        if distribution == "normal":
            mean = (min_val + max_val) / 2
            std = (max_val - min_val) / 6
            values = np.random.normal(mean, std, count)
            return np.clip(values, min_val, max_val).round(decimals).tolist()
        else:
            return [round(random.uniform(min_val, max_val), decimals) 
                   for _ in range(count)]
    
    def _generate_date(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞—Ç"""
        start_date = datetime.strptime(config.get("start_date", "2020-01-01"), "%Y-%m-%d")
        end_date = datetime.strptime(config.get("end_date", "2025-12-31"), "%Y-%m-%d")
        
        dates = []
        for _ in range(count):
            random_date = self.faker.date_between(start_date=start_date, end_date=end_date)
            dates.append(random_date.isoformat())
        
        return dates
    
    def _generate_email(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è email –∞–¥—Ä–µ—Å"""
        return [self.faker.email() for _ in range(count)]
    
    def _generate_phone(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–∏—Ö –Ω–æ–º–µ—Ä—ñ–≤"""
        country = config.get("country", "UA")
        if country == "UA":
            return [f"+380{random.randint(100000000, 999999999)}" for _ in range(count)]
        else:
            return [self.faker.phone_number() for _ in range(count)]
    
    def _generate_company(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–∞–∑–≤ –∫–æ–º–ø–∞–Ω—ñ–π"""
        return [self.faker.company() for _ in range(count)]
    
    def _generate_address(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∞–¥—Ä–µ—Å"""
        return [self.faker.address().replace('\n', ', ') for _ in range(count)]
    
    def _generate_hs_code(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è HS –∫–æ–¥—ñ–≤ (–º–∏—Ç–Ω—ñ –∫–æ–¥–∏)"""
        # –†–µ–∞–ª—å–Ω—ñ HS –∫–æ–¥–∏ –¥–ª—è –º–∏—Ç–Ω–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏
        common_hs = [
            "8544", "8517", "8471", "8473", "8708", "8703", 
            "2710", "7208", "7326", "3004", "6203", "6109"
        ]
        
        codes = []
        for _ in range(count):
            if random.random() < 0.7:  # 70% —Ä–µ–∞–ª—å–Ω–∏—Ö –∫–æ–¥—ñ–≤
                base = random.choice(common_hs)
                suffix = f"{random.randint(10, 99)}{random.randint(10, 99)}"
                codes.append(f"{base}{suffix}")
            else:  # 30% –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö
                codes.append(f"{random.randint(1000, 9999)}{random.randint(1000, 9999)}")
        
        return codes
    
    def _generate_amount(self, count: int, config: Dict) -> List[float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≥—Ä–æ—à–æ–≤–∏—Ö —Å—É–º"""
        min_amount = config.get("min_amount", 100)
        max_amount = config.get("max_amount", 1000000)
        currency = config.get("currency", "USD")
        
        # –õ–æ–≥–Ω–æ—Ä–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–ª—è —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö —Å—É–º
        amounts = np.random.lognormal(
            mean=np.log(min_amount * 10), 
            sigma=1.0, 
            size=count
        )
        
        amounts = np.clip(amounts, min_amount, max_amount)
        return amounts.round(2).tolist()
    
    def _generate_country(self, count: int, config: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫—Ä–∞—ó–Ω"""
        region = config.get("region", "all")
        
        if region == "europe":
            countries = ["Ukraine", "Poland", "Germany", "France", "Italy", "Spain"]
        elif region == "asia":
            countries = ["China", "Japan", "South Korea", "India", "Thailand", "Vietnam"]
        else:
            countries = [self.faker.country() for _ in range(min(50, count))]
        
        return [random.choice(countries) for _ in range(count)]
    
    def _inject_anomalies(self, df: pd.DataFrame, anomaly_rate: float) -> pd.DataFrame:
        """–î–æ–¥–∞–≤–∞–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
        
        num_anomalies = int(len(df) * anomaly_rate)
        anomaly_indices = random.sample(range(len(df)), num_anomalies)
        
        for idx in anomaly_indices:
            # –í–∏–±–∏—Ä–∞—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—É –∫–æ–ª–æ–Ω–∫—É –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ—ó
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                col = random.choice(numeric_cols)
                # –ú–Ω–æ–∂–∏–º–æ –Ω–∞ –≤–∏–ø–∞–¥–∫–æ–≤–∏–π —Ñ–∞–∫—Ç–æ—Ä –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ—ó
                factor = random.choice([0.1, 0.01, 10, 100])
                df.loc[idx, col] = df.loc[idx, col] * factor
        
        return df
    
    def _add_noise(self, df: pd.DataFrame, noise_level: float) -> pd.DataFrame:
        """–î–æ–¥–∞–≤–∞–Ω–Ω—è —à—É–º—É –¥–æ —á–∏—Å–ª–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            col_std = df[col].std()
            noise = np.random.normal(0, col_std * noise_level, len(df))
            df[col] = df[col] + noise
        
        return df
    
    def _apply_pii_policy(self, df: pd.DataFrame, policy: str, schema: Dict) -> pd.DataFrame:
        """–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ–ª—ñ—Ç–∏–∫–∏ PII"""
        
        pii_fields = []
        for field_name, field_config in schema.items():
            if field_config.get("is_pii", False):
                pii_fields.append(field_name)
        
        for field in pii_fields:
            if field in df.columns:
                if policy == "mask":
                    df[field] = df[field].apply(lambda x: "***MASKED***")
                elif policy == "remove":
                    df = df.drop(columns=[field])
                elif policy == "pseudonymize":
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø—Å–µ–≤–¥–æ–Ω—ñ–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ö–µ—à—É
                    df[field] = df[field].apply(lambda x: f"pseudo_{hash(str(x)) % 100000}")
        
        return df
    
    async def anonymize_dataset(self, dataset_id: str, pii_fields: List[str], 
                              method: str) -> Dict[str, Any]:
        """–ê–Ω–æ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è —ñ—Å–Ω—É—é—á–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
        
        # –¢—É—Ç –º–∞—î –±—É—Ç–∏ –ª–æ–≥—ñ–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
        # —Ç–∞ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤ –∞–Ω–æ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—ó (k-anonymity, l-diversity, t-closeness)
        
        return {
            "dataset_id": dataset_id,
            "anonymization_method": method,
            "pii_fields_processed": pii_fields,
            "status": "completed"
        }
    
    async def _publish_event(self, event_type: str, data: Dict[str, Any]):
        """–ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ–¥—ñ—ó –≤ Redis Streams"""
        try:
            event_data = {
                "event_type": event_type,
                "timestamp": datetime.now().isoformat(),
                "source": "SyntheticDataAgent",
                **data
            }
            
            self.redis_client.xadd("pred:events:synthetic", event_data)
            logger.debug("Event published", event_type=event_type)
            
        except Exception as e:
            logger.error("Failed to publish event", error=str(e))

# –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞
if __name__ == "__main__":
    import uvicorn
    
    agent = SyntheticDataAgent()
    uvicorn.run(agent.app, host="0.0.0.0", port=9015)
