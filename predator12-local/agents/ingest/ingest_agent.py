#!/usr/bin/env python3
"""
üì• Ingest Agent - File/Stream Ingestion with Profiling & PII Scanning
–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤/–ø–æ—Ç–æ–∫—ñ–≤ –∑ –ø—Ä–æ—Ñ—ñ–ª—é–≤–∞–Ω–Ω—è–º —Ç–∞ PII —Å–∫–∞–Ω–æ–º
"""

import os
import hashlib
import mimetypes
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Any, BinaryIO
from dataclasses import dataclass
from pathlib import Path
import re

import redis
import aiofiles
from fastapi import FastAPI, HTTPException, File, UploadFile, Form
from fastapi.responses import JSONResponse
import pandas as pd
import structlog
from minio import Minio
import psycopg2
from opensearchpy import OpenSearch

logger = structlog.get_logger(__name__)

@dataclass
class FileMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É"""
    filename: str
    size: int
    mime_type: str
    md5_hash: str
    upload_time: datetime
    user_id: str
    
@dataclass 
class DataProfile:
    """–ü—Ä–æ—Ñ—ñ–ª—å –¥–∞–Ω–∏—Ö"""
    row_count: int
    column_count: int
    columns_info: Dict[str, Any]
    data_types: Dict[str, str]
    null_counts: Dict[str, int]
    unique_counts: Dict[str, int]
    sample_data: List[Dict[str, Any]]
    
@dataclass
class PIIFindings:
    """–ó–Ω–∞–π–¥–µ–Ω—ñ PII –¥–∞–Ω—ñ"""
    has_pii: bool
    pii_columns: List[str]
    pii_types: Dict[str, List[str]]  # column -> [pii_type1, pii_type2]
    confidence_scores: Dict[str, float]

class PIIDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    
    def __init__(self):
        # –†–µ–≥—É–ª—è—Ä–Ω—ñ –≤–∏—Ä–∞–∑–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ PII
        self.patterns = {
            "email": re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            "phone": re.compile(r'\b(?:\+?38)?(?:\(0\d{2}\)|\d{3})\s?\d{3}[-\s]?\d{2}[-\s]?\d{2}\b'),
            "inn": re.compile(r'\b\d{10}\b'),
            "passport": re.compile(r'\b[–ê-–Ø]{2}\d{6}\b'),
            "card_number": re.compile(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'),
            "iban": re.compile(r'\bUA\d{27}\b'),
            "social_security": re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
        }
        
        # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è —Å—Ç–æ–≤–ø—Ü—ñ–≤
        self.column_keywords = {
            "email": ["email", "e-mail", "–µ–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞", "–ø–æ—à—Ç–∞", "mail"],
            "phone": ["phone", "tel", "mobile", "—Ç–µ–ª–µ—Ñ–æ–Ω", "–º–æ–±"],
            "name": ["name", "surname", "firstname", "lastname", "—ñ–º'—è", "–ø—Ä—ñ–∑–≤–∏—â–µ"],
            "address": ["address", "–∞–¥—Ä–µ—Å–∞", "–≤—É–ª–∏—Ü—è", "–º—ñ—Å—Ç–æ", "–æ–±–ª–∞—Å—Ç—å"],
            "birth_date": ["birth", "birthday", "–¥–∞—Ç–∞", "–Ω–∞—Ä–æ–¥–∂–µ–Ω–Ω—è", "birth_date"],
            "inn": ["inn", "—ñ–Ω–Ω", "–∫–æ–¥", "tax"],
            "passport": ["passport", "–ø–∞—Å–ø–æ—Ä—Ç", "–¥–æ–∫—É–º–µ–Ω—Ç"]
        }
    
    def detect_pii(self, df: pd.DataFrame) -> PIIFindings:
        """–í–∏—è–≤–ª–µ–Ω–Ω—è PII –≤ DataFrame"""
        
        pii_columns = []
        pii_types = {}
        confidence_scores = {}
        
        for column in df.columns:
            column_pii_types = []
            max_confidence = 0.0
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞–∑–≤–∏ —Å—Ç–æ–≤–ø—Ü—è
            col_lower = column.lower()
            for pii_type, keywords in self.column_keywords.items():
                if any(keyword in col_lower for keyword in keywords):
                    column_pii_types.append(pii_type)
                    max_confidence = max(max_confidence, 0.8)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–º—ñ—Å—Ç—É —Å—Ç–æ–≤–ø—Ü—è (sample)
            sample_values = df[column].dropna().astype(str).head(100)
            
            for pii_type, pattern in self.patterns.items():
                matches = sum(1 for value in sample_values if pattern.search(value))
                if matches > 0:
                    match_ratio = matches / len(sample_values)
                    if match_ratio >= 0.1:  # 10% —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω—å
                        column_pii_types.append(pii_type)
                        max_confidence = max(max_confidence, match_ratio)
            
            if column_pii_types:
                pii_columns.append(column)
                pii_types[column] = list(set(column_pii_types))
                confidence_scores[column] = max_confidence
        
        return PIIFindings(
            has_pii=len(pii_columns) > 0,
            pii_columns=pii_columns,
            pii_types=pii_types,
            confidence_scores=confidence_scores
        )

class DataProfiler:
    """–ü—Ä–æ—Ñ—ñ–ª—é–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö"""
    
    def profile_dataframe(self, df: pd.DataFrame) -> DataProfile:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é DataFrame"""
        
        columns_info = {}
        data_types = {}
        null_counts = {}
        unique_counts = {}
        
        for column in df.columns:
            series = df[column]
            
            # –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
            data_types[column] = str(series.dtype)
            null_counts[column] = series.isnull().sum()
            unique_counts[column] = series.nunique()
            
            # –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
            col_info = {
                "dtype": str(series.dtype),
                "null_count": int(series.isnull().sum()),
                "unique_count": int(series.nunique()),
                "null_percentage": round(series.isnull().sum() / len(series) * 100, 2)
            }
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
            if pd.api.types.is_numeric_dtype(series):
                col_info.update({
                    "min": float(series.min()) if not series.empty else None,
                    "max": float(series.max()) if not series.empty else None,
                    "mean": float(series.mean()) if not series.empty else None,
                    "std": float(series.std()) if not series.empty else None
                })
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
            elif pd.api.types.is_string_dtype(series):
                non_null_series = series.dropna()
                if not non_null_series.empty:
                    col_info.update({
                        "min_length": int(non_null_series.str.len().min()),
                        "max_length": int(non_null_series.str.len().max()),
                        "avg_length": round(non_null_series.str.len().mean(), 2)
                    })
            
            columns_info[column] = col_info
        
        # Sample –¥–∞–Ω–∏—Ö (–ø–µ—Ä—à—ñ 5 —Ä—è–¥–∫—ñ–≤)
        sample_data = df.head(5).to_dict('records')
        
        return DataProfile(
            row_count=len(df),
            column_count=len(df.columns),
            columns_info=columns_info,
            data_types=data_types,
            null_counts=null_counts,
            unique_counts=unique_counts,
            sample_data=sample_data
        )

class IngestAgent:
    """Ingest Agent - –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤"""
    
    def __init__(self):
        self.app = FastAPI(title="Ingest Agent", version="1.0.0")
        
        # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ —Å–µ—Ä–≤—ñ—Å—ñ–≤
        self.redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)
        self.minio_client = Minio(
            'minio:9000',
            access_key='minioadmin',
            secret_key='minioadmin',
            secure=False
        )
        self.opensearch_client = OpenSearch([{'host': 'opensearch', 'port': 9200}])
        
        # –£—Ç–∏–ª—ñ—Ç–∏
        self.pii_detector = PIIDetector()
        self.profiler = DataProfiler()
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
        self.bucket_name = "predator11-raw"
        self.max_file_size = 500 * 1024 * 1024  # 500MB
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è bucket —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
        self._ensure_bucket()
        
        self._setup_routes()
    
    def _ensure_bucket(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è MinIO bucket"""
        try:
            if not self.minio_client.bucket_exists(self.bucket_name):
                self.minio_client.make_bucket(self.bucket_name)
                logger.info("Created MinIO bucket", bucket=self.bucket_name)
        except Exception as e:
            logger.error("Failed to create bucket", error=str(e))
    
    def _setup_routes(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è HTTP –º–∞—Ä—à—Ä—É—Ç—ñ–≤"""
        
        @self.app.post("/ingest/upload")
        async def upload_file(
            file: UploadFile = File(...),
            user_id: str = Form("system"),
            tags: Optional[str] = Form(None)
        ):
            """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É"""
            try:
                # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–æ–∑–º—ñ—Ä—É
                if file.size and file.size > self.max_file_size:
                    raise HTTPException(400, f"File too large: {file.size} > {self.max_file_size}")
                
                # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É
                result = await self.process_upload(file, user_id, tags)
                return result
                
            except Exception as e:
                logger.error("Upload failed", filename=file.filename, error=str(e))
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/ingest/commit/{file_id}")
        async def commit_file(file_id: str):
            """–ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è"""
            try:
                result = await self.commit_and_index(file_id)
                return result
            except Exception as e:
                logger.error("Commit failed", file_id=file_id, error=str(e))
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/ingest/profile/{file_id}")
        async def get_profile(file_id: str):
            """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é —Ñ–∞–π–ª—É"""
            try:
                # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é –∑ Redis
                profile_data = self.redis_client.get(f"profile:{file_id}")
                if not profile_data:
                    raise HTTPException(404, "Profile not found")
                
                import json
                return json.loads(profile_data)
            except Exception as e:
                logger.error("Failed to get profile", file_id=file_id, error=str(e))
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/ingest/health")
        async def health():
            """Health check"""
            try:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ —Å–µ—Ä–≤—ñ—Å—ñ–≤
                self.redis_client.ping()
                self.minio_client.list_buckets()
                self.opensearch_client.cluster.health()
                
                return {"status": "healthy", "timestamp": datetime.now().isoformat()}
            except Exception as e:
                return {"status": "unhealthy", "error": str(e)}
    
    async def process_upload(self, file: UploadFile, user_id: str, tags: Optional[str]) -> Dict[str, Any]:
        """–û–±—Ä–æ–±–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É"""
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è ID —Ñ–∞–π–ª—É
        file_id = hashlib.md5(f"{file.filename}{datetime.now()}".encode()).hexdigest()
        
        # –ß–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É
        content = await file.read()
        
        # –ú–µ—Ç–∞–¥–∞–Ω—ñ
        metadata = FileMetadata(
            filename=file.filename,
            size=len(content),
            mime_type=file.content_type or mimetypes.guess_type(file.filename)[0],
            md5_hash=hashlib.md5(content).hexdigest(),
            upload_time=datetime.now(),
            user_id=user_id
        )
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ MinIO
        object_name = f"raw/{file_id}/{file.filename}"
        self.minio_client.put_object(
            self.bucket_name,
            object_name,
            data=BytesIO(content),
            length=len(content),
            content_type=metadata.mime_type
        )
        
        # –ü—Ä–æ—Ñ—ñ–ª—é–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö (—è–∫—â–æ —Ü–µ CSV/Excel)
        profile = None
        pii_findings = None
        
        if self._is_tabular_file(file.filename):
            try:
                df = await self._read_tabular_file(content, file.filename)
                profile = self.profiler.profile_dataframe(df)
                pii_findings = self.pii_detector.detect_pii(df)
                
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é
                import json
                profile_data = {
                    "metadata": metadata.__dict__,
                    "profile": profile.__dict__,
                    "pii_findings": pii_findings.__dict__ if pii_findings else None
                }
                
                self.redis_client.setex(
                    f"profile:{file_id}", 
                    3600,  # 1 –≥–æ–¥–∏–Ω–∞ TTL
                    json.dumps(profile_data, default=str)
                )
                
            except Exception as e:
                logger.warning("Failed to profile data", filename=file.filename, error=str(e))
        
        # –ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ–¥—ñ—ó
        await self._publish_event("dataset.uploaded", {
            "file_id": file_id,
            "filename": file.filename,
            "size": metadata.size,
            "user_id": user_id,
            "has_pii": pii_findings.has_pii if pii_findings else False,
            "row_count": profile.row_count if profile else None
        })
        
        return {
            "file_id": file_id,
            "filename": file.filename,
            "size": metadata.size,
            "status": "uploaded",
            "has_profile": profile is not None,
            "has_pii": pii_findings.has_pii if pii_findings else False,
            "pii_columns": pii_findings.pii_columns if pii_findings else []
        }
    
    async def commit_and_index(self, file_id: str) -> Dict[str, Any]:
        """–ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è —Ç–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —Ñ–∞–π–ª—É"""
        
        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é
        import json
        profile_data = self.redis_client.get(f"profile:{file_id}")
        if not profile_data:
            raise Exception("Profile not found")
        
        data = json.loads(profile_data)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É –≤ OpenSearch
        index_name = f"dataset_{file_id}"
        
        # Mapping –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º PII
        mapping = {
            "mappings": {
                "properties": {
                    "file_id": {"type": "keyword"},
                    "filename": {"type": "text"},
                    "upload_time": {"type": "date"},
                    "user_id": {"type": "keyword"},
                    "row_count": {"type": "integer"},
                    "column_count": {"type": "integer"},
                    "has_pii": {"type": "boolean"},
                    "pii_columns": {"type": "keyword"},
                    "tags": {"type": "keyword"}
                }
            }
        }
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É
        self.opensearch_client.indices.create(
            index=index_name,
            body=mapping,
            ignore=400  # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —è–∫—â–æ –≤–∂–µ —ñ—Å–Ω—É—î
        )
        
        # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
        doc = {
            "file_id": file_id,
            "filename": data["metadata"]["filename"],
            "upload_time": data["metadata"]["upload_time"],
            "user_id": data["metadata"]["user_id"],
            "row_count": data["profile"]["row_count"] if data["profile"] else 0,
            "column_count": data["profile"]["column_count"] if data["profile"] else 0,
            "has_pii": data["pii_findings"]["has_pii"] if data["pii_findings"] else False,
            "pii_columns": data["pii_findings"]["pii_columns"] if data["pii_findings"] else [],
            "indexed_at": datetime.now().isoformat()
        }
        
        self.opensearch_client.index(
            index=index_name,
            id=file_id,
            body=doc
        )
        
        # –ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ–¥—ñ—ó
        await self._publish_event("dataset.indexed", {
            "file_id": file_id,
            "index_name": index_name,
            "has_pii": doc["has_pii"]
        })
        
        return {
            "file_id": file_id,
            "status": "indexed",
            "index_name": index_name,
            "document_id": file_id
        }
    
    def _is_tabular_file(self, filename: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —î —Ñ–∞–π–ª —Ç–∞–±–ª–∏—á–Ω–∏–º"""
        ext = Path(filename).suffix.lower()
        return ext in ['.csv', '.xlsx', '.xls', '.tsv', '.parquet']
    
    async def _read_tabular_file(self, content: bytes, filename: str) -> pd.DataFrame:
        """–ß–∏—Ç–∞–Ω–Ω—è —Ç–∞–±–ª–∏—á–Ω–æ–≥–æ —Ñ–∞–π–ª—É"""
        ext = Path(filename).suffix.lower()
        
        if ext == '.csv':
            return pd.read_csv(BytesIO(content))
        elif ext in ['.xlsx', '.xls']:
            return pd.read_excel(BytesIO(content))
        elif ext == '.tsv':
            return pd.read_csv(BytesIO(content), sep='\t')
        elif ext == '.parquet':
            return pd.read_parquet(BytesIO(content))
        else:
            raise ValueError(f"Unsupported file type: {ext}")
    
    async def _publish_event(self, event_type: str, data: Dict[str, Any]):
        """–ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ–¥—ñ—ó –≤ Redis Streams"""
        try:
            event_data = {
                "event_type": event_type,
                "timestamp": datetime.now().isoformat(),
                "source": "IngestAgent",
                **data
            }
            
            self.redis_client.xadd("pred:events:ingest", event_data)
            logger.debug("Event published", event_type=event_type)
            
        except Exception as e:
            logger.error("Failed to publish event", error=str(e))

# –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞
if __name__ == "__main__":
    import uvicorn
    from io import BytesIO
    
    agent = IngestAgent()
    uvicorn.run(agent.app, host="0.0.0.0", port=9010)
