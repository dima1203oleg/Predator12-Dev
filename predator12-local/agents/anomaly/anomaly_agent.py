#!/usr/bin/env python3
"""
üîç Anomaly Agent - Statistical & ML Anomaly Detection
–î–µ—Ç–µ–∫—Ü—ñ—è –∞–Ω–æ–º–∞–ª—ñ–π (online/batch) –∑ –ø–æ—è—Å–Ω–µ–Ω–Ω—è–º–∏
"""

import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
from enum import Enum

import redis
from fastapi import FastAPI, HTTPException
import structlog
from opensearchpy import OpenSearch
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

logger = structlog.get_logger(__name__)

class AnomalyMethod(Enum):
    STATISTICAL = "statistical"
    ISOLATION_FOREST = "isolation_forest"
    SEASONAL_ESD = "seasonal_esd"
    Z_SCORE = "z_score"
    IQR = "iqr"

@dataclass
class AnomalyDetectionRequest:
    """–ó–∞–ø–∏—Ç –Ω–∞ –¥–µ—Ç–µ–∫—Ü—ñ—é –∞–Ω–æ–º–∞–ª—ñ–π"""
    index: str
    field: str
    method: AnomalyMethod
    window: str = "7d"  # –ß–∞—Å–æ–≤–µ –≤—ñ–∫–Ω–æ
    threshold: float = 0.95  # –ü–æ—Ä—ñ–≥ –∞–Ω–æ–º–∞–ª—ñ—ó
    parameters: Dict[str, Any] = None

@dataclass
class AnomalyPoint:
    """–¢–æ—á–∫–∞ –∞–Ω–æ–º–∞–ª—ñ—ó"""
    timestamp: datetime
    value: float
    anomaly_score: float
    method: str
    explanation: str
    context: Dict[str, Any]

@dataclass
class AnomalyDetectionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü—ñ—ó –∞–Ω–æ–º–∞–ª—ñ–π"""
    request_id: str
    method: str
    total_points: int
    anomaly_count: int
    anomaly_percentage: float
    anomalies: List[AnomalyPoint]
    summary: str
    recommendations: List[str]

class StatisticalDetector:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª—ñ–π"""
    
    @staticmethod
    def z_score_detection(data: pd.Series, threshold: float = 3.0) -> Dict[str, Any]:
        """Z-score –¥–µ—Ç–µ–∫—Ü—ñ—è"""
        mean = data.mean()
        std = data.std()
        
        if std == 0:
            return {"anomalies": [], "scores": np.zeros(len(data))}
        
        z_scores = np.abs((data - mean) / std)
        anomaly_mask = z_scores > threshold
        
        return {
            "anomalies": data.index[anomaly_mask].tolist(),
            "scores": z_scores.values,
            "mean": mean,
            "std": std,
            "threshold": threshold
        }
    
    @staticmethod
    def iqr_detection(data: pd.Series, k: float = 1.5) -> Dict[str, Any]:
        """IQR (Interquartile Range) –¥–µ—Ç–µ–∫—Ü—ñ—è"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - k * IQR
        upper_bound = Q3 + k * IQR
        
        anomaly_mask = (data < lower_bound) | (data > upper_bound)
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ score —è–∫ –≤—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ –º–µ–∂
        scores = np.zeros(len(data))
        for i, val in enumerate(data):
            if val < lower_bound:
                scores[i] = abs(val - lower_bound) / (Q1 - lower_bound + 1e-6)
            elif val > upper_bound:
                scores[i] = abs(val - upper_bound) / (upper_bound - Q3 + 1e-6)
            else:
                scores[i] = 0
        
        return {
            "anomalies": data.index[anomaly_mask].tolist(),
            "scores": scores,
            "Q1": Q1,
            "Q3": Q3,
            "IQR": IQR,
            "lower_bound": lower_bound,
            "upper_bound": upper_bound
        }
    
    @staticmethod
    def seasonal_esd_detection(data: pd.Series, seasonality: int = 24, alpha: float = 0.05) -> Dict[str, Any]:
        """Seasonal Extreme Studentized Deviate (ESD) –¥–µ—Ç–µ–∫—Ü—ñ—è"""
        
        # –°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è Seasonal ESD
        # –í –ø–æ–≤–Ω—ñ–π —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ –± STL –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü—ñ—é
        
        n = len(data)
        if n < seasonality * 2:
            # Fallback to simple ESD
            return StatisticalDetector._simple_esd(data, alpha)
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç—Ä–µ–Ω–¥ —ñ —Å–µ–∑–æ–Ω–Ω—ñ—Å—Ç—å (—Å–ø—Ä–æ—â–µ–Ω–æ)
        rolling_mean = data.rolling(window=seasonality, center=True).mean()
        detrended = data - rolling_mean
        detrended = detrended.fillna(0)
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ ESD –¥–æ detrended –¥–∞–Ω–∏—Ö
        return StatisticalDetector._simple_esd(detrended, alpha)
    
    @staticmethod
    def _simple_esd(data: pd.Series, alpha: float = 0.05) -> Dict[str, Any]:
        """–°–ø—Ä–æ—â–µ–Ω–∏–π ESD —Ç–µ—Å—Ç"""
        from scipy import stats
        
        n = len(data)
        if n < 3:
            return {"anomalies": [], "scores": np.zeros(n)}
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∞–Ω–æ–º–∞–ª—ñ–π –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        max_outliers = max(1, int(n * 0.1))  # –î–æ 10% —Ç–æ—á–æ–∫
        
        outliers = []
        working_data = data.copy()
        original_indices = data.index.tolist()
        
        for i in range(max_outliers):
            if len(working_data) < 3:
                break
            
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±—ñ–ª—å—à–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è
            mean = working_data.mean()
            std = working_data.std()
            
            if std == 0:
                break
            
            deviations = np.abs(working_data - mean) / std
            max_deviation_idx = deviations.idxmax()
            max_deviation = deviations.max()
            
            # –ö—Ä–∏—Ç–∏—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
            t_val = stats.t.ppf(1 - alpha/(2*(len(working_data)-i)), len(working_data)-i-2)
            lambda_critical = ((len(working_data)-i-1) * t_val) / np.sqrt((len(working_data)-i-2 + t_val**2) * (len(working_data)-i))
            
            if max_deviation > lambda_critical:
                outliers.append(max_deviation_idx)
                working_data = working_data.drop(max_deviation_idx)
            else:
                break
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ scores
        scores = np.zeros(n)
        mean = data.mean()
        std = data.std()
        if std > 0:
            scores = np.abs(data - mean) / std
        
        return {
            "anomalies": outliers,
            "scores": scores,
            "alpha": alpha,
            "outliers_found": len(outliers)
        }

class MLDetector:
    """ML-based –¥–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª—ñ–π"""
    
    @staticmethod
    def isolation_forest_detection(data: pd.DataFrame, contamination: float = 0.1) -> Dict[str, Any]:
        """Isolation Forest –¥–µ—Ç–µ–∫—Ü—ñ—è"""
        
        if len(data) < 10:
            return {"anomalies": [], "scores": np.zeros(len(data))}
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        scaler = StandardScaler()
        
        # –í–∏–±–∏—Ä–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —á–∏—Å–ª–æ–≤—ñ —Å—Ç–æ–≤–ø—Ü—ñ
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return {"anomalies": [], "scores": np.zeros(len(data))}
        
        X = data[numeric_cols].fillna(0)
        X_scaled = scaler.fit_transform(X)
        
        # –ú–æ–¥–µ–ª—å
        model = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100
        )
        
        # –ù–∞–≤—á–∞–Ω–Ω—è —ñ –ø—Ä–µ–¥–∏–∫—Ç–∏
        predictions = model.fit_predict(X_scaled)
        anomaly_scores = model.decision_function(X_scaled)
        
        # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ scores –≤ –¥—ñ–∞–ø–∞–∑–æ–Ω [0, 1]
        scores_normalized = (anomaly_scores - anomaly_scores.min()) / (anomaly_scores.max() - anomaly_scores.min() + 1e-6)
        
        # –ê–Ω–æ–º–∞–ª—ñ—ó (predictions == -1)
        anomaly_indices = data.index[predictions == -1].tolist()
        
        return {
            "anomalies": anomaly_indices,
            "scores": scores_normalized,
            "contamination": contamination,
            "features_used": numeric_cols.tolist()
        }

class AnomalyAgent:
    """Anomaly Detection Agent"""
    
    def __init__(self):
        self.app = FastAPI(title="Anomaly Agent", version="1.0.0")
        
        # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ —Å–µ—Ä–≤—ñ—Å—ñ–≤
        self.redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)
        self.opensearch_client = OpenSearch([{'host': 'opensearch', 'port': 9200}])
        
        # –î–µ—Ç–µ–∫—Ç–æ—Ä–∏
        self.stat_detector = StatisticalDetector()
        self.ml_detector = MLDetector()
        
        self._setup_routes()
    
    def _setup_routes(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è HTTP –º–∞—Ä—à—Ä—É—Ç—ñ–≤"""
        
        @self.app.post("/anomaly/run")
        async def run_detection(request: dict):
            """–ó–∞–ø—É—Å–∫ –¥–µ—Ç–µ–∫—Ü—ñ—ó –∞–Ω–æ–º–∞–ª—ñ–π"""
            try:
                detection_request = AnomalyDetectionRequest(
                    index=request["index"],
                    field=request["field"],
                    method=AnomalyMethod(request.get("method", "statistical")),
                    window=request.get("window", "7d"),
                    threshold=request.get("threshold", 0.95),
                    parameters=request.get("parameters", {})
                )
                
                result = await self.detect_anomalies(detection_request)
                return asdict(result)
                
            except Exception as e:
                logger.error("Anomaly detection failed", error=str(e))
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/anomaly/methods")
        async def get_methods():
            """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤"""
            return [
                {
                    "method": method.value,
                    "description": self._get_method_description(method)
                }
                for method in AnomalyMethod
            ]
        
        @self.app.get("/anomaly/health")
        async def health():
            """Health check"""
            try:
                self.redis_client.ping()
                self.opensearch_client.cluster.health()
                
                return {"status": "healthy", "timestamp": datetime.now().isoformat()}
            except Exception as e:
                return {"status": "unhealthy", "error": str(e)}
    
    async def detect_anomalies(self, request: AnomalyDetectionRequest) -> AnomalyDetectionResult:
        """–û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –¥–µ—Ç–µ–∫—Ü—ñ—ó –∞–Ω–æ–º–∞–ª—ñ–π"""
        
        request_id = f"anomaly_{int(datetime.now().timestamp())}"
        logger.info("Starting anomaly detection", request_id=request_id, method=request.method.value)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ OpenSearch
        data = await self._load_data_from_opensearch(request)
        
        if len(data) == 0:
            return AnomalyDetectionResult(
                request_id=request_id,
                method=request.method.value,
                total_points=0,
                anomaly_count=0,
                anomaly_percentage=0.0,
                anomalies=[],
                summary="No data found for the specified criteria",
                recommendations=[]
            )
        
        # –í–∏–±—ñ—Ä –º–µ—Ç–æ–¥—É –¥–µ—Ç–µ–∫—Ü—ñ—ó
        if request.method == AnomalyMethod.Z_SCORE:
            detection_result = self.stat_detector.z_score_detection(
                data[request.field], 
                threshold=request.parameters.get("threshold", 3.0)
            )
        
        elif request.method == AnomalyMethod.IQR:
            detection_result = self.stat_detector.iqr_detection(
                data[request.field],
                k=request.parameters.get("k", 1.5)
            )
        
        elif request.method == AnomalyMethod.SEASONAL_ESD:
            detection_result = self.stat_detector.seasonal_esd_detection(
                data[request.field],
                seasonality=request.parameters.get("seasonality", 24),
                alpha=request.parameters.get("alpha", 0.05)
            )
        
        elif request.method == AnomalyMethod.ISOLATION_FOREST:
            detection_result = self.ml_detector.isolation_forest_detection(
                data,
                contamination=request.parameters.get("contamination", 0.1)
            )
        
        else:
            # Fallback –¥–æ statistical
            detection_result = self.stat_detector.z_score_detection(data[request.field])
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è AnomalyPoint –æ–±'—î–∫—Ç—ñ–≤
        anomaly_points = []
        anomaly_indices = detection_result["anomalies"]
        scores = detection_result["scores"]
        
        for idx in anomaly_indices:
            if idx < len(data):
                anomaly_points.append(AnomalyPoint(
                    timestamp=data.index[idx] if hasattr(data.index[idx], 'to_pydatetime') else datetime.now(),
                    value=float(data[request.field].iloc[idx]),
                    anomaly_score=float(scores[idx]) if idx < len(scores) else 1.0,
                    method=request.method.value,
                    explanation=self._generate_explanation(request.method, detection_result, idx, data),
                    context={"index": request.index, "field": request.field}
                ))
        
        # –°–æ—Ä—Ç—É—î–º–æ –ø–æ anomaly_score (–Ω–∞–π–≤–∏—â—ñ —Å–ø–æ—á–∞—Ç–∫—É)
        anomaly_points.sort(key=lambda x: x.anomaly_score, reverse=True)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        result = AnomalyDetectionResult(
            request_id=request_id,
            method=request.method.value,
            total_points=len(data),
            anomaly_count=len(anomaly_points),
            anomaly_percentage=round((len(anomaly_points) / max(1, len(data))) * 100, 2),
            anomalies=anomaly_points[:50],  # –¢–æ–ø 50 –∞–Ω–æ–º–∞–ª—ñ–π
            summary=self._generate_summary(request.method, len(data), len(anomaly_points)),
            recommendations=self._generate_recommendations(request.method, len(anomaly_points), len(data))
        )
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        result_json = json.dumps(asdict(result), default=str)
        self.redis_client.setex(f"anomaly_result:{request_id}", 3600, result_json)
        
        # –ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ–¥—ñ—ó
        await self._publish_event("anomaly.detected", {
            "request_id": request_id,
            "index": request.index,
            "field": request.field,
            "method": request.method.value,
            "anomaly_count": len(anomaly_points),
            "anomaly_percentage": result.anomaly_percentage
        })
        
        return result
    
    async def _load_data_from_opensearch(self, request: AnomalyDetectionRequest) -> pd.DataFrame:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ OpenSearch"""
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —á–∞—Å–æ–≤–æ–≥–æ –≤—ñ–∫–Ω–∞
        now = datetime.now()
        if request.window.endswith('d'):
            days = int(request.window[:-1])
            start_time = now - timedelta(days=days)
        elif request.window.endswith('h'):
            hours = int(request.window[:-1])
            start_time = now - timedelta(hours=hours)
        else:
            start_time = now - timedelta(days=7)  # Fallback
        
        # –ó–∞–ø–∏—Ç –¥–æ OpenSearch
        query = {
            "query": {
                "range": {
                    "@timestamp": {
                        "gte": start_time.isoformat(),
                        "lte": now.isoformat()
                    }
                }
            },
            "sort": [{"@timestamp": {"order": "asc"}}],
            "size": 10000
        }
        
        try:
            response = self.opensearch_client.search(
                index=request.index,
                body=query
            )
            
            # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤ DataFrame
            hits = response["hits"]["hits"]
            if not hits:
                return pd.DataFrame()
            
            data = []
            for hit in hits:
                doc = hit["_source"]
                doc["_timestamp"] = doc.get("@timestamp", now.isoformat())
                data.append(doc)
            
            df = pd.DataFrame(data)
            
            # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è timestamp –≤ —ñ–Ω–¥–µ–∫—Å
            if "_timestamp" in df.columns:
                df["_timestamp"] = pd.to_datetime(df["_timestamp"])
                df.set_index("_timestamp", inplace=True)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –ø–æ–ª—è
            if request.field not in df.columns:
                logger.warning("Field not found in data", field=request.field, available_fields=list(df.columns))
                return pd.DataFrame()
            
            # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–ª—è –≤ —á–∏—Å–ª–æ–≤–∏–π —Ç–∏–ø
            df[request.field] = pd.to_numeric(df[request.field], errors='coerce')
            df = df.dropna(subset=[request.field])
            
            return df
            
        except Exception as e:
            logger.error("Failed to load data from OpenSearch", error=str(e))
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
            return self._generate_test_data(request.field)
    
    def _generate_test_data(self, field: str) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö"""
        
        dates = pd.date_range(
            start=datetime.now() - timedelta(days=7),
            end=datetime.now(),
            freq='1H'
        )
        
        # –ù–æ—Ä–º–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑ —Ç—Ä–µ–Ω–¥–æ–º
        normal_values = np.random.normal(100, 10, len(dates))
        trend = np.linspace(0, 20, len(dates))
        values = normal_values + trend
        
        # –î–æ–¥–∞—î–º–æ –∫—ñ–ª—å–∫–∞ –∞–Ω–æ–º–∞–ª—ñ–π
        anomaly_indices = np.random.choice(len(values), size=5, replace=False)
        for idx in anomaly_indices:
            values[idx] = values[idx] + np.random.choice([-1, 1]) * np.random.uniform(50, 100)
        
        df = pd.DataFrame({
            field: values
        }, index=dates)
        
        return df
    
    def _generate_explanation(self, method: AnomalyMethod, detection_result: Dict, idx: int, data: pd.DataFrame) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø–æ—è—Å–Ω–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ—ó"""
        
        if method == AnomalyMethod.Z_SCORE:
            z_score = detection_result["scores"][idx]
            return f"Z-score of {z_score:.2f} exceeds threshold (mean¬±3œÉ). Value is {z_score:.1f} standard deviations from mean."
        
        elif method == AnomalyMethod.IQR:
            Q1, Q3 = detection_result["Q1"], detection_result["Q3"]
            lower, upper = detection_result["lower_bound"], detection_result["upper_bound"]
            value = data.iloc[idx, 0]
            return f"Value {value:.2f} outside IQR bounds [{lower:.2f}, {upper:.2f}]. Normal range: [{Q1:.2f}, {Q3:.2f}]"
        
        elif method == AnomalyMethod.ISOLATION_FOREST:
            score = detection_result["scores"][idx]
            return f"Isolation Forest anomaly score: {score:.3f}. Point is isolated from normal patterns."
        
        else:
            return f"Detected by {method.value} method with high confidence."
    
    def _generate_summary(self, method: AnomalyMethod, total_points: int, anomaly_count: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—ñ–¥—Å—É–º–∫—É"""
        
        percentage = (anomaly_count / max(1, total_points)) * 100
        
        if anomaly_count == 0:
            return f"No anomalies detected using {method.value} method across {total_points} data points."
        
        elif percentage < 1:
            return f"Low anomaly rate: {anomaly_count}/{total_points} points ({percentage:.1f}%) detected as anomalous."
        
        elif percentage < 5:
            return f"Normal anomaly rate: {anomaly_count}/{total_points} points ({percentage:.1f}%) show unusual patterns."
        
        else:
            return f"High anomaly rate: {anomaly_count}/{total_points} points ({percentage:.1f}%) are anomalous. Investigate data quality."
    
    def _generate_recommendations(self, method: AnomalyMethod, anomaly_count: int, total_points: int) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π"""
        
        recommendations = []
        percentage = (anomaly_count / max(1, total_points)) * 100
        
        if anomaly_count == 0:
            recommendations.append("No anomalies found. Data appears normal.")
            recommendations.append("Consider adjusting detection sensitivity if anomalies are expected.")
        
        elif percentage > 10:
            recommendations.append("High anomaly rate detected. Investigate data quality issues.")
            recommendations.append("Check for systematic errors or data collection problems.")
            recommendations.append("Consider data cleaning before further analysis.")
        
        elif anomaly_count > 0:
            recommendations.append("Review detected anomalies for business significance.")
            recommendations.append("Investigate root causes of unusual patterns.")
            
        if method in [AnomalyMethod.Z_SCORE, AnomalyMethod.IQR]:
            recommendations.append("Consider using ML-based methods for complex pattern detection.")
        
        return recommendations
    
    def _get_method_description(self, method: AnomalyMethod) -> str:
        """–û–ø–∏—Å –º–µ—Ç–æ–¥—É –¥–µ—Ç–µ–∫—Ü—ñ—ó"""
        
        descriptions = {
            AnomalyMethod.Z_SCORE: "Statistical method using Z-score (3 sigma rule)",
            AnomalyMethod.IQR: "Interquartile Range method for outlier detection",
            AnomalyMethod.SEASONAL_ESD: "Seasonal Extreme Studentized Deviate for time series",
            AnomalyMethod.ISOLATION_FOREST: "ML ensemble method for multivariate anomaly detection",
            AnomalyMethod.STATISTICAL: "Combined statistical methods"
        }
        
        return descriptions.get(method, "Unknown method")
    
    async def _publish_event(self, event_type: str, data: Dict[str, Any]):
        """–ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ–¥—ñ—ó –≤ Redis Streams"""
        try:
            event_data = {
                "event_type": event_type,
                "timestamp": datetime.now().isoformat(),
                "source": "AnomalyAgent",
                **data
            }
            
            self.redis_client.xadd("pred:events:anomaly", event_data)
            logger.debug("Event published", event_type=event_type)
            
        except Exception as e:
            logger.error("Failed to publish event", error=str(e))

# –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞
if __name__ == "__main__":
    import uvicorn
    
    agent = AnomalyAgent()
    uvicorn.run(agent.app, host="0.0.0.0", port=9020)
