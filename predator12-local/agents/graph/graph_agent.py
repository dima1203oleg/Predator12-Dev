#!/usr/bin/env python3
"""
GraphAgent - –ê–≥–µ–Ω—Ç –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —Ç–∞ –ø–æ—à—É–∫—É –∑–≤'—è–∑–∫—ñ–≤
–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –¥–∞–Ω—ñ —É –≤–∏–≥–ª—è–¥—ñ –≥—Ä–∞—Ñ—ñ–≤ —ñ –≤–∏–∫–æ–Ω—É—î –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –Ω–∞ –≥—Ä–∞—Ñ–∞—Ö –∑–≥—ñ–¥–Ω–æ –∑ –¢–ó
"""

import asyncio
import logging
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass
import networkx as nx
from collections import defaultdict
import aiohttp
import aiofiles
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class GraphNode:
    id: str
    type: str
    properties: Dict[str, Any]

@dataclass
class GraphEdge:
    source: str
    target: str
    type: str
    weight: float = 1.0
    properties: Dict[str, Any] = None

@dataclass
class CommunityInfo:
    community_id: int
    nodes: List[str]
    size: int
    density: float
    key_nodes: List[str]

class GraphAgent:
    """
    –ê–≥–µ–Ω—Ç –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –∑–≥—ñ–¥–Ω–æ –∑ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–º –∑–∞–≤–¥–∞–Ω–Ω—è–º
    """

    def __init__(self):
        self.graphs: Dict[str, nx.Graph] = {}
        self.analysis_results: Dict[str, Any] = {}
        self.opensearch_url = "http://opensearch:9200"
        self.postgres_url = "postgresql://postgres:postgres@db:5432/predator11"

        # –ö–µ—à –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
        self.node_cache: Dict[str, GraphNode] = {}
        self.edge_cache: Dict[str, List[GraphEdge]] = {}

        # –ê–ª–≥–æ—Ä–∏—Ç–º–∏ —è–∫—ñ –ø—ñ–¥—Ç—Ä–∏–º—É—î–º–æ
        self.supported_algorithms = {
            'pagerank': self.calculate_pagerank,
            'betweenness_centrality': self.calculate_betweenness_centrality,
            'closeness_centrality': self.calculate_closeness_centrality,
            'community_detection': self.detect_communities,
            'shortest_path': self.find_shortest_path,
            'connected_components': self.find_connected_components,
            'clustering_coefficient': self.calculate_clustering,
            'influence_propagation': self.simulate_influence_propagation
        }

    async def initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –≥—Ä–∞—Ñ–æ–≤–∏–π –∞–≥–µ–Ω—Ç"""
        logger.info("üöÄ Initializing Graph Agent...")

        # –°—Ç–≤–æ—Ä—é—î–º–æ –±–∞–∑–æ–≤—ñ –≥—Ä–∞—Ñ–∏
        await self.create_base_graphs()

        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª
        await self.load_data_from_sources()

        logger.info("‚úÖ Graph Agent initialized successfully")

    async def create_base_graphs(self):
        """–°—Ç–≤–æ—Ä—é—î –±–∞–∑–æ–≤—ñ –≥—Ä–∞—Ñ–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö"""
        # –ì—Ä–∞—Ñ —Å–æ—Ü—ñ–∞–ª—å–Ω–∏—Ö –∑–≤'—è–∑–∫—ñ–≤
        self.graphs['social'] = nx.Graph()

        # –ì—Ä–∞—Ñ –º–µ—Ä–µ–∂–µ–≤–∏—Ö –≤–∑–∞—î–º–æ–¥—ñ–π
        self.graphs['network'] = nx.DiGraph()  # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≥—Ä–∞—Ñ

        # –ì—Ä–∞—Ñ –ø–æ–¥—ñ–π —Ç–∞ –∑–≤'—è–∑–∫—ñ–≤
        self.graphs['events'] = nx.Graph()

        # –ì—Ä–∞—Ñ –∑–Ω–∞–Ω—å (–æ–Ω—Ç–æ–ª–æ–≥—ñ—á–Ω–∏–π)
        self.graphs['knowledge'] = nx.DiGraph()

        logger.info("üìä Created base graphs: social, network, events, knowledge")

    async def load_data_from_sources(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –≥—Ä–∞—Ñ—ñ–≤"""
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ –∑ OpenSearch
            await self.load_from_opensearch()

            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ –∑ PostgreSQL
            await self.load_from_postgres()

            logger.info("‚úÖ Data loaded from all sources")

        except Exception as e:
            logger.error(f"Failed to load data from sources: {e}")

    async def load_from_opensearch(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –∑ OpenSearch –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –≥—Ä–∞—Ñ—ñ–≤"""
        try:
            async with aiohttp.ClientSession() as session:
                # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞–Ω—ñ –ø—Ä–æ –≤–∑–∞—î–º–æ–¥—ñ—ó –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
                query = {
                    "size": 1000,
                    "query": {"match_all": {}},
                    "_source": ["user_id", "target_id", "interaction_type", "timestamp"]
                }

                async with session.post(f"{self.opensearch_url}/*/_search", json=query) as response:
                    if response.status == 200:
                        data = await response.json()
                        hits = data.get('hits', {}).get('hits', [])

                        for hit in hits:
                            source_data = hit['_source']
                            await self.add_interaction_to_graph(source_data)

                        logger.info(f"üìà Loaded {len(hits)} interactions from OpenSearch")

        except Exception as e:
            logger.error(f"Failed to load from OpenSearch: {e}")

    async def load_from_postgres(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –∑ PostgreSQL –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –≥—Ä–∞—Ñ—ñ–≤"""
        try:
            import asyncpg

            conn = await asyncpg.connect(self.postgres_url)

            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ —Ç–∞ —ó—Ö –∞—Ç—Ä–∏–±—É—Ç–∏
            users = await conn.fetch("""
                SELECT user_id, username, role, created_at, properties 
                FROM users 
                WHERE active = true
            """)

            for user in users:
                await self.add_user_node(user)

            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π–Ω—ñ –∑–≤'—è–∑–∫–∏
            relationships = await conn.fetch("""
                SELECT source_id, target_id, relationship_type, strength, metadata
                FROM user_relationships
            """)

            for rel in relationships:
                await self.add_relationship_edge(rel)

            await conn.close()

            logger.info(f"üë• Loaded {len(users)} users and {len(relationships)} relationships from PostgreSQL")

        except Exception as e:
            logger.error(f"Failed to load from PostgreSQL: {e}")

    async def add_interaction_to_graph(self, interaction_data: Dict[str, Any]):
        """–î–æ–¥–∞—î –≤–∑–∞—î–º–æ–¥—ñ—é –¥–æ –≥—Ä–∞—Ñ—É"""
        user_id = interaction_data.get('user_id')
        target_id = interaction_data.get('target_id')
        interaction_type = interaction_data.get('interaction_type', 'unknown')
        timestamp = interaction_data.get('timestamp')

        if user_id and target_id:
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –≥—Ä–∞—Ñ –∑–∞ —Ç–∏–ø–æ–º –≤–∑–∞—î–º–æ–¥—ñ—ó
            if interaction_type in ['message', 'call', 'meeting']:
                graph = self.graphs['social']
            elif interaction_type in ['network_request', 'api_call', 'data_transfer']:
                graph = self.graphs['network']
            else:
                graph = self.graphs['events']

            # –î–æ–¥–∞—î–º–æ –≤—É–∑–ª–∏ —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—é—Ç—å
            if not graph.has_node(user_id):
                graph.add_node(user_id, type='user', first_seen=timestamp)

            if not graph.has_node(target_id):
                graph.add_node(target_id, type='entity', first_seen=timestamp)

            # –î–æ–¥–∞—î–º–æ –∞–±–æ –æ–Ω–æ–≤–ª—é—î–º–æ —Ä–µ–±—Ä–æ
            if graph.has_edge(user_id, target_id):
                # –ó–±—ñ–ª—å—à—É—î–º–æ –≤–∞–≥—É —ñ—Å–Ω—É—é—á–æ–≥–æ —Ä–µ–±—Ä–∞
                graph[user_id][target_id]['weight'] += 1
                graph[user_id][target_id]['last_interaction'] = timestamp
            else:
                graph.add_edge(user_id, target_id,
                             weight=1,
                             type=interaction_type,
                             first_interaction=timestamp,
                             last_interaction=timestamp)

    async def add_user_node(self, user_data):
        """–î–æ–¥–∞—î –≤—É–∑–æ–ª –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –¥–æ —Å–æ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ—É"""
        user_id = str(user_data['user_id'])

        node_properties = {
            'type': 'user',
            'username': user_data.get('username'),
            'role': user_data.get('role'),
            'created_at': user_data.get('created_at'),
            'properties': user_data.get('properties', {})
        }

        # –î–æ–¥–∞—î–º–æ –¥–æ —Å–æ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ—É
        self.graphs['social'].add_node(user_id, **node_properties)

        # –ö–µ—à—É—î–º–æ –≤—É–∑–æ–ª
        self.node_cache[user_id] = GraphNode(
            id=user_id,
            type='user',
            properties=node_properties
        )

    async def add_relationship_edge(self, relationship_data):
        """–î–æ–¥–∞—î —Ä–µ–±—Ä–æ –∑–≤'—è–∑–∫—É –º—ñ–∂ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º–∏"""
        source_id = str(relationship_data['source_id'])
        target_id = str(relationship_data['target_id'])
        rel_type = relationship_data.get('relationship_type', 'connected')
        strength = relationship_data.get('strength', 1.0)

        edge_properties = {
            'type': rel_type,
            'weight': strength,
            'metadata': relationship_data.get('metadata', {})
        }

        self.graphs['social'].add_edge(source_id, target_id, **edge_properties)

    async def run_graph_analysis(self, graph_name: str, algorithm: str, **kwargs) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞—î –≥—Ä–∞—Ñ–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑"""
        if graph_name not in self.graphs:
            raise ValueError(f"Graph '{graph_name}' not found")

        if algorithm not in self.supported_algorithms:
            raise ValueError(f"Algorithm '{algorithm}' not supported")

        graph = self.graphs[graph_name]
        logger.info(f"üîç Running {algorithm} on {graph_name} graph ({graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges)")

        start_time = datetime.now()

        try:
            # –í–∏–∫–æ–Ω—É—î–º–æ –∞–ª–≥–æ—Ä–∏—Ç–º
            result = await self.supported_algorithms[algorithm](graph, **kwargs)

            execution_time = (datetime.now() - start_time).total_seconds()

            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'graph_name': graph_name,
                'algorithm': algorithm,
                'execution_time': execution_time,
                'graph_stats': {
                    'nodes': graph.number_of_nodes(),
                    'edges': graph.number_of_edges(),
                    'density': nx.density(graph)
                },
                'result': result,
                'parameters': kwargs
            }

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.analysis_results[f"{graph_name}_{algorithm}_{int(start_time.timestamp())}"] = analysis_result

            logger.info(f"‚úÖ Completed {algorithm} analysis in {execution_time:.2f}s")
            return analysis_result

        except Exception as e:
            logger.error(f"Failed to run {algorithm} on {graph_name}: {e}")
            raise

    async def calculate_pagerank(self, graph: nx.Graph, **kwargs) -> Dict[str, float]:
        """–û–±—á–∏—Å–ª—é—î PageRank –¥–ª—è –≤—É–∑–ª—ñ–≤ –≥—Ä–∞—Ñ—É"""
        alpha = kwargs.get('alpha', 0.85)
        max_iter = kwargs.get('max_iter', 100)

        if isinstance(graph, nx.DiGraph):
            pagerank = nx.pagerank(graph, alpha=alpha, max_iter=max_iter)
        else:
            # –î–ª—è –Ω–µ–æ—Ä—ñ—î–Ω—Ç–æ–≤–∞–Ω–æ–≥–æ –≥—Ä–∞—Ñ—É —Å—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ø—ñ—é –∑ –æ—Ä—ñ—î–Ω—Ç–æ–≤–∞–Ω–∏–º–∏ —Ä–µ–±—Ä–∞–º–∏
            directed_graph = graph.to_directed()
            pagerank = nx.pagerank(directed_graph, alpha=alpha, max_iter=max_iter)

        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é
        sorted_pagerank = dict(sorted(pagerank.items(), key=lambda x: x[1], reverse=True))

        return {
            'pagerank_scores': sorted_pagerank,
            'top_nodes': list(sorted_pagerank.keys())[:10],
            'algorithm_params': {'alpha': alpha, 'max_iter': max_iter}
        }

    async def calculate_betweenness_centrality(self, graph: nx.Graph, **kwargs) -> Dict[str, float]:
        """–û–±—á–∏—Å–ª—é—î —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ñ—Å—Ç—å –∑–∞ –ø–æ—Å–µ—Ä–µ–¥–Ω–∏—Ü—Ç–≤–æ–º"""
        normalized = kwargs.get('normalized', True)

        betweenness = nx.betweenness_centrality(graph, normalized=normalized)
        sorted_betweenness = dict(sorted(betweenness.items(), key=lambda x: x[1], reverse=True))

        return {
            'betweenness_centrality': sorted_betweenness,
            'top_bridge_nodes': list(sorted_betweenness.keys())[:10],
            'algorithm_params': {'normalized': normalized}
        }

    async def calculate_closeness_centrality(self, graph: nx.Graph, **kwargs) -> Dict[str, float]:
        """–û–±—á–∏—Å–ª—é—î —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ñ—Å—Ç—å –∑–∞ –±–ª–∏–∑—å–∫—ñ—Å—Ç—é"""
        closeness = nx.closeness_centrality(graph)
        sorted_closeness = dict(sorted(closeness.items(), key=lambda x: x[1], reverse=True))

        return {
            'closeness_centrality': sorted_closeness,
            'most_central_nodes': list(sorted_closeness.keys())[:10]
        }

    async def detect_communities(self, graph: nx.Graph, **kwargs) -> Dict[str, Any]:
        """–í–∏—è–≤–ª—è—î —Å–ø—ñ–ª—å–Ω–æ—Ç–∏ —É –≥—Ä–∞—Ñ—ñ"""
        algorithm = kwargs.get('algorithm', 'louvain')

        if algorithm == 'louvain':
            communities = nx.community.louvain_communities(graph)
        elif algorithm == 'greedy_modularity':
            communities = nx.community.greedy_modularity_communities(graph)
        else:
            communities = nx.community.louvain_communities(graph)  # default

        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∫–æ–∂–Ω—É —Å–ø—ñ–ª—å–Ω–æ—Ç—É
        community_analysis = []
        for i, community in enumerate(communities):
            subgraph = graph.subgraph(community)

            community_info = CommunityInfo(
                community_id=i,
                nodes=list(community),
                size=len(community),
                density=nx.density(subgraph),
                key_nodes=await self._find_key_nodes_in_community(subgraph)
            )

            community_analysis.append(community_info)

        # –û–±—á–∏—Å–ª—é—î–º–æ –º–æ–¥—É–ª—è—Ä–Ω—ñ—Å—Ç—å
        modularity = nx.community.modularity(graph, communities)

        return {
            'communities': [
                {
                    'id': c.community_id,
                    'size': c.size,
                    'density': c.density,
                    'nodes': c.nodes,
                    'key_nodes': c.key_nodes
                } for c in community_analysis
            ],
            'total_communities': len(communities),
            'modularity': modularity,
            'algorithm': algorithm
        }

    async def _find_key_nodes_in_community(self, subgraph: nx.Graph) -> List[str]:
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∫–ª—é—á–æ–≤—ñ –≤—É–∑–ª–∏ –≤ —Å–ø—ñ–ª—å–Ω–æ—Ç—ñ"""
        if subgraph.number_of_nodes() == 0:
            return []

        # –û–±—á–∏—Å–ª—é—î–º–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ñ—Å—Ç—å –¥–ª—è –≤—É–∑–ª—ñ–≤ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏
        centrality = nx.degree_centrality(subgraph)

        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–æ–ø-3 –Ω–∞–π–±—ñ–ª—å—à —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∏—Ö –≤—É–∑–ª–∏
        sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
        return [node for node, _ in sorted_nodes[:3]]

    async def find_shortest_path(self, graph: nx.Graph, **kwargs) -> Dict[str, Any]:
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π–∫–æ—Ä–æ—Ç—à–∏–π —à–ª—è—Ö –º—ñ–∂ –≤—É–∑–ª–∞–º–∏"""
        source = kwargs.get('source')
        target = kwargs.get('target')

        if not source or not target:
            raise ValueError("Source and target nodes must be specified")

        if not graph.has_node(source) or not graph.has_node(target):
            return {'path': None, 'length': None, 'error': 'Node not found'}

        try:
            if nx.is_weighted(graph):
                path = nx.shortest_path(graph, source, target, weight='weight')
                length = nx.shortest_path_length(graph, source, target, weight='weight')
            else:
                path = nx.shortest_path(graph, source, target)
                length = nx.shortest_path_length(graph, source, target)

            return {
                'path': path,
                'length': length,
                'hops': len(path) - 1,
                'source': source,
                'target': target
            }

        except nx.NetworkXNoPath:
            return {
                'path': None,
                'length': float('inf'),
                'error': 'No path exists',
                'source': source,
                'target': target
            }

    async def find_connected_components(self, graph: nx.Graph, **kwargs) -> Dict[str, Any]:
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∑–≤'—è–∑–∞–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏"""
        if isinstance(graph, nx.DiGraph):
            # –î–ª—è –æ—Ä—ñ—î–Ω—Ç–æ–≤–∞–Ω–æ–≥–æ –≥—Ä–∞—Ñ—É –∑–Ω–∞—Ö–æ–¥–∏–º–æ —Å–ª–∞–±–∫–æ –∑–≤'—è–∑–∞–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
            components = list(nx.weakly_connected_components(graph))
            component_type = "weakly_connected"
        else:
            # –î–ª—è –Ω–µ–æ—Ä—ñ—î–Ω—Ç–æ–≤–∞–Ω–æ–≥–æ –≥—Ä–∞—Ñ—É
            components = list(nx.connected_components(graph))
            component_type = "connected"

        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        component_analysis = []
        for i, component in enumerate(components):
            subgraph = graph.subgraph(component)

            component_analysis.append({
                'id': i,
                'size': len(component),
                'density': nx.density(subgraph),
                'nodes': list(component),
                'diameter': nx.diameter(subgraph) if nx.is_connected(subgraph) else None
            })

        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º
        component_analysis.sort(key=lambda x: x['size'], reverse=True)

        return {
            'components': component_analysis,
            'total_components': len(components),
            'largest_component_size': max(len(c) for c in components) if components else 0,
            'component_type': component_type
        }

    async def calculate_clustering(self, graph: nx.Graph, **kwargs) -> Dict[str, Any]:
        """–û–±—á–∏—Å–ª—é—î –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó"""
        # –ó–∞–≥–∞–ª—å–Ω–∏–π –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
        avg_clustering = nx.average_clustering(graph)

        # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—É–∑–ª–∞
        clustering_coeffs = nx.clustering(graph)

        # –°–æ—Ä—Ç—É—î–º–æ –≤—É–∑–ª–∏ –∑–∞ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
        sorted_clustering = dict(sorted(clustering_coeffs.items(), key=lambda x: x[1], reverse=True))

        return {
            'average_clustering': avg_clustering,
            'node_clustering': sorted_clustering,
            'highly_clustered_nodes': [
                node for node, coeff in sorted_clustering.items()
                if coeff > avg_clustering
            ][:10]
        }

    async def simulate_influence_propagation(self, graph: nx.Graph, **kwargs) -> Dict[str, Any]:
        """–°–∏–º—É–ª—é—î –ø–æ—à–∏—Ä–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É –ø–æ –≥—Ä–∞—Ñ—É"""
        seed_nodes = kwargs.get('seed_nodes', [])
        propagation_prob = kwargs.get('propagation_prob', 0.3)
        max_iterations = kwargs.get('max_iterations', 10)

        if not seed_nodes:
            # –Ø–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ –ø–æ—á–∞—Ç–∫–æ–≤—ñ –≤—É–∑–ª–∏, –±–µ—Ä–µ–º–æ –Ω–∞–π–±—ñ–ª—å—à —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ñ
            centrality = nx.degree_centrality(graph)
            seed_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:3]
            seed_nodes = [node for node, _ in seed_nodes]

        # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω - —Ç—ñ–ª—å–∫–∏ seed –≤—É–∑–ª–∏ –∞–∫—Ç–∏–≤–Ω—ñ
        active_nodes = set(seed_nodes)
        propagation_history = [set(seed_nodes)]

        for iteration in range(max_iterations):
            new_active = set()

            for active_node in active_nodes:
                # –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å—É—Å—ñ–¥–∞ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—É–∑–ª–∞
                for neighbor in graph.neighbors(active_node):
                    if neighbor not in active_nodes:
                        # –Ü–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–∞–≥–∏ —Ä–µ–±—Ä–∞
                        edge_data = graph.get_edge_data(active_node, neighbor)
                        weight = edge_data.get('weight', 1.0) if edge_data else 1.0

                        # –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ —ñ–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å
                        activation_prob = min(propagation_prob * weight, 1.0)

                        if np.random.random() < activation_prob:
                            new_active.add(neighbor)

            if not new_active:
                break  # –ü–æ—à–∏—Ä–µ–Ω–Ω—è –∑—É–ø–∏–Ω–∏–ª–æ—Å—è

            active_nodes.update(new_active)
            propagation_history.append(active_nodes.copy())

        return {
            'seed_nodes': seed_nodes,
            'final_active_nodes': list(active_nodes),
            'total_infected': len(active_nodes),
            'infection_rate': len(active_nodes) / graph.number_of_nodes(),
            'iterations': len(propagation_history) - 1,
            'propagation_history': [list(nodes) for nodes in propagation_history],
            'parameters': {
                'propagation_prob': propagation_prob,
                'max_iterations': max_iterations
            }
        }

    async def find_anomalous_nodes(self, graph_name: str) -> Dict[str, Any]:
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∞–Ω–æ–º–∞–ª—å–Ω—ñ –≤—É–∑–ª–∏ —É –≥—Ä–∞—Ñ—ñ"""
        graph = self.graphs[graph_name]

        # –û–±—á–∏—Å–ª—é—î–º–æ —Ä—ñ–∑–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—ñ
        degree_centrality = nx.degree_centrality(graph)
        betweenness_centrality = nx.betweenness_centrality(graph)
        closeness_centrality = nx.closeness_centrality(graph)

        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—É–∑–ª–∏ –∑ –∞–Ω–æ–º–∞–ª—å–Ω–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        anomalous_nodes = []

        for node in graph.nodes():
            # –ó–±–∏—Ä–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—É–∑–ª–∞
            metrics = {
                'degree': degree_centrality.get(node, 0),
                'betweenness': betweenness_centrality.get(node, 0),
                'closeness': closeness_centrality.get(node, 0),
                'clustering': nx.clustering(graph, node)
            }

            # –®—É–∫–∞—î–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –¥—É–∂–µ –≤–∏—Å–æ–∫—ñ –∞–±–æ –Ω–∏–∑—å–∫—ñ –∑–Ω–∞—á–µ–Ω–Ω—è)
            anomaly_score = 0

            # –í–∏—Å–æ–∫–∏–π —Å—Ç—É–ø—ñ–Ω—å –ø—Ä–∏ –Ω–∏–∑—å–∫—ñ–π —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—ñ –∑–∞ –±–ª–∏–∑—å–∫—ñ—Å—Ç—é
            if metrics['degree'] > 0.1 and metrics['closeness'] < 0.1:
                anomaly_score += 1

            # –í–∏—Å–æ–∫–µ –ø–æ—Å–µ—Ä–µ–¥–Ω–∏—Ü—Ç–≤–æ –ø—Ä–∏ –Ω–∏–∑—å–∫—ñ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
            if metrics['betweenness'] > 0.1 and metrics['clustering'] < 0.1:
                anomaly_score += 1

            # –Ü–∑–æ–ª—å–æ–≤–∞–Ω—ñ –≤—É–∑–ª–∏ –∑ –≤–∏—Å–æ–∫–æ—é —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ñ—Å—Ç—é
            if graph.degree(node) == 1 and metrics['degree'] > 0.05:
                anomaly_score += 1

            if anomaly_score > 0:
                anomalous_nodes.append({
                    'node': node,
                    'anomaly_score': anomaly_score,
                    'metrics': metrics,
                    'degree': graph.degree(node)
                })

        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ñ—Å—Ç—é
        anomalous_nodes.sort(key=lambda x: x['anomaly_score'], reverse=True)

        return {
            'anomalous_nodes': anomalous_nodes[:20],  # –¢–æ–ø-20 –∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö –≤—É–∑–ª—ñ–≤
            'total_anomalies': len(anomalous_nodes),
            'graph_stats': {
                'nodes': graph.number_of_nodes(),
                'edges': graph.number_of_edges(),
                'density': nx.density(graph)
            }
        }

    async def export_graph_visualization(self, graph_name: str, output_format: str = 'json') -> Dict[str, Any]:
        """–ï–∫—Å–ø–æ—Ä—Ç—É—î –≥—Ä–∞—Ñ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
        graph = self.graphs[graph_name]

        if output_format == 'json':
            # –§–æ—Ä–º–∞—Ç –¥–ª—è D3.js –∞–±–æ —ñ–Ω—à–∏—Ö –≤–µ–±-–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π
            nodes = []
            links = []

            for node in graph.nodes(data=True):
                nodes.append({
                    'id': node[0],
                    'type': node[1].get('type', 'unknown'),
                    'properties': node[1]
                })

            for edge in graph.edges(data=True):
                links.append({
                    'source': edge[0],
                    'target': edge[1],
                    'weight': edge[2].get('weight', 1),
                    'type': edge[2].get('type', 'unknown'),
                    'properties': edge[2]
                })

            return {
                'nodes': nodes,
                'links': links,
                'metadata': {
                    'graph_name': graph_name,
                    'node_count': len(nodes),
                    'edge_count': len(links),
                    'created_at': datetime.now().isoformat()
                }
            }

        else:
            raise ValueError(f"Unsupported output format: {output_format}")

    async def get_graph_statistics(self, graph_name: str) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä–∞—Ñ—É"""
        graph = self.graphs[graph_name]

        stats = {
            'name': graph_name,
            'type': 'directed' if isinstance(graph, nx.DiGraph) else 'undirected',
            'nodes': graph.number_of_nodes(),
            'edges': graph.number_of_edges(),
            'density': nx.density(graph),
            'is_connected': nx.is_connected(graph) if not isinstance(graph, nx.DiGraph) else nx.is_weakly_connected(graph)
        }

        if graph.number_of_nodes() > 0:
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–µ–ø—É—Å—Ç–∏—Ö –≥—Ä–∞—Ñ—ñ–≤
            if stats['is_connected']:
                stats['diameter'] = nx.diameter(graph)
                stats['radius'] = nx.radius(graph)
                stats['average_shortest_path_length'] = nx.average_shortest_path_length(graph)

            stats['average_clustering'] = nx.average_clustering(graph)
            stats['number_of_triangles'] = sum(nx.triangles(graph).values()) // 3

        return stats

    async def get_status(self) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç—É—Å GraphAgent"""
        return {
            'timestamp': datetime.now().isoformat(),
            'graphs': {
                name: await self.get_graph_statistics(name)
                for name in self.graphs.keys()
            },
            'cached_nodes': len(self.node_cache),
            'cached_edges': len(self.edge_cache),
            'analysis_results_count': len(self.analysis_results),
            'supported_algorithms': list(self.supported_algorithms.keys()),
            'status': 'active'
        }

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä –∞–≥–µ–Ω—Ç–∞
graph_agent = GraphAgent()

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–ø—É—Å–∫—É –∞–≥–µ–Ω—Ç–∞"""
    logger.info("üöÄ Starting Predator Analytics Graph Agent...")

    try:
        await graph_agent.initialize()
        logger.info("‚úÖ Graph Agent is ready for analysis requests")

        # –¢—Ä–∏–º–∞—î–º–æ –∞–≥–µ–Ω—Ç –∑–∞–ø—É—â–µ–Ω–∏–º
        while True:
            await asyncio.sleep(60)

    except KeyboardInterrupt:
        logger.info("Received interrupt signal")
    except Exception as e:
        logger.error(f"Error in Graph Agent: {e}")

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
