version: '3.8'

services:
  # ============================================================================
  # CORE SERVICES
  # ============================================================================
  
  db:
    image: postgres:15
    container_name: predator-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./observability/db/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: predator-redis
    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # ============================================================================
  # APPLICATION SERVICES
  # ============================================================================
  
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: predator-backend
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OPENSEARCH_URL=${OPENSEARCH_URL}
      - OPENSEARCH_USERNAME=${OPENSEARCH_USERNAME}
      - OPENSEARCH_PASSWORD=${OPENSEARCH_PASSWORD}
      - MINIO_URL=${MINIO_URL}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - MODEL_SDK_BASE_URL=${MODEL_SDK_BASE_URL}
      - MODEL_SDK_KEY=${MODEL_SDK_KEY}
      - QDRANT_URL=${QDRANT_URL}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT}
      - OTEL_SERVICE_NAME=${OTEL_SERVICE_NAME}
    ports:
      - "8000:8000"
    volumes:
      - ./logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: predator-frontend
    depends_on:
      - backend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://backend:8000
      - NEXT_PUBLIC_KEYCLOAK_URL=${KEYCLOAK_URL}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # WORKER SERVICES (CELERY)
  # ============================================================================
  
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: predator-worker
    depends_on:
      backend:
        condition: service_healthy
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND}
      - CELERY_CONCURRENCY=${CELERY_CONCURRENCY}
      - OPENSEARCH_URL=${OPENSEARCH_URL}
      - MINIO_URL=${MINIO_URL}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - QDRANT_URL=${QDRANT_URL}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
    command: >
      sh -c "celery -A app.workers.celery_app worker -l info --concurrency=${CELERY_CONCURRENCY}"
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped

  beat:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: predator-beat
    depends_on:
      - worker
      - redis
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND}
      - CELERY_BEAT_SCHEDULE_FILENAME=${CELERY_BEAT_SCHEDULE_FILENAME}
    command: >
      sh -c "celery -A app.workers.celery_app beat -l info --schedule=${CELERY_BEAT_SCHEDULE_FILENAME}"
    volumes:
      - ./logs:/app/logs
      - celery_beat_data:/tmp
    restart: unless-stopped

  agent-supervisor:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: predator-agent-supervisor
    depends_on:
      - backend
      - worker
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OPENSEARCH_URL=${OPENSEARCH_URL}
      - PROMETHEUS_URL=${PROMETHEUS_URL}
      - LOKI_URL=${LOKI_URL}
      - TEMPO_URL=${TEMPO_URL}
    command: python -u app/agents/supervisor.py
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped

  # ============================================================================
  # MESSAGE BROKER (REDPANDA)
  # ============================================================================
  
  redpanda:
    image: redpandadata/redpanda:v24.1.2
    container_name: predator-redpanda
    command:
      - redpanda start --overprovisioned --smp 1 --memory 1G --reserve-memory 0M --check=false
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:19092
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr internal://redpanda:8082,external://localhost:18082
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr redpanda:33145
      - --advertise-rpc-addr redpanda:33145
    ports:
      - "19092:19092"
      - "18081:18081"
      - "18082:18082"
      - "9644:9644"
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -q 'Healthy'"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # ============================================================================
  # VECTOR DATABASE
  # ============================================================================
  
  qdrant:
    image: qdrant/qdrant:latest
    container_name: predator-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # SEARCH & STORAGE
  # ============================================================================
  
  opensearch:
    image: opensearchproject/opensearch:2.9.0
    container_name: predator-opensearch
    environment:
      - cluster.name=predator-opensearch-cluster
      - node.name=predator-opensearch-node1
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms2g -Xmx2g"
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      # For production, remove DISABLE_SECURITY_PLUGIN=true and configure security
      - "DISABLE_SECURITY_PLUGIN=true"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
      - "9600:9600"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.9.0
    container_name: predator-opensearch-dashboards
    ports:
      - "5601:5601"
    environment:
      - OPENSEARCH_HOSTS=http://opensearch:9200
      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true"
    depends_on:
      opensearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5601/api/status"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  opensearch-setup:
    image: curlimages/curl:7.85.0
    container_name: predator-opensearch-setup
    depends_on:
      opensearch:
        condition: service_healthy
    entrypoint: ["sh", "-c"]
    command: >
      "
      echo 'Waiting for OpenSearch to be ready...';
      sleep 30;
      echo 'Creating ISM policies...';
      curl -X PUT -H 'Content-Type: application/json' 
           http://opensearch:9200/_plugins/_ism/policies/logs_policy 
           -d @/policies/logs_policy.json;
      curl -X PUT -H 'Content-Type: application/json' 
           http://opensearch:9200/_index_template/logs_template 
           -d @/policies/logs_template.json;
      curl -X PUT -H 'Content-Type: application/json' 
           http://opensearch:9200/logs-000001 
           -d '{\"aliases\":{\"logs_current\":{\"is_write_index\":true}}}';
      echo 'OpenSearch setup completed';
      "
    volumes:
      - ./observability/opensearch/policies:/policies
    restart: "no"

  minio:
    image: minio/minio:RELEASE.2023-10-07T15-07-38Z
    container_name: predator-minio
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # AUTHENTICATION
  # ============================================================================
  
  keycloak:
    image: quay.io/keycloak/keycloak:23.0.0
    container_name: predator-keycloak
    command: start-dev
    environment:
      - KEYCLOAK_ADMIN=${KEYCLOAK_ADMIN}
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD}
      - KC_DB=postgres
      - KC_DB_URL=${KC_DB_URL}
      - KC_DB_USERNAME=${KC_DB_USERNAME}
      - KC_DB_PASSWORD=${KC_DB_PASSWORD}
      - KC_HOSTNAME=localhost
      - KC_HTTP_ENABLED=true
    ports:
      - "8080:8080"
    depends_on:
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # ============================================================================
  # OBSERVABILITY STACK
  # ============================================================================
  
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: predator-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./observability/prometheus/rules:/etc/prometheus/rules
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  grafana:
    image: grafana/grafana:10.1.0
    container_name: predator-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana/provisioning:/etc/grafana/provisioning
      - ./observability/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
      - loki
      - tempo
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  loki:
    image: grafana/loki:2.9.0
    container_name: predator-loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./observability/loki/local-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.0
    container_name: predator-promtail
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./observability/promtail/config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    restart: unless-stopped

  tempo:
    image: grafana/tempo:2.5.0
    container_name: predator-tempo
    ports:
      - "4317:4317"   # OTLP grpc port for traces
      - "3200:3200"   # HTTP query port (for Grafana)
    volumes:
      - ./observability/tempo/config.yml:/etc/tempo/config.yml
      - tempo_data:/var/tempo
    command: -config.file=/etc/tempo/config.yml
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: predator-alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./observability/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # PROMETHEUS EXPORTERS
  # ============================================================================
  
  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: predator-node-exporter
    restart: unless-stopped
    pid: host
    network_mode: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: predator-cadvisor
    ports:
      - "8085:8080"
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    privileged: true
    devices:
      - /dev/kmsg

  opensearch-exporter:
    image: quay.io/prometheuscommunity/elasticsearch-exporter:v1.7.0
    container_name: predator-opensearch-exporter
    ports:
      - "9114:9114"
    restart: unless-stopped
    command:
      - "--es.uri=http://opensearch:9200"
      - "--es.all"
      - "--es.indices"
      - "--es.shards"
    depends_on:
      opensearch:
        condition: service_healthy

  blackbox-exporter:
    image: prom/blackbox-exporter:v0.24.0
    container_name: predator-blackbox-exporter
    ports:
      - "9115:9115"
    restart: unless-stopped
    volumes:
      - ./observability/blackbox/config.yml:/etc/blackbox_exporter/config.yml

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  postgres_data:
  redis_data:
  opensearch_data:
  minio_data:
  grafana_data:
  prometheus_data:
  alertmanager_data:
  loki_data:
  tempo_data:
  qdrant_data:
  redpanda_data:
  celery_beat_data:

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  default:
    driver: bridge
    name: predator-network
